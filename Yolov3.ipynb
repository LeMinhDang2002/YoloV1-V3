{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['0', '1', '2',  '3', '4', \n",
    "          '5', '6', '7',  '8','9']\n",
    "\n",
    "num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Number Plate Region\\env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "from collections.abc import Iterable\n",
    "from keras.utils import Sequence\n",
    "from utils import tools\n",
    "\n",
    "class Yolo_data(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_shape=(416, 416, 3),\n",
    "                 class_names=[]):\n",
    "        self.input_shape = input_shape\n",
    "        self.grid_shape = input_shape[0]//32, input_shape[1]//32\n",
    "        self.class_names = class_names\n",
    "        self.class_num = len(class_names)\n",
    "        self.fpn_layers = 3\n",
    "        self.file_names = None\n",
    "        \n",
    "    def read_file_to_dataset(\n",
    "        self, img_path=None, label_path=None,\n",
    "        label_format=\"labelimg\",\n",
    "        rescale=1/255,\n",
    "        preprocessing=None,\n",
    "        augmenter=None,\n",
    "        aug_times=1,\n",
    "        shuffle=True, seed=None,\n",
    "        encoding=\"big5\",\n",
    "        thread_num=10):\n",
    "        \n",
    "        grid_amp = 2**(self.fpn_layers - 1)\n",
    "        grid_shape = (self.grid_shape[0]*grid_amp,\n",
    "                      self.grid_shape[1]*grid_amp)\n",
    "        img_data, label_data, path_list = tools.read_file(\n",
    "            img_path=img_path, \n",
    "            label_path=label_path,\n",
    "            label_format=label_format,\n",
    "            size=self.input_shape[:2], \n",
    "            grid_shape=grid_shape,\n",
    "            class_names=self.class_names,\n",
    "            rescale=rescale,\n",
    "            preprocessing=preprocessing,\n",
    "            augmenter=augmenter,\n",
    "            aug_times=aug_times,\n",
    "            shuffle=shuffle, seed=seed,\n",
    "            encoding=encoding,\n",
    "            thread_num=thread_num)\n",
    "        self.file_names = path_list\n",
    "\n",
    "        label_list = [label_data]\n",
    "        for _ in range(self.fpn_layers - 1):\n",
    "            label_data = tools.down2xlabel(label_data)\n",
    "            label_list.insert(0, label_data)\n",
    "\n",
    "        return img_data, label_list\n",
    "    def vis_img(self, img, *label_datas,\n",
    "                conf_threshold=0.5,\n",
    "                show_conf=True,\n",
    "                nms_mode=0,\n",
    "                nms_threshold=0.5,\n",
    "                nms_sigma=0.5,\n",
    "                **kwargs):\n",
    "\n",
    "        return tools.vis_img(\n",
    "                             img, \n",
    "                             *label_datas, \n",
    "                             class_names=self.class_names,\n",
    "                             conf_threshold=conf_threshold,\n",
    "                             show_conf=show_conf,\n",
    "                             nms_mode=nms_mode,  \n",
    "                             nms_threshold=nms_threshold,\n",
    "                             nms_sigma=nms_sigma,\n",
    "                             version=3,\n",
    "                             **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice! Repeat!: 000945.jpg 42 33\n"
     ]
    }
   ],
   "source": [
    "yolo_data = Yolo_data(class_names=class_names)\n",
    "\n",
    "img_path   = \"../01_1K_MNIST/mnist_train/\"\n",
    "label_path = \"../01_1K_MNIST/xml_train/\"\n",
    "\n",
    "train_img, train_label = yolo_data.read_file_to_dataset(\n",
    "    img_path, label_path,\n",
    "    thread_num=50,\n",
    "    shuffle=False)\n",
    "\n",
    "img_path   = \"../01_1K_MNIST/mnist_val/\"\n",
    "label_path = \"../01_1K_MNIST/xml_val/\"\n",
    "\n",
    "test_img, test_label = yolo_data.read_file_to_dataset(\n",
    "    img_path, label_path,\n",
    "    thread_num=50,\n",
    "    shuffle=False)\n",
    "\n",
    "valid_img  = test_img\n",
    "valid_label = test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Conv2D, UpSampling2D, Concatenate, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.utils import get_file\n",
    "\n",
    "from functools import wraps, reduce\n",
    "\n",
    "from keras.layers import concatenate, Add, ZeroPadding2D, LeakyReLU, BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "import numpy as np\n",
    "\n",
    "## Hyperparameter\n",
    "n_epoch = 10\n",
    "keep_prob = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###DropBlock\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.layers import Layer\n",
    "# from keras.src.layers import Layer\n",
    "\n",
    "def _bernoulli(shape, mean):\n",
    "    return tf.nn.relu(tf.sign(mean - tf.random.uniform(shape, minval=0, maxval=1, dtype=tf.float32)))\n",
    "\n",
    "class DropBlock2D(Layer):\n",
    "    def __init__(self, keep_prob, block_size, scale=True,name=None, **kwargs):\n",
    "        super(DropBlock2D, self).__init__(name=\"DropBlock2D\")\n",
    "        self.keep_prob = float(keep_prob) if isinstance(keep_prob, int) else keep_prob\n",
    "        self.block_size = int(block_size)\n",
    "        self.names = name\n",
    "        self.scale = tf.constant(scale, dtype=tf.bool) if isinstance(scale, bool) else scale\n",
    "        super(DropBlock2D, self).__init__(**kwargs)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update( {\"block_size\": self.block_size,\"keep_prob\": self.keep_prob,\"name\": self.names })\n",
    "\n",
    "        return config\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 4\n",
    "        _, self.h, self.w, self.channel = input_shape.as_list()\n",
    "        # pad the mask\n",
    "        p1 = (self.block_size - 1) // 2\n",
    "        p0 = (self.block_size - 1) - p1\n",
    "        self.padding = [[0, 0], [p0, p1], [p0, p1], [0, 0]]\n",
    "        self.set_keep_prob()\n",
    "        super(DropBlock2D, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        def drop():\n",
    "            mask = self._create_mask(tf.shape(inputs))\n",
    "            output = inputs * mask\n",
    "            output = tf.cond(self.scale,\n",
    "                             true_fn=lambda: output *tf.cast(tf.size(mask), dtype=tf.float32)  / tf.reduce_sum(mask),\n",
    "                             false_fn=lambda: output)\n",
    "            return output\n",
    "\n",
    "        if training is None:\n",
    "            training = K.learning_phase()\n",
    "        # output = tf.cond(tf.logical_or(tf.logical_not(training), tf.equal(self.keep_prob, 1.0)),\n",
    "        output = tf.cond(tf.logical_or(tf.logical_not(bool(training)), tf.equal(self.keep_prob, 1.0)),\n",
    "                         true_fn=lambda: inputs,\n",
    "                         false_fn=drop)\n",
    "        return output\n",
    "\n",
    "    def set_keep_prob(self, keep_prob=None):\n",
    "        \"\"\"This method only supports Eager Execution\"\"\"\n",
    "        if keep_prob is not None:\n",
    "            self.keep_prob = keep_prob\n",
    "        w, h = tf.cast(self.w, dtype=tf.float32), tf.cast(self.h, dtype=tf.float32)\n",
    "        \n",
    "        self.gamma = (1. - self.keep_prob) * (w * h) / (self.block_size ** 2) / \\\n",
    "                     ((w - self.block_size + 1) * (h - self.block_size + 1))\n",
    "\n",
    "    def _create_mask(self, input_shape):\n",
    "        sampling_mask_shape = tf.stack([input_shape[0],\n",
    "                                       self.h - self.block_size + 1,\n",
    "                                       self.w - self.block_size + 1,\n",
    "                                       self.channel])\n",
    "        mask = _bernoulli(sampling_mask_shape, self.gamma)\n",
    "        mask = tf.pad(mask, self.padding)\n",
    "        mask = tf.nn.max_pool(mask, [1, self.block_size, self.block_size, 1], [1, 1, 1, 1], 'SAME')\n",
    "        mask = 1 - mask\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Leaky Convolutional\n",
    "def compose(*funcs):\n",
    "    # return lambda x: reduce(lambda v, f: f(v), funcs, x)\n",
    "    if funcs:\n",
    "        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)\n",
    "    else:\n",
    "        raise ValueError('Composition of empty sequence not supported.')\n",
    "    \n",
    "@wraps(Conv2D)\n",
    "def DarknetConv2D(*args, **kwargs):\n",
    "    '''Wrapper to set Darknet parameters for Convolution2D.'''\n",
    "    darknet_conv_kwargs = {'kernel_initializer': 'he_normal'}\n",
    "    if kwargs.get('strides') == (2, 2):\n",
    "        darknet_conv_kwargs['padding'] = 'valid'\n",
    "    else:\n",
    "        darknet_conv_kwargs['padding'] = 'same'\n",
    "    darknet_conv_kwargs.update(kwargs)\n",
    "    return Conv2D(*args, **darknet_conv_kwargs)\n",
    "\n",
    "def DarknetConv2D_BN_Leaky(*args, **kwargs):\n",
    "    '''Darknet Convolution2D followed by BatchNormalization and LeakyReLU.'''\n",
    "    no_bias_kwargs = {'use_bias': False}\n",
    "    no_bias_kwargs.update(kwargs)\n",
    "    return compose(\n",
    "        DarknetConv2D(*args, **no_bias_kwargs),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Residual Block\n",
    "def resblock_body(x, num_filters, num_blocks):\n",
    "    '''A series of resblocks starting with a downsampling Convolution2D'''\n",
    "    # Darknet uses left and top padding instead of 'same' mode\n",
    "    x = ZeroPadding2D(((1, 0), (1, 0)))(x)\n",
    "    x = DarknetConv2D_BN_Leaky(num_filters, (3, 3), strides=(2, 2))(x)\n",
    "    x = DropBlock2D(keep_prob=keep_prob, block_size=3)(x)\n",
    "    for _ in range(num_blocks):\n",
    "        y = compose(\n",
    "            DarknetConv2D_BN_Leaky(num_filters//2, (1, 1)),\n",
    "            DropBlock2D(keep_prob=keep_prob, block_size=3),\n",
    "            DarknetConv2D_BN_Leaky(num_filters, (3, 3)))(x)\n",
    "        x = Add()([x, y])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Backbone Darknet\n",
    "def Backbone_darknet(input_tensor):\n",
    "    '''Darknent body having 52 Convolution2D layers'''\n",
    "    x = DarknetConv2D_BN_Leaky(32, (3, 3))(input_tensor)\n",
    "    x = resblock_body(x, 64, 1)\n",
    "    x = resblock_body(x, 128, 2)\n",
    "    x = resblock_body(x, 256, 8)\n",
    "    x = resblock_body(x, 512, 8)\n",
    "    x = resblock_body(x, 1024, 4)\n",
    "    return x\n",
    "\n",
    "\n",
    "def make_last_layers(x, num_filters):\n",
    "    '''6 Conv2D_BN_Leaky layers followed by a Conv2D_linear layer'''\n",
    "    x = compose(\n",
    "            DarknetConv2D_BN_Leaky(num_filters, (1, 1)),\n",
    "            DropBlock2D(keep_prob=keep_prob, block_size=3),\n",
    "            DarknetConv2D_BN_Leaky(num_filters*2, (3, 3)),\n",
    "            DropBlock2D(keep_prob=keep_prob, block_size=3),\n",
    "            DarknetConv2D_BN_Leaky(num_filters, (1, 1)),\n",
    "            DropBlock2D(keep_prob=keep_prob, block_size=3),\n",
    "            DarknetConv2D_BN_Leaky(num_filters*2, (3, 3)),\n",
    "            DropBlock2D(keep_prob=keep_prob, block_size=3),\n",
    "            DarknetConv2D_BN_Leaky(num_filters, (1, 1)))(x)\n",
    "    y = DarknetConv2D_BN_Leaky(num_filters*2, (3, 3))(x)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOLO Neck\n",
    "def yolo_neck(input_shape=(416, 416, 3),\n",
    "              pretrained_darknet=None,\n",
    "              pretrained_weights=None):\n",
    "    '''Create YOLO_V3 model CNN body in Keras.'''\n",
    "    inputs = Input(input_shape)\n",
    "    darknet = Model(inputs, Backbone_darknet(inputs))\n",
    "    if pretrained_darknet is not None:\n",
    "        darknet.set_weights(pretrained_darknet.get_weights())\n",
    "    \n",
    "    x, y1 = make_last_layers(darknet.output, 512)\n",
    "\n",
    "    x = compose(\n",
    "            DarknetConv2D_BN_Leaky(256, (1,1)),\n",
    "            UpSampling2D(2))(x)\n",
    "    x = Concatenate()([x, darknet.layers[152].output])\n",
    "    x, y2 = make_last_layers(x, 256)\n",
    "\n",
    "    x = compose(\n",
    "            DarknetConv2D_BN_Leaky(128, (1,1)),\n",
    "            UpSampling2D(2))(x)\n",
    "    x = Concatenate()([x, darknet.layers[92].output])\n",
    "    x, y3 = make_last_layers(x, 128)\n",
    "    model = Model(inputs, [y1, y2, y3])\n",
    "    \n",
    "    '''\n",
    "    if pretrained_weights is not None:\n",
    "        if pretrained_weights == \"pascal_voc\":\n",
    "            pretrained_weights = get_file(\n",
    "                \"tf_keras_yolov3_body.h5\",\n",
    "                WEIGHTS_PATH_DN_BODY,\n",
    "                cache_subdir=\"models\")\n",
    "        model.load_weights(pretrained_weights)\n",
    "    ''' \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOLO Head\n",
    "def yolo_head(model_body, class_num=10, \n",
    "              anchors=[[0.89663461, 0.78365384],\n",
    "                       [0.37500000, 0.47596153],\n",
    "                       [0.27884615, 0.21634615],\n",
    "                       [0.14182692, 0.28605769],\n",
    "                       [0.14903846, 0.10817307],\n",
    "                       [0.07211538, 0.14663461],\n",
    "                       [0.07932692, 0.05528846],\n",
    "                       [0.03846153, 0.07211538],\n",
    "                       [0.02403846, 0.03125000]]):\n",
    "    anchors = np.array(anchors)\n",
    "    inputs = model_body.input\n",
    "    output = model_body.output\n",
    "    tensor_num = len(output)\n",
    "\n",
    "    if len(anchors)%tensor_num > 0:\n",
    "        raise ValueError((\"The total number of anchor boxs\"\n",
    "                          \" should be a multiple of the number(%s)\"\n",
    "                          \" of output tensors\") % tensor_num)    \n",
    "    abox_num = len(anchors)//tensor_num\n",
    "\n",
    "    outputs_list = []\n",
    "    for tensor_i, output_tensor in enumerate(output):\n",
    "        output_list = []\n",
    "        start_i = tensor_i*abox_num\n",
    "        for box in anchors[start_i:start_i + abox_num]:\n",
    "            xy_output = DarknetConv2D(2, 1,\n",
    "                            activation='sigmoid')(output_tensor)\n",
    "            wh_output = DarknetConv2D(2, 1,\n",
    "                            activation='exponential')(output_tensor)\n",
    "            wh_output = wh_output * box\n",
    "            c_output = DarknetConv2D(1, 1,\n",
    "                            activation='sigmoid')(output_tensor)\n",
    "            p_output = DarknetConv2D(class_num, 1,\n",
    "                            activation='sigmoid')(output_tensor)\n",
    "            output_list += [xy_output,\n",
    "                            wh_output,\n",
    "                            c_output,\n",
    "                            p_output]\n",
    "\n",
    "        outputs = concatenate(output_list, axis=-1)\n",
    "        outputs_list.append(outputs)\n",
    "    \n",
    "    model = Model(inputs, outputs_list)    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-07\n",
    "## IoU\n",
    "def cal_iou(xywh_true, xywh_pred, grid_shape):\n",
    "    grid_shape = np.array(grid_shape[::-1])\n",
    "    xy_true = xywh_true[..., 0:2]/grid_shape # N*S*S*1*2\n",
    "    wh_true = xywh_true[..., 2:4]\n",
    "\n",
    "    xy_pred = xywh_pred[..., 0:2]/grid_shape # N*S*S*B*2\n",
    "    wh_pred = xywh_pred[..., 2:4]\n",
    "    \n",
    "    half_xy_true = wh_true / 2.\n",
    "    mins_true    = xy_true - half_xy_true\n",
    "    maxes_true   = xy_true + half_xy_true\n",
    "\n",
    "    half_xy_pred = wh_pred / 2.\n",
    "    mins_pred    = xy_pred - half_xy_pred\n",
    "    maxes_pred   = xy_pred + half_xy_pred       \n",
    "    \n",
    "    intersect_mins  = tf.maximum(mins_pred,  mins_true)\n",
    "    intersect_maxes = tf.minimum(maxes_pred, maxes_true)\n",
    "    intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "    \n",
    "    true_areas = wh_true[..., 0] * wh_true[..., 1]\n",
    "    pred_areas = wh_pred[..., 0] * wh_pred[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores  = (intersect_areas + epsilon)/(union_areas + epsilon)\n",
    "    \n",
    "    return iou_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Yolo Loss Function\n",
    "def wrap_yolo_loss(grid_shape,\n",
    "                   bbox_num,\n",
    "                   class_num,\n",
    "                   anchors,\n",
    "                   binary_weight=1,\n",
    "                   loss_weight=[1, 1, 1, 1],\n",
    "                   ignore_thresh=.6,\n",
    "                   ):\n",
    "    def yolo_loss(y_true, y_pred):\n",
    "        panchors = tf.reshape(anchors, (1, 1, 1, bbox_num, 2))\n",
    "\n",
    "        y_true = tf.reshape(\n",
    "            y_true,\n",
    "            (-1, *grid_shape, 1, 5 + class_num)) # N*S*S*1*(5+C)\n",
    "        y_pred = tf.reshape(\n",
    "            y_pred,\n",
    "            (-1, *grid_shape, bbox_num, 5 + class_num)) # N*S*S*B*(5+C)\n",
    "\n",
    "        xywh_true = y_true[..., :4] # N*S*S*1*4\n",
    "        xywh_pred = y_pred[..., :4] # N*S*S*B*4\n",
    "\n",
    "        iou_scores = cal_iou(xywh_true, xywh_pred, grid_shape) # N*S*S*B\n",
    "        \n",
    "        response_mask = tf.one_hot(tf.argmax(iou_scores, axis=-1),\n",
    "                                   depth=bbox_num,\n",
    "                                   dtype=xywh_true.dtype) # N*S*S*B\n",
    "\n",
    "        has_obj_mask = y_true[..., 4]*response_mask # N*S*S*B\n",
    "        has_obj_mask_exp = tf.expand_dims(has_obj_mask, axis=-1) # N*S*S*B*1\n",
    "\n",
    "        no_obj_mask = tf.cast(\n",
    "            iou_scores < ignore_thresh,\n",
    "            iou_scores.dtype) # N*S*S*B\n",
    "        no_obj_mask = (1 - has_obj_mask)*no_obj_mask # N*S*S*B\n",
    "\n",
    "        xy_true = y_true[..., 0:2] # N*S*S*1*2\n",
    "        xy_pred = y_pred[..., 0:2] # N*S*S*B*2\n",
    "\n",
    "        wh_true = tf.maximum(y_true[..., 2:4]/panchors, epsilon) # N*S*S*1*2\n",
    "        wh_pred = y_pred[..., 2:4]/panchors\n",
    "        \n",
    "        wh_true = tf.math.log(wh_true) # N*S*S*B*2\n",
    "        wh_pred = tf.math.log(wh_pred) # N*S*S*B*2\n",
    "\n",
    "        c_pred = y_pred[..., 4] # N*S*S*B\n",
    "\n",
    "        box_loss_scale = 2 - y_true[..., 2:3]*y_true[..., 3:4] # N*S*S*1*1\n",
    "\n",
    "        xy_loss = tf.reduce_sum(\n",
    "            tf.reduce_mean(\n",
    "                has_obj_mask_exp # N*S*S*B*1\n",
    "                *box_loss_scale # N*S*S*1*1\n",
    "                *tf.square(xy_true - xy_pred), # N*S*S*B*2\n",
    "                axis=0))\n",
    "\n",
    "        wh_loss = tf.reduce_sum(\n",
    "            tf.reduce_mean(\n",
    "                has_obj_mask_exp # N*S*S*B*1\n",
    "                *box_loss_scale # N*S*S*1*1\n",
    "                *tf.square(wh_true - wh_pred), # N*S*S*B*2\n",
    "                axis=0))\n",
    "\n",
    "        has_obj_c_loss = tf.reduce_sum(\n",
    "                tf.reduce_mean(\n",
    "                has_obj_mask # N*S*S*1\n",
    "                *(tf.square(1 - c_pred)), # N*S*S*B\n",
    "                axis=0))\n",
    "\n",
    "        no_obj_c_loss = tf.reduce_sum(\n",
    "                tf.reduce_mean(\n",
    "                no_obj_mask # N*S*S*1\n",
    "                *(tf.square(0 - c_pred)), # N*S*S*B\n",
    "                axis=0))\n",
    "        \n",
    "        c_loss = has_obj_c_loss + binary_weight*no_obj_c_loss\n",
    "\n",
    "        p_true = y_true[..., -class_num:] # N*S*S*1*C\n",
    "        p_pred = y_pred[..., -class_num:] # N*S*S*B*C\n",
    "        p_pred = tf.clip_by_value(p_pred, epsilon, 1 - epsilon)\n",
    "        p_loss = -tf.reduce_sum(\n",
    "            tf.reduce_mean(\n",
    "                has_obj_mask_exp # N*S*S*B*1\n",
    "                *(p_true*tf.math.log(p_pred)\n",
    "                + (1 - p_true)*tf.math.log(1 - p_pred)), # N*S*S*B*C\n",
    "                axis=0))\n",
    "        \n",
    "        regularizer = tf.reduce_sum(\n",
    "            tf.reduce_mean(wh_pred**2, axis=0))*0.01\n",
    "\n",
    "        loss = (loss_weight[0]*xy_loss\n",
    "                + loss_weight[1]*wh_loss\n",
    "                + loss_weight[2]*c_loss\n",
    "                + loss_weight[3]*p_loss\n",
    "                + regularizer)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    return yolo_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Object Accuracy\n",
    "from keras.metrics import binary_accuracy\n",
    "def wrap_obj_acc(grid_shape, bbox_num, class_num):\n",
    "    def obj_acc(y_true, y_pred):\n",
    "        y_true = tf.reshape(\n",
    "            y_true,\n",
    "            (-1, *grid_shape, 1, 5 + class_num)) # N*S*S*1*5+C\n",
    "        y_pred = tf.reshape(\n",
    "            y_pred,\n",
    "            (-1, *grid_shape, bbox_num, 5 + class_num)) # N*S*S*B*5+C\n",
    "        \n",
    "        c_true = y_true[..., 4] # N*S*S*1\n",
    "        c_pred = tf.reduce_max(y_pred[..., 4], # N*S*S*B\n",
    "                               axis=-1,\n",
    "                               keepdims=True) # N*S*S*1\n",
    "\n",
    "        bi_acc = binary_accuracy(c_true, c_pred)\n",
    "\n",
    "        return bi_acc\n",
    "    return obj_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mean IoU\n",
    "def wrap_mean_iou(grid_shape, bbox_num, class_num):\n",
    "    def mean_iou(y_true, y_pred):\n",
    "        y_true = tf.reshape(\n",
    "            y_true,\n",
    "            (-1, *grid_shape, 1, 5 + class_num)) # N*S*S*1*5+C\n",
    "        y_pred = tf.reshape(\n",
    "            y_pred,\n",
    "            (-1, *grid_shape, bbox_num, 5 + class_num)) # N*S*S*B*5+C\n",
    "\n",
    "        has_obj_mask = y_true[..., 4] # N*S*S*1\n",
    "        \n",
    "        xywh_true = y_true[..., :4] # N*S*S*1*4\n",
    "        xywh_pred = y_pred[..., :4] # N*S*S*B*4\n",
    "\n",
    "        iou_scores = cal_iou(xywh_true, xywh_pred, grid_shape) # N*S*S*B\n",
    "        iou_scores = tf.reduce_max(iou_scores, axis=-1, keepdims=True) # N*S*S*1\n",
    "        iou_scores = iou_scores*has_obj_mask # N*S*S*1\n",
    "\n",
    "        num_p = tf.reduce_sum(has_obj_mask)\n",
    "\n",
    "        return tf.reduce_sum(iou_scores)/(num_p + epsilon)\n",
    "    return mean_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Class Accuracy\n",
    "def wrap_class_acc(grid_shape, bbox_num, class_num):\n",
    "    def class_acc(y_true, y_pred):\n",
    "        y_true = tf.reshape(\n",
    "            y_true,\n",
    "            (-1, *grid_shape, 1, 5 + class_num)) # N*S*S*1*5+C\n",
    "        y_pred = tf.reshape(\n",
    "            y_pred,\n",
    "            (-1, *grid_shape, bbox_num, 5 + class_num)) # N*S*S*B*5+C\n",
    "\n",
    "        has_obj_mask = y_true[..., 4] # N*S*S*1\n",
    "\n",
    "        pi_true = tf.argmax(y_true[..., -class_num:], # N*S*S*1*C\n",
    "                            axis=-1) # N*S*S*1\n",
    "        pi_pred = tf.argmax(y_pred[..., -class_num:], # N*S*S*B*C\n",
    "                            axis=-1) # N*S*S*B\n",
    "        \n",
    "        equal_mask = tf.cast(pi_true == pi_pred,\n",
    "                             dtype=y_true.dtype) # N*S*S*B\n",
    "        equal_mask = equal_mask*has_obj_mask # N*S*S*B\n",
    "\n",
    "        num_p = tf.reduce_sum(has_obj_mask)*bbox_num\n",
    "\n",
    "        return tf.reduce_sum(equal_mask)/(num_p + epsilon)\n",
    "    return class_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build Class for Model, Loss, Metrics\n",
    "class Yolo(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_shape=(416, 416, 3),\n",
    "                 class_names=[]):\n",
    "        self.input_shape = input_shape\n",
    "        self.grid_shape = input_shape[0]//32, input_shape[1]//32\n",
    "        self.abox_num = 3\n",
    "        self.class_names = class_names\n",
    "        self.class_num = len(class_names)\n",
    "        self.fpn_layers = 3\n",
    "        self.anchors = None\n",
    "        self.model = None\n",
    "        self.file_names = None\n",
    "\n",
    "    \"\"\"\n",
    "    Model Create\n",
    "    \"\"\"\n",
    "    def create_model(self,\n",
    "                     anchors=[[0.89663461, 0.78365384],\n",
    "                              [0.37500000, 0.47596153],\n",
    "                              [0.27884615, 0.21634615],\n",
    "                              [0.14182692, 0.28605769],\n",
    "                              [0.14903846, 0.10817307],\n",
    "                              [0.07211538, 0.14663461],\n",
    "                              [0.07932692, 0.05528846],\n",
    "                              [0.03846153, 0.07211538],\n",
    "                              [0.02403846, 0.03125000]],\n",
    "                     backbone=\"full_darknet\",\n",
    "                     pretrained_weights=None,\n",
    "                     pretrained_darknet=\"pascal_voc\"):\n",
    "        \n",
    "        if isinstance(pretrained_darknet, str):\n",
    "            pre_body_weights = pretrained_darknet\n",
    "            pretrained_darknet = None\n",
    "        else:\n",
    "            pre_body_weights = None\n",
    "        \n",
    "        model_body = yolo_neck(self.input_shape,\n",
    "            pretrained_weights=pre_body_weights)\n",
    "\n",
    "        if pretrained_darknet is not None:\n",
    "            model_body.set_weights(pretrained_darknet.get_weights())\n",
    "        self.model = yolo_head(model_body,\n",
    "                               self.class_num,\n",
    "                               anchors)\n",
    "         \n",
    "        if pretrained_weights is not None:\n",
    "            self.model.load_weights(pretrained_weights)\n",
    "        self.anchors = anchors\n",
    "        self.grid_shape = self.model.output[0].shape[1:3]\n",
    "        self.fpn_layers = len(self.model.output)\n",
    "        self.abox_num = len(self.anchors)//self.fpn_layers\n",
    "\n",
    "    \"\"\"\n",
    "    Loss Create\n",
    "    \"\"\"\n",
    "    def loss(self,\n",
    "             binary_weight=1,\n",
    "             loss_weight=[1, 1, 5, 1],\n",
    "             ignore_thresh=0.6):\n",
    "\n",
    "        if (not isinstance(binary_weight, Iterable)\n",
    "            or len(binary_weight) != self.fpn_layers):\n",
    "            binary_weight = [binary_weight]*self.fpn_layers\n",
    "        \n",
    "        if isinstance(loss_weight, dict):\n",
    "            loss_weight_list = []\n",
    "            loss_weight_list.append(loss_weight[\"xy\"])\n",
    "            loss_weight_list.append(loss_weight[\"wh\"])\n",
    "            loss_weight_list.append(loss_weight[\"conf\"])\n",
    "            loss_weight_list.append(loss_weight[\"prob\"])\n",
    "            loss_weight = loss_weight_list\n",
    "        \n",
    "        loss_list = []\n",
    "        for fpn_id in range(self.fpn_layers):\n",
    "            grid_amp = 2**(fpn_id)\n",
    "            grid_shape = (self.grid_shape[0]*grid_amp,\n",
    "                          self.grid_shape[1]*grid_amp)\n",
    "            anchors_id = self.abox_num*fpn_id\n",
    "            loss_list.append(wrap_yolo_loss(\n",
    "                grid_shape=grid_shape,\n",
    "                bbox_num=self.abox_num, \n",
    "                class_num=self.class_num,\n",
    "                anchors=self.anchors[\n",
    "                    anchors_id:anchors_id + self.abox_num],\n",
    "                binary_weight=binary_weight[fpn_id],\n",
    "                loss_weight=loss_weight,\n",
    "                ignore_thresh=ignore_thresh))\n",
    "        return loss_list\n",
    "    \n",
    "    \"\"\"\n",
    "    Metrics Create\n",
    "    \"\"\"\n",
    "    def metrics(self, type=\"obj_acc\"):\n",
    "        \n",
    "        \n",
    "        metrics_list = [[] for _ in range(self.fpn_layers)]\n",
    "        for fpn_id in range(self.fpn_layers):\n",
    "            grid_amp = 2**(fpn_id)\n",
    "            grid_shape = (self.grid_shape[0]*grid_amp,\n",
    "                            self.grid_shape[1]*grid_amp)\n",
    "            \n",
    "            if \"obj\" in type:\n",
    "                metrics_list[fpn_id].append(\n",
    "                    wrap_obj_acc(\n",
    "                        grid_shape,\n",
    "                        self.abox_num, \n",
    "                        self.class_num))\n",
    "            if \"iou\" in type:\n",
    "                metrics_list[fpn_id].append(\n",
    "                    wrap_mean_iou(\n",
    "                        grid_shape,\n",
    "                        self.abox_num, \n",
    "                        self.class_num))\n",
    "            if \"class\" in type:\n",
    "                metrics_list[fpn_id].append(\n",
    "                    wrap_class_acc(\n",
    "                        grid_shape,\n",
    "                        self.abox_num, \n",
    "                        self.class_num))\n",
    "        \n",
    "        return metrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1: loss = 0.5723\n",
      "epoch  2: loss = 0.2200\n",
      "epoch  3: loss = 0.2395\n",
      "epoch  4: loss = 0.0959\n",
      "epoch  5: loss = 0.1181\n",
      "epoch  6: loss = 0.1256\n",
      "epoch  7: loss = 0.1375\n",
      "epoch  8: loss = 0.1217\n",
      "epoch  9: loss = 0.1063\n",
      "epoch 10: loss = 0.1824\n",
      "epoch 11: loss = 0.1401\n",
      "epoch 12: loss = 0.1703\n",
      "epoch 13: loss = 0.1012\n",
      "epoch 14: loss = 0.1413\n",
      "epoch 15: loss = 0.1552\n",
      "epoch 16: loss = 0.1272\n",
      "epoch 17: loss = 0.0169\n",
      "epoch 18: loss = 0.0898\n",
      "epoch 19: loss = 0.1481\n",
      "epoch 20: loss = 0.1782\n",
      "epoch 21: loss = 0.1347\n",
      "epoch 22: loss = 0.0520\n",
      "epoch 23: loss = 0.1182\n",
      "epoch 24: loss = 0.1384\n",
      "epoch 25: loss = 0.1378\n",
      "epoch 26: loss = 0.0828\n",
      "epoch 27: loss = 0.1039\n",
      "epoch 28: loss = 0.1058\n",
      "epoch 29: loss = 0.1280\n",
      "epoch 30: loss = 0.1172\n",
      "epoch 31: loss = 0.0412\n",
      "epoch 32: loss = 0.1177\n",
      "epoch 33: loss = 0.1372\n",
      "epoch 34: loss = 0.1189\n",
      "epoch 35: loss = 0.0623\n",
      "epoch 36: loss = 0.1178\n",
      "epoch 37: loss = 0.1577\n",
      "epoch 38: loss = 0.0495\n",
      "epoch 39: loss = 0.0937\n",
      "epoch 40: loss = 0.0947\n",
      "epoch 41: loss = 0.0671\n",
      "epoch 42: loss = 0.0643\n",
      "epoch 43: loss = 0.1553\n",
      "epoch 44: loss = 0.1119\n",
      "epoch 45: loss = 0.1043\n",
      "epoch 46: loss = 0.1917\n",
      "epoch 47: loss = 0.1768\n",
      "epoch 48: loss = 0.0317\n",
      "epoch 49: loss = 0.1183\n",
      "epoch 50: loss = 0.1484\n",
      "epoch 51: loss = 0.0863\n",
      "epoch 52: loss = 0.1104\n",
      "epoch 53: loss = 0.1377\n",
      "epoch 54: loss = 0.0112\n",
      "epoch 55: loss = 0.1219\n",
      "epoch 56: loss = 0.1016\n",
      "epoch 57: loss = 0.1278\n",
      "epoch 58: loss = 0.1384\n",
      "epoch 59: loss = 0.0873\n",
      "epoch 60: loss = 0.1948\n",
      "epoch 61: loss = 0.1802\n",
      "epoch 62: loss = 0.0966\n",
      "epoch 63: loss = 0.1026\n",
      "epoch 64: loss = 0.1568\n",
      "epoch 65: loss = 0.0862\n",
      "epoch 66: loss = 0.1410\n",
      "epoch 67: loss = 0.0528\n",
      "epoch 68: loss = 0.1312\n",
      "epoch 69: loss = 0.0542\n",
      "epoch 70: loss = 0.1606\n",
      "epoch 71: loss = 0.1651\n",
      "epoch 72: loss = 0.0836\n",
      "epoch 73: loss = 0.0830\n",
      "epoch 74: loss = 0.1179\n",
      "epoch 75: loss = 0.1965\n",
      "epoch 76: loss = 0.2009\n",
      "epoch 77: loss = 0.1818\n",
      "epoch 78: loss = 0.1717\n",
      "epoch 79: loss = 0.1599\n",
      "epoch 80: loss = 0.1151\n",
      "epoch 81: loss = 0.0907\n",
      "epoch 82: loss = 0.1239\n",
      "epoch 83: loss = 0.0524\n",
      "epoch 84: loss = 0.1038\n",
      "epoch 85: loss = 0.1125\n",
      "epoch 86: loss = 0.1474\n",
      "epoch 87: loss = 0.1025\n",
      "epoch 88: loss = 0.0559\n",
      "epoch 89: loss = 0.1287\n",
      "epoch 90: loss = 0.0862\n",
      "epoch 91: loss = 0.1267\n",
      "epoch 92: loss = 0.1516\n",
      "epoch 93: loss = 0.1187\n",
      "epoch 94: loss = 0.0632\n",
      "epoch 95: loss = 0.0852\n",
      "epoch 96: loss = 0.1420\n",
      "epoch 97: loss = 0.1181\n",
      "epoch 98: loss = 0.1467\n",
      "epoch 99: loss = 0.0549\n",
      "epoch 100: loss = 0.1018\n",
      "epoch 101: loss = 0.1476\n",
      "epoch 102: loss = 0.0745\n",
      "epoch 103: loss = 0.0989\n",
      "epoch 104: loss = 0.0768\n",
      "epoch 105: loss = 0.1685\n",
      "epoch 106: loss = 0.0898\n",
      "epoch 107: loss = 0.0987\n",
      "epoch 108: loss = 0.0662\n",
      "epoch 109: loss = 0.0498\n",
      "epoch 110: loss = 0.1224\n",
      "epoch 111: loss = 0.1006\n",
      "epoch 112: loss = 0.0518\n",
      "epoch 113: loss = 0.1004\n",
      "epoch 114: loss = 0.0797\n",
      "epoch 115: loss = 0.1453\n",
      "epoch 116: loss = 0.0844\n",
      "epoch 117: loss = 0.1257\n",
      "epoch 118: loss = 0.1267\n",
      "epoch 119: loss = 0.1509\n",
      "epoch 120: loss = 0.0568\n",
      "epoch 121: loss = 0.0625\n",
      "epoch 122: loss = 0.1429\n",
      "epoch 123: loss = 0.1495\n",
      "epoch 124: loss = 0.1646\n",
      "epoch 125: loss = 0.1658\n",
      "epoch 126: loss = 0.1521\n",
      "epoch 127: loss = 0.1266\n",
      "epoch 128: loss = 0.0875\n",
      "epoch 129: loss = 0.1390\n",
      "epoch 130: loss = 0.1680\n",
      "epoch 131: loss = 0.1276\n",
      "epoch 132: loss = 0.1550\n",
      "epoch 133: loss = 0.1856\n",
      "epoch 134: loss = 0.1088\n",
      "epoch 135: loss = 0.1115\n",
      "epoch 136: loss = 0.1004\n",
      "epoch 137: loss = 0.0982\n",
      "epoch 138: loss = 0.0980\n",
      "epoch 139: loss = 0.0812\n",
      "epoch 140: loss = 0.1166\n",
      "epoch 141: loss = 0.1003\n",
      "epoch 142: loss = 0.1456\n",
      "epoch 143: loss = 0.0658\n",
      "epoch 144: loss = 0.0917\n",
      "epoch 145: loss = 0.1127\n",
      "epoch 146: loss = 0.0887\n",
      "epoch 147: loss = 0.1050\n",
      "epoch 148: loss = 0.1519\n",
      "epoch 149: loss = 0.1047\n",
      "epoch 150: loss = 0.1095\n",
      "epoch 151: loss = 0.0675\n",
      "epoch 152: loss = 0.1209\n",
      "epoch 153: loss = 0.1189\n",
      "epoch 154: loss = 0.1078\n",
      "epoch 155: loss = 0.0443\n",
      "epoch 156: loss = 0.0871\n",
      "epoch 157: loss = 0.1159\n",
      "epoch 158: loss = 0.1242\n",
      "epoch 159: loss = 0.1213\n",
      "epoch 160: loss = 0.1027\n",
      "epoch 161: loss = 0.1274\n",
      "epoch 162: loss = 0.1134\n",
      "epoch 163: loss = 0.1265\n",
      "epoch 164: loss = 0.0978\n",
      "epoch 165: loss = 0.1931\n",
      "epoch 166: loss = 0.1878\n",
      "epoch 167: loss = 0.1042\n",
      "epoch 168: loss = 0.1910\n",
      "epoch 169: loss = 0.1744\n",
      "epoch 170: loss = 0.0578\n",
      "epoch 171: loss = 0.0735\n",
      "epoch 172: loss = 0.0872\n",
      "epoch 173: loss = 0.0880\n",
      "epoch 174: loss = 0.0859\n",
      "epoch 175: loss = 0.0862\n",
      "epoch 176: loss = 0.1044\n",
      "epoch 177: loss = 0.0566\n",
      "epoch 178: loss = 0.0891\n",
      "epoch 179: loss = 0.0769\n",
      "epoch 180: loss = 0.1195\n",
      "epoch 181: loss = 0.0893\n",
      "epoch 182: loss = 0.1056\n",
      "epoch 183: loss = 0.0384\n",
      "epoch 184: loss = 0.1068\n",
      "epoch 185: loss = 0.0622\n",
      "epoch 186: loss = 0.1479\n",
      "epoch 187: loss = 0.0608\n",
      "epoch 188: loss = 0.1228\n",
      "epoch 189: loss = 0.1208\n",
      "epoch 190: loss = 0.1954\n",
      "epoch 191: loss = 0.1816\n",
      "epoch 192: loss = 0.0630\n",
      "epoch 193: loss = 0.0964\n",
      "epoch 194: loss = 0.1242\n",
      "epoch 195: loss = 0.1276\n",
      "epoch 196: loss = 0.1338\n",
      "epoch 197: loss = 0.1417\n",
      "epoch 198: loss = 0.1295\n",
      "epoch 199: loss = 0.1459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200: loss = 0.1362\n",
      "epoch 201: loss = 0.1566\n",
      "epoch 202: loss = 0.1611\n",
      "epoch 203: loss = 0.1671\n",
      "epoch 204: loss = 0.1928\n",
      "epoch 205: loss = 0.1967\n",
      "epoch 206: loss = 0.1310\n",
      "epoch 207: loss = 0.0726\n",
      "epoch 208: loss = 0.0673\n",
      "epoch 209: loss = 0.0434\n",
      "epoch 210: loss = 0.1208\n",
      "epoch 211: loss = 0.1242\n",
      "epoch 212: loss = 0.1391\n",
      "epoch 213: loss = 0.1090\n",
      "epoch 214: loss = 0.1603\n",
      "epoch 215: loss = 0.0517\n",
      "epoch 216: loss = 0.0594\n",
      "epoch 217: loss = 0.1462\n",
      "epoch 218: loss = 0.0664\n",
      "epoch 219: loss = 0.0833\n",
      "epoch 220: loss = 0.1480\n",
      "epoch 221: loss = 0.1330\n",
      "epoch 222: loss = 0.1132\n",
      "epoch 223: loss = 0.1804\n",
      "epoch 224: loss = 0.0969\n",
      "epoch 225: loss = 0.1382\n",
      "epoch 226: loss = 0.0917\n",
      "epoch 227: loss = 0.1060\n",
      "epoch 228: loss = 0.0817\n",
      "epoch 229: loss = 0.1600\n",
      "epoch 230: loss = 0.1577\n",
      "epoch 231: loss = 0.1510\n",
      "epoch 232: loss = 0.0784\n",
      "epoch 233: loss = 0.1118\n",
      "epoch 234: loss = 0.0255\n",
      "epoch 235: loss = 0.1128\n",
      "epoch 236: loss = 0.1518\n",
      "epoch 237: loss = 0.0167\n",
      "epoch 238: loss = 0.0969\n",
      "epoch 239: loss = 0.1973\n",
      "epoch 240: loss = 0.1875\n",
      "epoch 241: loss = 0.1186\n",
      "epoch 242: loss = 0.0890\n",
      "epoch 243: loss = 0.0968\n",
      "epoch 244: loss = 0.0993\n",
      "epoch 245: loss = 0.1787\n",
      "epoch 246: loss = 0.1685\n",
      "epoch 247: loss = 0.0185\n",
      "epoch 248: loss = 0.0813\n",
      "epoch 249: loss = 0.0872\n",
      "epoch 250: loss = 0.0960\n",
      "epoch 251: loss = 0.1354\n",
      "epoch 252: loss = 0.1527\n",
      "epoch 253: loss = 0.1295\n",
      "epoch 254: loss = 0.1476\n",
      "epoch 255: loss = 0.1614\n",
      "epoch 256: loss = 0.1660\n",
      "epoch 257: loss = 0.1543\n",
      "epoch 258: loss = 0.0978\n",
      "epoch 259: loss = 0.0604\n",
      "epoch 260: loss = 0.1001\n",
      "epoch 261: loss = 0.1071\n",
      "epoch 262: loss = 0.1338\n",
      "epoch 263: loss = 0.0784\n",
      "epoch 264: loss = 0.0953\n",
      "epoch 265: loss = 0.1251\n",
      "epoch 266: loss = 0.0623\n",
      "epoch 267: loss = 0.1450\n",
      "epoch 268: loss = 0.1116\n",
      "epoch 269: loss = 0.1454\n",
      "epoch 270: loss = 0.1011\n",
      "epoch 271: loss = 0.1741\n",
      "epoch 272: loss = 0.1380\n",
      "epoch 273: loss = 0.1345\n",
      "epoch 274: loss = 0.0742\n",
      "epoch 275: loss = 0.0963\n",
      "epoch 276: loss = 0.0810\n",
      "epoch 277: loss = 0.1413\n",
      "epoch 278: loss = 0.1099\n",
      "epoch 279: loss = 0.1296\n",
      "epoch 280: loss = 0.0760\n",
      "epoch 281: loss = 0.1208\n",
      "epoch 282: loss = 0.1750\n",
      "epoch 283: loss = 0.1480\n",
      "epoch 284: loss = 0.0997\n",
      "epoch 285: loss = 0.0664\n",
      "epoch 286: loss = 0.0429\n",
      "epoch 287: loss = 0.1502\n",
      "epoch 288: loss = 0.1239\n",
      "epoch 289: loss = 0.0336\n",
      "epoch 290: loss = 0.1023\n",
      "epoch 291: loss = 0.1218\n",
      "epoch 292: loss = 0.0237\n",
      "epoch 293: loss = 0.1196\n",
      "epoch 294: loss = 0.1179\n",
      "epoch 295: loss = 0.1024\n",
      "epoch 296: loss = 0.1827\n",
      "epoch 297: loss = 0.1479\n",
      "epoch 298: loss = 0.0751\n",
      "epoch 299: loss = 0.1772\n",
      "epoch 300: loss = 0.1185\n",
      "epoch 301: loss = 0.0781\n",
      "epoch 302: loss = 0.0243\n",
      "epoch 303: loss = 0.0699\n",
      "epoch 304: loss = 0.1407\n",
      "epoch 305: loss = 0.1211\n",
      "epoch 306: loss = 0.0127\n",
      "epoch 307: loss = 0.1268\n",
      "epoch 308: loss = 0.1433\n",
      "epoch 309: loss = 0.0426\n",
      "epoch 310: loss = 0.0831\n",
      "epoch 311: loss = 0.1338\n",
      "epoch 312: loss = 0.1556\n",
      "epoch 313: loss = 0.1156\n",
      "epoch 314: loss = 0.1374\n",
      "epoch 315: loss = 0.0990\n",
      "epoch 316: loss = 0.0946\n",
      "epoch 317: loss = 0.1529\n",
      "epoch 318: loss = 0.1452\n",
      "epoch 319: loss = 0.1731\n",
      "epoch 320: loss = 0.1554\n",
      "epoch 321: loss = 0.1038\n",
      "epoch 322: loss = 0.1625\n",
      "epoch 323: loss = 0.0864\n",
      "epoch 324: loss = 0.0817\n",
      "epoch 325: loss = 0.0535\n",
      "epoch 326: loss = 0.1606\n",
      "epoch 327: loss = 0.1953\n",
      "epoch 328: loss = 0.0879\n",
      "epoch 329: loss = 0.1155\n",
      "epoch 330: loss = 0.0908\n",
      "epoch 331: loss = 0.1160\n",
      "epoch 332: loss = 0.1113\n",
      "epoch 333: loss = 0.1202\n",
      "epoch 334: loss = 0.1446\n",
      "epoch 335: loss = 0.1844\n",
      "epoch 336: loss = 0.1835\n",
      "epoch 337: loss = 0.1337\n",
      "epoch 338: loss = 0.1616\n",
      "epoch 339: loss = 0.1884\n",
      "epoch 340: loss = 0.0552\n",
      "epoch 341: loss = 0.1248\n",
      "epoch 342: loss = 0.0582\n",
      "epoch 343: loss = 0.1105\n",
      "epoch 344: loss = 0.1192\n",
      "epoch 345: loss = 0.1569\n",
      "epoch 346: loss = 0.0864\n",
      "epoch 347: loss = 0.1630\n",
      "epoch 348: loss = 0.0854\n",
      "epoch 349: loss = 0.0690\n",
      "epoch 350: loss = 0.1458\n",
      "epoch 351: loss = 0.1226\n",
      "epoch 352: loss = 0.0877\n",
      "epoch 353: loss = 0.0952\n",
      "epoch 354: loss = 0.1505\n",
      "epoch 355: loss = 0.0735\n",
      "epoch 356: loss = 0.1260\n",
      "epoch 357: loss = 0.0533\n",
      "epoch 358: loss = 0.1396\n",
      "epoch 359: loss = 0.1040\n",
      "epoch 360: loss = 0.0670\n",
      "epoch 361: loss = 0.1599\n",
      "epoch 362: loss = 0.1862\n",
      "epoch 363: loss = 0.0977\n",
      "epoch 364: loss = 0.0907\n",
      "epoch 365: loss = 0.0596\n",
      "epoch 366: loss = 0.1127\n",
      "epoch 367: loss = 0.0894\n",
      "epoch 368: loss = 0.0776\n",
      "epoch 369: loss = 0.0788\n",
      "epoch 370: loss = 0.0906\n",
      "epoch 371: loss = 0.0774\n",
      "epoch 372: loss = 0.0838\n",
      "epoch 373: loss = 0.0746\n",
      "epoch 374: loss = 0.0970\n",
      "epoch 375: loss = 0.0726\n",
      "epoch 376: loss = 0.0918\n",
      "epoch 377: loss = 0.1442\n",
      "epoch 378: loss = 0.1525\n",
      "epoch 379: loss = 0.1124\n",
      "epoch 380: loss = 0.1269\n",
      "epoch 381: loss = 0.1371\n",
      "epoch 382: loss = 0.1893\n",
      "epoch 383: loss = 0.1189\n",
      "epoch 384: loss = 0.1104\n",
      "epoch 385: loss = 0.1401\n",
      "epoch 386: loss = 0.1773\n",
      "epoch 387: loss = 0.0318\n",
      "epoch 388: loss = 0.1142\n",
      "epoch 389: loss = 0.0919\n",
      "epoch 390: loss = 0.1165\n",
      "epoch 391: loss = 0.0797\n",
      "epoch 392: loss = 0.1412\n",
      "epoch 393: loss = 0.1198\n",
      "epoch 394: loss = 0.0591\n",
      "epoch 395: loss = 0.1448\n",
      "epoch 396: loss = 0.0838\n",
      "epoch 397: loss = 0.0907\n",
      "epoch 398: loss = 0.0587\n",
      "epoch 399: loss = 0.0786\n",
      "epoch 400: loss = 0.1522\n",
      "epoch 401: loss = 0.1430\n",
      "epoch 402: loss = 0.0345\n",
      "epoch 403: loss = 0.0607\n",
      "epoch 404: loss = 0.1251\n",
      "epoch 405: loss = 0.1435\n",
      "epoch 406: loss = 0.1306\n",
      "epoch 407: loss = 0.0927\n",
      "epoch 408: loss = 0.0945\n",
      "epoch 409: loss = 0.1335\n",
      "epoch 410: loss = 0.0798\n",
      "epoch 411: loss = 0.1271\n",
      "epoch 412: loss = 0.0812\n",
      "epoch 413: loss = 0.1684\n",
      "epoch 414: loss = 0.0908\n",
      "epoch 415: loss = 0.0762\n",
      "epoch 416: loss = 0.1804\n",
      "epoch 417: loss = 0.1856\n",
      "epoch 418: loss = 0.1131\n",
      "epoch 419: loss = 0.0791\n",
      "epoch 420: loss = 0.0363\n",
      "epoch 421: loss = 0.0767\n",
      "epoch 422: loss = 0.1339\n",
      "epoch 423: loss = 0.1278\n",
      "epoch 424: loss = 0.0941\n",
      "epoch 425: loss = 0.1115\n",
      "epoch 426: loss = 0.1098\n",
      "epoch 427: loss = 0.1092\n",
      "epoch 428: loss = 0.0948\n",
      "epoch 429: loss = 0.1153\n",
      "epoch 430: loss = 0.1145\n",
      "epoch 431: loss = 0.0758\n",
      "epoch 432: loss = 0.1086\n",
      "epoch 433: loss = 0.0799\n",
      "epoch 434: loss = 0.0811\n",
      "epoch 435: loss = 0.0891\n",
      "epoch 436: loss = 0.1179\n",
      "epoch 437: loss = 0.0895\n",
      "epoch 438: loss = 0.1441\n",
      "epoch 439: loss = 0.0825\n",
      "epoch 440: loss = 0.1616\n",
      "epoch 441: loss = 0.1270\n",
      "epoch 442: loss = 0.0407\n",
      "epoch 443: loss = 0.1167\n",
      "epoch 444: loss = 0.1213\n",
      "epoch 445: loss = 0.1323\n",
      "epoch 446: loss = 0.0952\n",
      "epoch 447: loss = 0.1401\n",
      "epoch 448: loss = 0.0914\n",
      "epoch 449: loss = 0.1079\n",
      "epoch 450: loss = 0.1523\n",
      "epoch 451: loss = 0.0561\n",
      "epoch 452: loss = 0.1157\n",
      "epoch 453: loss = 0.0608\n",
      "epoch 454: loss = 0.0927\n",
      "epoch 455: loss = 0.1062\n",
      "epoch 456: loss = 0.0765\n",
      "epoch 457: loss = 0.1145\n",
      "epoch 458: loss = 0.0256\n",
      "epoch 459: loss = 0.1337\n",
      "epoch 460: loss = 0.0995\n",
      "epoch 461: loss = 0.0157\n",
      "epoch 462: loss = 0.1013\n",
      "epoch 463: loss = 0.0831\n",
      "epoch 464: loss = 0.1210\n",
      "epoch 465: loss = 0.1064\n",
      "epoch 466: loss = 0.1119\n",
      "epoch 467: loss = 0.1090\n",
      "epoch 468: loss = 0.1178\n",
      "epoch 469: loss = 0.0761\n",
      "epoch 470: loss = 0.1227\n",
      "epoch 471: loss = 0.0395\n",
      "epoch 472: loss = 0.1020\n",
      "epoch 473: loss = 0.0077\n",
      "epoch 474: loss = 0.0916\n",
      "epoch 475: loss = 0.0859\n",
      "epoch 476: loss = 0.1228\n",
      "epoch 477: loss = 0.1877\n",
      "epoch 478: loss = 0.0376\n",
      "epoch 479: loss = 0.1228\n",
      "epoch 480: loss = 0.1265\n",
      "epoch 481: loss = 0.0751\n",
      "epoch 482: loss = 0.1504\n",
      "epoch 483: loss = 0.1323\n",
      "epoch 484: loss = 0.1045\n",
      "epoch 485: loss = 0.1210\n",
      "epoch 486: loss = 0.1538\n",
      "epoch 487: loss = 0.1023\n",
      "epoch 488: loss = 0.1424\n",
      "epoch 489: loss = 0.1598\n",
      "epoch 490: loss = 0.1242\n",
      "epoch 491: loss = 0.0600\n",
      "epoch 492: loss = 0.1420\n",
      "epoch 493: loss = 0.1006\n",
      "epoch 494: loss = 0.1276\n",
      "epoch 495: loss = 0.1968\n",
      "epoch 496: loss = 0.1130\n",
      "epoch 497: loss = 0.1710\n",
      "epoch 498: loss = 0.0270\n",
      "epoch 499: loss = 0.0749\n",
      "epoch 500: loss = 0.0927\n",
      "epoch 501: loss = 0.0980\n",
      "epoch 502: loss = 0.0498\n",
      "epoch 503: loss = 0.0969\n",
      "epoch 504: loss = 0.1505\n",
      "epoch 505: loss = 0.1705\n",
      "epoch 506: loss = 0.1645\n",
      "epoch 507: loss = 0.1016\n",
      "epoch 508: loss = 0.0798\n",
      "epoch 509: loss = 0.0791\n",
      "epoch 510: loss = 0.1233\n",
      "epoch 511: loss = 0.1297\n",
      "epoch 512: loss = 0.0771\n",
      "epoch 513: loss = 0.1379\n",
      "epoch 514: loss = 0.1857\n",
      "epoch 515: loss = 0.1329\n",
      "epoch 516: loss = 0.0979\n",
      "epoch 517: loss = 0.1146\n",
      "epoch 518: loss = 0.1535\n",
      "epoch 519: loss = 0.1901\n",
      "epoch 520: loss = 0.1357\n",
      "epoch 521: loss = 0.0814\n",
      "epoch 522: loss = 0.0796\n",
      "epoch 523: loss = 0.0243\n",
      "epoch 524: loss = 0.1681\n",
      "epoch 525: loss = 0.1701\n",
      "epoch 526: loss = 0.1517\n",
      "epoch 527: loss = 0.1005\n",
      "epoch 528: loss = 0.1564\n",
      "epoch 529: loss = 0.1247\n",
      "epoch 530: loss = 0.1241\n",
      "epoch 531: loss = 0.1710\n",
      "epoch 532: loss = 0.1244\n",
      "epoch 533: loss = 0.1568\n",
      "epoch 534: loss = 0.1021\n",
      "epoch 535: loss = 0.1067\n",
      "epoch 536: loss = 0.1673\n",
      "epoch 537: loss = 0.1704\n",
      "epoch 538: loss = 0.1077\n",
      "epoch 539: loss = 0.1254\n",
      "epoch 540: loss = 0.1157\n",
      "epoch 541: loss = 0.0796\n",
      "epoch 542: loss = 0.1246\n",
      "epoch 543: loss = 0.0385\n",
      "epoch 544: loss = 0.0552\n",
      "epoch 545: loss = 0.1386\n",
      "epoch 546: loss = 0.1257\n",
      "epoch 547: loss = 0.0972\n",
      "epoch 548: loss = 0.0757\n",
      "epoch 549: loss = 0.0807\n",
      "epoch 550: loss = 0.1009\n",
      "epoch 551: loss = 0.0930\n",
      "epoch 552: loss = 0.1617\n",
      "epoch 553: loss = 0.1726\n",
      "epoch 554: loss = 0.1741\n",
      "epoch 555: loss = 0.0832\n",
      "epoch 556: loss = 0.0958\n",
      "epoch 557: loss = 0.1174\n",
      "epoch 558: loss = 0.0637\n",
      "epoch 559: loss = 0.1105\n",
      "epoch 560: loss = 0.0421\n",
      "epoch 561: loss = 0.1605\n",
      "epoch 562: loss = 0.1675\n",
      "epoch 563: loss = 0.0840\n",
      "epoch 564: loss = 0.0441\n",
      "epoch 565: loss = 0.0985\n",
      "epoch 566: loss = 0.1151\n",
      "epoch 567: loss = 0.1018\n",
      "epoch 568: loss = 0.1951\n",
      "epoch 569: loss = 0.1503\n",
      "epoch 570: loss = 0.1287\n",
      "epoch 571: loss = 0.0967\n",
      "epoch 572: loss = 0.0921\n",
      "epoch 573: loss = 0.1041\n",
      "epoch 574: loss = 0.0640\n",
      "epoch 575: loss = 0.1589\n",
      "epoch 576: loss = 0.1865\n",
      "epoch 577: loss = 0.0921\n",
      "epoch 578: loss = 0.0907\n",
      "epoch 579: loss = 0.1566\n",
      "epoch 580: loss = 0.1016\n",
      "epoch 581: loss = 0.1168\n",
      "epoch 582: loss = 0.0650\n",
      "epoch 583: loss = 0.1021\n",
      "epoch 584: loss = 0.1326\n",
      "epoch 585: loss = 0.1286\n",
      "epoch 586: loss = 0.0238\n",
      "epoch 587: loss = 0.0631\n",
      "epoch 588: loss = 0.0907\n",
      "epoch 589: loss = 0.1065\n",
      "epoch 590: loss = 0.0754\n",
      "epoch 591: loss = 0.0880\n",
      "epoch 592: loss = 0.1272\n",
      "epoch 593: loss = 0.1267\n",
      "epoch 594: loss = 0.1535\n",
      "epoch 595: loss = 0.1193\n",
      "epoch 596: loss = 0.0715\n",
      "epoch 597: loss = 0.1188\n",
      "epoch 598: loss = 0.1692\n",
      "epoch 599: loss = 0.1733\n",
      "epoch 600: loss = 0.1043\n",
      "epoch 601: loss = 0.0217\n",
      "epoch 602: loss = 0.0830\n",
      "epoch 603: loss = 0.1471\n",
      "epoch 604: loss = 0.1130\n",
      "epoch 605: loss = 0.0710\n",
      "epoch 606: loss = 0.1364\n",
      "epoch 607: loss = 0.1153\n",
      "epoch 608: loss = 0.1860\n",
      "epoch 609: loss = 0.0131\n",
      "epoch 610: loss = 0.1238\n",
      "epoch 611: loss = 0.1242\n",
      "epoch 612: loss = 0.0719\n",
      "epoch 613: loss = 0.0574\n",
      "epoch 614: loss = 0.1022\n",
      "epoch 615: loss = 0.1674\n",
      "epoch 616: loss = 0.1736\n",
      "epoch 617: loss = 0.1215\n",
      "epoch 618: loss = 0.0790\n",
      "epoch 619: loss = 0.0501\n",
      "epoch 620: loss = 0.0782\n",
      "epoch 621: loss = 0.0771\n",
      "epoch 622: loss = 0.0856\n",
      "epoch 623: loss = 0.0913\n",
      "epoch 624: loss = 0.1341\n",
      "epoch 625: loss = 0.1769\n",
      "epoch 626: loss = 0.0542\n",
      "epoch 627: loss = 0.0395\n",
      "epoch 628: loss = 0.1881\n",
      "epoch 629: loss = 0.1334\n",
      "epoch 630: loss = 0.1554\n",
      "epoch 631: loss = 0.1037\n",
      "epoch 632: loss = 0.0934\n",
      "epoch 633: loss = 0.0975\n",
      "epoch 634: loss = 0.0503\n",
      "epoch 635: loss = 0.1521\n",
      "epoch 636: loss = 0.1738\n",
      "epoch 637: loss = 0.1718\n",
      "epoch 638: loss = 0.1418\n",
      "epoch 639: loss = 0.0813\n",
      "epoch 640: loss = 0.0889\n",
      "epoch 641: loss = 0.0808\n",
      "epoch 642: loss = 0.1799\n",
      "epoch 643: loss = 0.1436\n",
      "epoch 644: loss = 0.1515\n",
      "epoch 645: loss = 0.1645\n",
      "epoch 646: loss = 0.0792\n",
      "epoch 647: loss = 0.0415\n",
      "epoch 648: loss = 0.0965\n",
      "epoch 649: loss = 0.1074\n",
      "epoch 650: loss = 0.1454\n",
      "epoch 651: loss = 0.1066\n",
      "epoch 652: loss = 0.0812\n",
      "epoch 653: loss = 0.0949\n",
      "epoch 654: loss = 0.0905\n",
      "epoch 655: loss = 0.0828\n",
      "epoch 656: loss = 0.1153\n",
      "epoch 657: loss = 0.1076\n",
      "epoch 658: loss = 0.1268\n",
      "epoch 659: loss = 0.1143\n",
      "epoch 660: loss = 0.0753\n",
      "epoch 661: loss = 0.0789\n",
      "epoch 662: loss = 0.1403\n",
      "epoch 663: loss = 0.1799\n",
      "epoch 664: loss = 0.1715\n",
      "epoch 665: loss = 0.0615\n",
      "epoch 666: loss = 0.0195\n",
      "epoch 667: loss = 0.1038\n",
      "epoch 668: loss = 0.1836\n",
      "epoch 669: loss = 0.1237\n",
      "epoch 670: loss = 0.0607\n",
      "epoch 671: loss = 0.0554\n",
      "epoch 672: loss = 0.0746\n",
      "epoch 673: loss = 0.0631\n",
      "epoch 674: loss = 0.1009\n",
      "epoch 675: loss = 0.0378\n",
      "epoch 676: loss = 0.0770\n",
      "epoch 677: loss = 0.1049\n",
      "epoch 678: loss = 0.0769\n",
      "epoch 679: loss = 0.1360\n",
      "epoch 680: loss = 0.1584\n",
      "epoch 681: loss = 0.0608\n",
      "epoch 682: loss = 0.1101\n",
      "epoch 683: loss = 0.0897\n",
      "epoch 684: loss = 0.0666\n",
      "epoch 685: loss = 0.0439\n",
      "epoch 686: loss = 0.0341\n",
      "epoch 687: loss = 0.0593\n",
      "epoch 688: loss = 0.0335\n",
      "epoch 689: loss = 0.0290\n",
      "epoch 690: loss = 0.1481\n",
      "epoch 691: loss = 0.0731\n",
      "epoch 692: loss = 0.0130\n",
      "epoch 693: loss = 0.0555\n",
      "epoch 694: loss = 0.1723\n",
      "epoch 695: loss = 0.1522\n",
      "epoch 696: loss = 0.1233\n",
      "epoch 697: loss = 0.1433\n",
      "epoch 698: loss = 0.0961\n",
      "epoch 699: loss = 0.0868\n",
      "epoch 700: loss = 0.1131\n",
      "epoch 701: loss = 0.1246\n",
      "epoch 702: loss = 0.0982\n",
      "epoch 703: loss = 0.0679\n",
      "epoch 704: loss = 0.1026\n",
      "epoch 705: loss = 0.1726\n",
      "epoch 706: loss = 0.0703\n",
      "epoch 707: loss = 0.0749\n",
      "epoch 708: loss = 0.0626\n",
      "epoch 709: loss = 0.0931\n",
      "epoch 710: loss = 0.0152\n",
      "epoch 711: loss = 0.0391\n",
      "epoch 712: loss = 0.1498\n",
      "epoch 713: loss = 0.0885\n",
      "epoch 714: loss = 0.1274\n",
      "epoch 715: loss = 0.0699\n",
      "epoch 716: loss = 0.1293\n",
      "epoch 717: loss = 0.0667\n",
      "epoch 718: loss = 0.0877\n",
      "epoch 719: loss = 0.0603\n",
      "epoch 720: loss = 0.0883\n",
      "epoch 721: loss = 0.1343\n",
      "epoch 722: loss = 0.1527\n",
      "epoch 723: loss = 0.1239\n",
      "epoch 724: loss = 0.1122\n",
      "epoch 725: loss = 0.0796\n",
      "epoch 726: loss = 0.1774\n",
      "epoch 727: loss = 0.1401\n",
      "epoch 728: loss = 0.0612\n",
      "epoch 729: loss = 0.0698\n",
      "epoch 730: loss = 0.0399\n",
      "epoch 731: loss = 0.0589\n",
      "epoch 732: loss = 0.0783\n",
      "epoch 733: loss = 0.0932\n",
      "epoch 734: loss = 0.1385\n",
      "epoch 735: loss = 0.1021\n",
      "epoch 736: loss = 0.0721\n",
      "epoch 737: loss = 0.0729\n",
      "epoch 738: loss = 0.1559\n",
      "epoch 739: loss = 0.1634\n",
      "epoch 740: loss = 0.0398\n",
      "epoch 741: loss = 0.1435\n",
      "epoch 742: loss = 0.1496\n",
      "epoch 743: loss = 0.1167\n",
      "epoch 744: loss = 0.1154\n",
      "epoch 745: loss = 0.0900\n",
      "epoch 746: loss = 0.0741\n",
      "epoch 747: loss = 0.1143\n",
      "epoch 748: loss = 0.1118\n",
      "epoch 749: loss = 0.1467\n",
      "epoch 750: loss = 0.1665\n",
      "epoch 751: loss = 0.1415\n",
      "epoch 752: loss = 0.1311\n",
      "epoch 753: loss = 0.0962\n",
      "epoch 754: loss = 0.1693\n",
      "epoch 755: loss = 0.1391\n",
      "epoch 756: loss = 0.1907\n",
      "epoch 757: loss = 0.1438\n",
      "epoch 758: loss = 0.1421\n",
      "epoch 759: loss = 0.0989\n",
      "epoch 760: loss = 0.1678\n",
      "epoch 761: loss = 0.0980\n",
      "epoch 762: loss = 0.1224\n",
      "epoch 763: loss = 0.1265\n",
      "epoch 764: loss = 0.1519\n",
      "epoch 765: loss = 0.1194\n",
      "epoch 766: loss = 0.0746\n",
      "epoch 767: loss = 0.1018\n",
      "epoch 768: loss = 0.0482\n",
      "epoch 769: loss = 0.1237\n",
      "epoch 770: loss = 0.0752\n",
      "epoch 771: loss = 0.1111\n",
      "epoch 772: loss = 0.1680\n",
      "epoch 773: loss = 0.1306\n",
      "epoch 774: loss = 0.0486\n",
      "epoch 775: loss = 0.1130\n",
      "epoch 776: loss = 0.1072\n",
      "epoch 777: loss = 0.0948\n",
      "epoch 778: loss = 0.1093\n",
      "epoch 779: loss = 0.1298\n",
      "epoch 780: loss = 0.1210\n",
      "epoch 781: loss = 0.0656\n",
      "epoch 782: loss = 0.0391\n",
      "epoch 783: loss = 0.0715\n",
      "epoch 784: loss = 0.0676\n",
      "epoch 785: loss = 0.1287\n",
      "epoch 786: loss = 0.1202\n",
      "epoch 787: loss = 0.1003\n",
      "epoch 788: loss = 0.1231\n",
      "epoch 789: loss = 0.1596\n",
      "epoch 790: loss = 0.1257\n",
      "epoch 791: loss = 0.1364\n",
      "epoch 792: loss = 0.1618\n",
      "epoch 793: loss = 0.1358\n",
      "epoch 794: loss = 0.1094\n",
      "epoch 795: loss = 0.0745\n",
      "epoch 796: loss = 0.1011\n",
      "epoch 797: loss = 0.0798\n",
      "epoch 798: loss = 0.1377\n",
      "epoch 799: loss = 0.1405\n",
      "epoch 800: loss = 0.1150\n",
      "epoch 801: loss = 0.1259\n",
      "epoch 802: loss = 0.1273\n",
      "epoch 803: loss = 0.1600\n",
      "epoch 804: loss = 0.0452\n",
      "epoch 805: loss = 0.0683\n",
      "epoch 806: loss = 0.1530\n",
      "epoch 807: loss = 0.1691\n",
      "epoch 808: loss = 0.1009\n",
      "epoch 809: loss = 0.1097\n",
      "epoch 810: loss = 0.1580\n",
      "epoch 811: loss = 0.0677\n",
      "epoch 812: loss = 0.0670\n",
      "epoch 813: loss = 0.0901\n",
      "epoch 814: loss = 0.1050\n",
      "epoch 815: loss = 0.1615\n",
      "epoch 816: loss = 0.1488\n",
      "epoch 817: loss = 0.1046\n",
      "epoch 818: loss = 0.1754\n",
      "epoch 819: loss = 0.1345\n",
      "epoch 820: loss = 0.0966\n",
      "epoch 821: loss = 0.0660\n",
      "epoch 822: loss = 0.0363\n",
      "epoch 823: loss = 0.1287\n",
      "epoch 824: loss = 0.0708\n",
      "epoch 825: loss = 0.1299\n",
      "epoch 826: loss = 0.0893\n",
      "epoch 827: loss = 0.1031\n",
      "epoch 828: loss = 0.1653\n",
      "epoch 829: loss = 0.1952\n",
      "epoch 830: loss = 0.1098\n",
      "epoch 831: loss = 0.1520\n",
      "epoch 832: loss = 0.1032\n",
      "epoch 833: loss = 0.1084\n",
      "epoch 834: loss = 0.0644\n",
      "epoch 835: loss = 0.1272\n",
      "epoch 836: loss = 0.1006\n",
      "epoch 837: loss = 0.1039\n",
      "epoch 838: loss = 0.1625\n",
      "epoch 839: loss = 0.1658\n",
      "epoch 840: loss = 0.0991\n",
      "epoch 841: loss = 0.0898\n",
      "epoch 842: loss = 0.1345\n",
      "epoch 843: loss = 0.0978\n",
      "epoch 844: loss = 0.1043\n",
      "epoch 845: loss = 0.1301\n",
      "epoch 846: loss = 0.1242\n",
      "epoch 847: loss = 0.1141\n",
      "epoch 848: loss = 0.0989\n",
      "epoch 849: loss = 0.1722\n",
      "epoch 850: loss = 0.1474\n",
      "epoch 851: loss = 0.0758\n",
      "epoch 852: loss = 0.1641\n",
      "epoch 853: loss = 0.1910\n",
      "epoch 854: loss = 0.1169\n",
      "epoch 855: loss = 0.1690\n",
      "epoch 856: loss = 0.1404\n",
      "epoch 857: loss = 0.1253\n",
      "epoch 858: loss = 0.0980\n",
      "epoch 859: loss = 0.1750\n",
      "epoch 860: loss = 0.1647\n",
      "epoch 861: loss = 0.1057\n",
      "epoch 862: loss = 0.1100\n",
      "epoch 863: loss = 0.1038\n",
      "epoch 864: loss = 0.0966\n",
      "epoch 865: loss = 0.1081\n",
      "epoch 866: loss = 0.0166\n",
      "epoch 867: loss = 0.0976\n",
      "epoch 868: loss = 0.0705\n",
      "epoch 869: loss = 0.0247\n",
      "epoch 870: loss = 0.1274\n",
      "epoch 871: loss = 0.1881\n",
      "epoch 872: loss = 0.1100\n",
      "epoch 873: loss = 0.1103\n",
      "epoch 874: loss = 0.1148\n",
      "epoch 875: loss = 0.0451\n",
      "epoch 876: loss = 0.0885\n",
      "epoch 877: loss = 0.0684\n",
      "epoch 878: loss = 0.0923\n",
      "epoch 879: loss = 0.1481\n",
      "epoch 880: loss = 0.1150\n",
      "epoch 881: loss = 0.1305\n",
      "epoch 882: loss = 0.1754\n",
      "epoch 883: loss = 0.1819\n",
      "epoch 884: loss = 0.1671\n",
      "epoch 885: loss = 0.1129\n",
      "epoch 886: loss = 0.0391\n",
      "epoch 887: loss = 0.1722\n",
      "epoch 888: loss = 0.1821\n",
      "epoch 889: loss = 0.1096\n",
      "epoch 890: loss = 0.0338\n",
      "epoch 891: loss = 0.1028\n",
      "epoch 892: loss = 0.1681\n",
      "epoch 893: loss = 0.1025\n",
      "epoch 894: loss = 0.0453\n",
      "epoch 895: loss = 0.0523\n",
      "epoch 896: loss = 0.0755\n",
      "epoch 897: loss = 0.1757\n",
      "epoch 898: loss = 0.1811\n",
      "epoch 899: loss = 0.1644\n",
      "epoch 900: loss = 0.0188\n",
      "epoch 901: loss = 0.0483\n",
      "epoch 902: loss = 0.1423\n",
      "epoch 903: loss = 0.1292\n",
      "epoch 904: loss = 0.1177\n",
      "epoch 905: loss = 0.0545\n",
      "epoch 906: loss = 0.1704\n",
      "epoch 907: loss = 0.1372\n",
      "epoch 908: loss = 0.1708\n",
      "epoch 909: loss = 0.1666\n",
      "epoch 910: loss = 0.1001\n",
      "epoch 911: loss = 0.0370\n",
      "epoch 912: loss = 0.0113\n",
      "epoch 913: loss = 0.0750\n",
      "epoch 914: loss = 0.1186\n",
      "epoch 915: loss = 0.1031\n",
      "epoch 916: loss = 0.1800\n",
      "epoch 917: loss = 0.1484\n",
      "epoch 918: loss = 0.0716\n",
      "epoch 919: loss = 0.1464\n",
      "epoch 920: loss = 0.1255\n",
      "epoch 921: loss = 0.1856\n",
      "epoch 922: loss = 0.1538\n",
      "epoch 923: loss = 0.1148\n",
      "epoch 924: loss = 0.1770\n",
      "epoch 925: loss = 0.1068\n",
      "epoch 926: loss = 0.1285\n",
      "epoch 927: loss = 0.0723\n",
      "epoch 928: loss = 0.0922\n",
      "epoch 929: loss = 0.1295\n",
      "epoch 930: loss = 0.1109\n",
      "epoch 931: loss = 0.1208\n",
      "epoch 932: loss = 0.0827\n",
      "epoch 933: loss = 0.0470\n",
      "epoch 934: loss = 0.0040\n",
      "epoch 935: loss = 0.0115\n",
      "epoch 936: loss = 0.1189\n",
      "epoch 937: loss = 0.0892\n",
      "epoch 938: loss = 0.0424\n",
      "epoch 939: loss = 0.0347\n",
      "epoch 940: loss = 0.0709\n",
      "epoch 941: loss = 0.1002\n",
      "epoch 942: loss = 0.1434\n",
      "epoch 943: loss = 0.0827\n",
      "epoch 944: loss = 0.1220\n",
      "epoch 945: loss = 0.1247\n",
      "epoch 946: loss = 0.1055\n",
      "epoch 947: loss = 0.1203\n",
      "epoch 948: loss = 0.1418\n",
      "epoch 949: loss = 0.1332\n",
      "epoch 950: loss = 0.1524\n",
      "epoch 951: loss = 0.1382\n",
      "epoch 952: loss = 0.1105\n",
      "epoch 953: loss = 0.1016\n",
      "epoch 954: loss = 0.1120\n",
      "epoch 955: loss = 0.0749\n",
      "epoch 956: loss = 0.0679\n",
      "epoch 957: loss = 0.1387\n",
      "epoch 958: loss = 0.1784\n",
      "epoch 959: loss = 0.1368\n",
      "epoch 960: loss = 0.1600\n",
      "epoch 961: loss = 0.1442\n",
      "epoch 962: loss = 0.1109\n",
      "epoch 963: loss = 0.1162\n",
      "epoch 964: loss = 0.0620\n",
      "epoch 965: loss = 0.1467\n",
      "epoch 966: loss = 0.1352\n",
      "epoch 967: loss = 0.0729\n",
      "epoch 968: loss = 0.1126\n",
      "epoch 969: loss = 0.1278\n",
      "epoch 970: loss = 0.0709\n",
      "epoch 971: loss = 0.0547\n",
      "epoch 972: loss = 0.1188\n",
      "epoch 973: loss = 0.0902\n",
      "epoch 974: loss = 0.1284\n",
      "epoch 975: loss = 0.0608\n",
      "epoch 976: loss = 0.1136\n",
      "epoch 977: loss = 0.1265\n",
      "epoch 978: loss = 0.0788\n",
      "epoch 979: loss = 0.1300\n",
      "epoch 980: loss = 0.1319\n",
      "epoch 981: loss = 0.0943\n",
      "epoch 982: loss = 0.1034\n",
      "epoch 983: loss = 0.1087\n",
      "epoch 984: loss = 0.0174\n",
      "epoch 985: loss = 0.0427\n",
      "epoch 986: loss = 0.0986\n",
      "epoch 987: loss = 0.0880\n",
      "epoch 988: loss = 0.1175\n",
      "epoch 989: loss = 0.1263\n",
      "epoch 990: loss = 0.0537\n",
      "epoch 991: loss = 0.0481\n",
      "epoch 992: loss = 0.0790\n",
      "epoch 993: loss = 0.0749\n",
      "epoch 994: loss = 0.1276\n",
      "epoch 995: loss = 0.1136\n",
      "epoch 996: loss = 0.0576\n",
      "epoch 997: loss = 0.1004\n",
      "epoch 998: loss = 0.1229\n",
      "epoch 999: loss = 0.1462\n",
      "epoch 1000: loss = 0.1482\n",
      "epoch 1001: loss = 0.0849\n",
      "epoch 1002: loss = 0.0597\n",
      "epoch 1003: loss = 0.0921\n",
      "epoch 1004: loss = 0.1796\n",
      "epoch 1005: loss = 0.0817\n",
      "epoch 1006: loss = 0.1655\n",
      "epoch 1007: loss = 0.1717\n",
      "epoch 1008: loss = 0.1249\n",
      "epoch 1009: loss = 0.0489\n",
      "epoch 1010: loss = 0.1822\n",
      "epoch 1011: loss = 0.1830\n",
      "epoch 1012: loss = 0.1305\n",
      "epoch 1013: loss = 0.1012\n",
      "epoch 1014: loss = 0.1048\n",
      "epoch 1015: loss = 0.1509\n",
      "epoch 1016: loss = 0.0782\n",
      "epoch 1017: loss = 0.0717\n",
      "epoch 1018: loss = 0.0987\n",
      "epoch 1019: loss = 0.1292\n",
      "epoch 1020: loss = 0.1207\n",
      "epoch 1021: loss = 0.1250\n",
      "epoch 1022: loss = 0.1050\n",
      "epoch 1023: loss = 0.0481\n",
      "epoch 1024: loss = 0.0911\n",
      "epoch 1025: loss = 0.0932\n",
      "epoch 1026: loss = 0.1125\n",
      "epoch 1027: loss = 0.0509\n",
      "epoch 1028: loss = 0.0545\n",
      "epoch 1029: loss = 0.0610\n",
      "epoch 1030: loss = 0.0677\n",
      "epoch 1031: loss = 0.0553\n",
      "epoch 1032: loss = 0.1256\n",
      "epoch 1033: loss = 0.0889\n",
      "epoch 1034: loss = 0.1928\n",
      "epoch 1035: loss = 0.1217\n",
      "epoch 1036: loss = 0.0737\n",
      "epoch 1037: loss = 0.0834\n",
      "epoch 1038: loss = 0.0978\n",
      "epoch 1039: loss = 0.0875\n",
      "epoch 1040: loss = 0.0624\n",
      "epoch 1041: loss = 0.0805\n",
      "epoch 1042: loss = 0.0600\n",
      "epoch 1043: loss = 0.1010\n",
      "epoch 1044: loss = 0.1225\n",
      "epoch 1045: loss = 0.1018\n",
      "epoch 1046: loss = 0.0560\n",
      "epoch 1047: loss = 0.1019\n",
      "epoch 1048: loss = 0.0077\n",
      "epoch 1049: loss = 0.1458\n",
      "epoch 1050: loss = 0.0602\n",
      "epoch 1051: loss = 0.0933\n",
      "epoch 1052: loss = 0.0901\n",
      "epoch 1053: loss = 0.0370\n",
      "epoch 1054: loss = 0.1540\n",
      "epoch 1055: loss = 0.1147\n",
      "epoch 1056: loss = 0.1180\n",
      "epoch 1057: loss = 0.1875\n",
      "epoch 1058: loss = 0.1734\n",
      "epoch 1059: loss = 0.0924\n",
      "epoch 1060: loss = 0.1510\n",
      "epoch 1061: loss = 0.0392\n",
      "epoch 1062: loss = 0.1299\n",
      "epoch 1063: loss = 0.0915\n",
      "epoch 1064: loss = 0.1794\n",
      "epoch 1065: loss = 0.1216\n",
      "epoch 1066: loss = 0.0381\n",
      "epoch 1067: loss = 0.0609\n",
      "epoch 1068: loss = 0.1334\n",
      "epoch 1069: loss = 0.1033\n",
      "epoch 1070: loss = 0.1000\n",
      "epoch 1071: loss = 0.1358\n",
      "epoch 1072: loss = 0.0579\n",
      "epoch 1073: loss = 0.0468\n",
      "epoch 1074: loss = 0.0383\n",
      "epoch 1075: loss = 0.1258\n",
      "epoch 1076: loss = 0.0550\n",
      "epoch 1077: loss = 0.1065\n",
      "epoch 1078: loss = 0.0260\n",
      "epoch 1079: loss = 0.1489\n",
      "epoch 1080: loss = 0.1437\n",
      "epoch 1081: loss = 0.1315\n",
      "epoch 1082: loss = 0.1390\n",
      "epoch 1083: loss = 0.0981\n",
      "epoch 1084: loss = 0.1680\n",
      "epoch 1085: loss = 0.1391\n",
      "epoch 1086: loss = 0.1308\n",
      "epoch 1087: loss = 0.1758\n",
      "epoch 1088: loss = 0.0792\n",
      "epoch 1089: loss = 0.1105\n",
      "epoch 1090: loss = 0.0418\n",
      "epoch 1091: loss = 0.0915\n",
      "epoch 1092: loss = 0.0551\n",
      "epoch 1093: loss = 0.1426\n",
      "epoch 1094: loss = 0.1249\n",
      "epoch 1095: loss = 0.0772\n",
      "epoch 1096: loss = 0.1466\n",
      "epoch 1097: loss = 0.1015\n",
      "epoch 1098: loss = 0.1042\n",
      "epoch 1099: loss = 0.1535\n",
      "epoch 1100: loss = 0.0885\n",
      "epoch 1101: loss = 0.0969\n",
      "epoch 1102: loss = 0.1355\n",
      "epoch 1103: loss = 0.1122\n",
      "epoch 1104: loss = 0.1338\n",
      "epoch 1105: loss = 0.1488\n",
      "epoch 1106: loss = 0.0832\n",
      "epoch 1107: loss = 0.1076\n",
      "epoch 1108: loss = 0.1485\n",
      "epoch 1109: loss = 0.1289\n",
      "epoch 1110: loss = 0.1502\n",
      "epoch 1111: loss = 0.1488\n",
      "epoch 1112: loss = 0.1203\n",
      "epoch 1113: loss = 0.1367\n",
      "epoch 1114: loss = 0.1184\n",
      "epoch 1115: loss = 0.1072\n",
      "epoch 1116: loss = 0.1459\n",
      "epoch 1117: loss = 0.1616\n",
      "epoch 1118: loss = 0.1157\n",
      "epoch 1119: loss = 0.0903\n",
      "epoch 1120: loss = 0.1432\n",
      "epoch 1121: loss = 0.1495\n",
      "epoch 1122: loss = 0.0942\n",
      "epoch 1123: loss = 0.0853\n",
      "epoch 1124: loss = 0.0245\n",
      "epoch 1125: loss = 0.1042\n",
      "epoch 1126: loss = 0.1054\n",
      "epoch 1127: loss = 0.1269\n",
      "epoch 1128: loss = 0.0480\n",
      "epoch 1129: loss = 0.0931\n",
      "epoch 1130: loss = 0.0792\n",
      "epoch 1131: loss = 0.0878\n",
      "epoch 1132: loss = 0.1421\n",
      "epoch 1133: loss = 0.1070\n",
      "epoch 1134: loss = 0.0515\n",
      "epoch 1135: loss = 0.0262\n",
      "epoch 1136: loss = 0.1341\n",
      "epoch 1137: loss = 0.0934\n",
      "epoch 1138: loss = 0.0266\n",
      "epoch 1139: loss = 0.1349\n",
      "epoch 1140: loss = 0.1497\n",
      "epoch 1141: loss = 0.0477\n",
      "epoch 1142: loss = 0.1703\n",
      "epoch 1143: loss = 0.1088\n",
      "epoch 1144: loss = 0.1368\n",
      "epoch 1145: loss = 0.0998\n",
      "epoch 1146: loss = 0.1315\n",
      "epoch 1147: loss = 0.0928\n",
      "epoch 1148: loss = 0.1381\n",
      "epoch 1149: loss = 0.1198\n",
      "epoch 1150: loss = 0.0860\n",
      "epoch 1151: loss = 0.1481\n",
      "epoch 1152: loss = 0.0986\n",
      "epoch 1153: loss = 0.1779\n",
      "epoch 1154: loss = 0.1105\n",
      "epoch 1155: loss = 0.1074\n",
      "epoch 1156: loss = 0.0308\n",
      "epoch 1157: loss = 0.1672\n",
      "epoch 1158: loss = 0.1157\n",
      "epoch 1159: loss = 0.1470\n",
      "epoch 1160: loss = 0.1488\n",
      "epoch 1161: loss = 0.1240\n",
      "epoch 1162: loss = 0.1191\n",
      "epoch 1163: loss = 0.1776\n",
      "epoch 1164: loss = 0.1417\n",
      "epoch 1165: loss = 0.0284\n",
      "epoch 1166: loss = 0.1663\n",
      "epoch 1167: loss = 0.1413\n",
      "epoch 1168: loss = 0.1002\n",
      "epoch 1169: loss = 0.0950\n",
      "epoch 1170: loss = 0.1355\n",
      "epoch 1171: loss = 0.1288\n",
      "epoch 1172: loss = 0.1796\n",
      "epoch 1173: loss = 0.1031\n",
      "epoch 1174: loss = 0.1077\n",
      "epoch 1175: loss = 0.1795\n",
      "epoch 1176: loss = 0.0724\n",
      "epoch 1177: loss = 0.1352\n",
      "epoch 1178: loss = 0.1752\n",
      "epoch 1179: loss = 0.0905\n",
      "epoch 1180: loss = 0.1027\n",
      "epoch 1181: loss = 0.0962\n",
      "epoch 1182: loss = 0.1301\n",
      "epoch 1183: loss = 0.0927\n",
      "epoch 1184: loss = 0.1004\n",
      "epoch 1185: loss = 0.0908\n",
      "epoch 1186: loss = 0.0522\n",
      "epoch 1187: loss = 0.0923\n",
      "epoch 1188: loss = 0.0756\n",
      "epoch 1189: loss = 0.1280\n",
      "epoch 1190: loss = 0.0862\n",
      "epoch 1191: loss = 0.1047\n",
      "epoch 1192: loss = 0.0998\n",
      "epoch 1193: loss = 0.0162\n",
      "epoch 1194: loss = 0.1494\n",
      "epoch 1195: loss = 0.1089\n",
      "epoch 1196: loss = 0.0791\n",
      "epoch 1197: loss = 0.1062\n",
      "epoch 1198: loss = 0.1471\n",
      "epoch 1199: loss = 0.0892\n",
      "epoch 1200: loss = 0.1845\n",
      "epoch 1201: loss = 0.1658\n",
      "epoch 1202: loss = 0.1125\n",
      "epoch 1203: loss = 0.1084\n",
      "epoch 1204: loss = 0.0061\n",
      "epoch 1205: loss = 0.0979\n",
      "epoch 1206: loss = 0.1741\n",
      "epoch 1207: loss = 0.2048\n",
      "epoch 1208: loss = 0.1995\n",
      "epoch 1209: loss = 0.0980\n",
      "epoch 1210: loss = 0.1379\n",
      "epoch 1211: loss = 0.0766\n",
      "epoch 1212: loss = 0.1323\n",
      "epoch 1213: loss = 0.1671\n",
      "epoch 1214: loss = 0.0933\n",
      "epoch 1215: loss = 0.1081\n",
      "epoch 1216: loss = 0.1359\n",
      "epoch 1217: loss = 0.1398\n",
      "epoch 1218: loss = 0.0812\n",
      "epoch 1219: loss = 0.1299\n",
      "epoch 1220: loss = 0.1438\n",
      "epoch 1221: loss = 0.1736\n",
      "epoch 1222: loss = 0.0984\n",
      "epoch 1223: loss = 0.1396\n",
      "epoch 1224: loss = 0.0276\n",
      "epoch 1225: loss = 0.1648\n",
      "epoch 1226: loss = 0.1092\n",
      "epoch 1227: loss = 0.1183\n",
      "epoch 1228: loss = 0.0803\n",
      "epoch 1229: loss = 0.0823\n",
      "epoch 1230: loss = 0.1296\n",
      "epoch 1231: loss = 0.1419\n",
      "epoch 1232: loss = 0.0397\n",
      "epoch 1233: loss = 0.1052\n",
      "epoch 1234: loss = 0.0974\n",
      "epoch 1235: loss = 0.1327\n",
      "epoch 1236: loss = 0.0854\n",
      "epoch 1237: loss = 0.1592\n",
      "epoch 1238: loss = 0.1342\n",
      "epoch 1239: loss = 0.0669\n",
      "epoch 1240: loss = 0.1209\n",
      "epoch 1241: loss = 0.1290\n",
      "epoch 1242: loss = 0.1615\n",
      "epoch 1243: loss = 0.1411\n",
      "epoch 1244: loss = 0.1001\n",
      "epoch 1245: loss = 0.1253\n",
      "epoch 1246: loss = 0.1853\n",
      "epoch 1247: loss = 0.1716\n",
      "epoch 1248: loss = 0.1151\n",
      "epoch 1249: loss = 0.0843\n",
      "epoch 1250: loss = 0.1406\n",
      "epoch 1251: loss = 0.1087\n",
      "epoch 1252: loss = 0.1044\n",
      "epoch 1253: loss = 0.0760\n",
      "epoch 1254: loss = 0.0236\n",
      "epoch 1255: loss = 0.1199\n",
      "epoch 1256: loss = 0.1828\n",
      "epoch 1257: loss = 0.1126\n",
      "epoch 1258: loss = 0.1417\n",
      "epoch 1259: loss = 0.0661\n",
      "epoch 1260: loss = 0.1324\n",
      "epoch 1261: loss = 0.0256\n",
      "epoch 1262: loss = 0.0804\n",
      "epoch 1263: loss = 0.1805\n",
      "epoch 1264: loss = 0.1162\n",
      "epoch 1265: loss = 0.1045\n",
      "epoch 1266: loss = 0.1618\n",
      "epoch 1267: loss = 0.1544\n",
      "epoch 1268: loss = 0.1149\n",
      "epoch 1269: loss = 0.1142\n",
      "epoch 1270: loss = 0.0791\n",
      "epoch 1271: loss = 0.1044\n",
      "epoch 1272: loss = 0.0780\n",
      "epoch 1273: loss = 0.0452\n",
      "epoch 1274: loss = 0.1375\n",
      "epoch 1275: loss = 0.1311\n",
      "epoch 1276: loss = 0.1235\n",
      "epoch 1277: loss = 0.0437\n",
      "epoch 1278: loss = 0.0737\n",
      "epoch 1279: loss = 0.0912\n",
      "epoch 1280: loss = 0.0884\n",
      "epoch 1281: loss = 0.1051\n",
      "epoch 1282: loss = 0.1034\n",
      "epoch 1283: loss = 0.1195\n",
      "epoch 1284: loss = 0.1021\n",
      "epoch 1285: loss = 0.1468\n",
      "epoch 1286: loss = 0.1697\n",
      "epoch 1287: loss = 0.0837\n",
      "epoch 1288: loss = 0.1693\n",
      "epoch 1289: loss = 0.0992\n",
      "epoch 1290: loss = 0.0554\n",
      "epoch 1291: loss = 0.0635\n",
      "epoch 1292: loss = 0.0933\n",
      "epoch 1293: loss = 0.1311\n",
      "epoch 1294: loss = 0.0953\n",
      "epoch 1295: loss = 0.0851\n",
      "epoch 1296: loss = 0.0203\n",
      "epoch 1297: loss = 0.1880\n",
      "epoch 1298: loss = 0.0782\n",
      "epoch 1299: loss = 0.1231\n",
      "epoch 1300: loss = 0.0810\n",
      "epoch 1301: loss = 0.0927\n",
      "epoch 1302: loss = 0.1158\n",
      "epoch 1303: loss = 0.0809\n",
      "epoch 1304: loss = 0.0798\n",
      "epoch 1305: loss = 0.0999\n",
      "epoch 1306: loss = 0.1355\n",
      "epoch 1307: loss = 0.1028\n",
      "epoch 1308: loss = 0.1528\n",
      "epoch 1309: loss = 0.1343\n",
      "epoch 1310: loss = 0.0922\n",
      "epoch 1311: loss = 0.1365\n",
      "epoch 1312: loss = 0.1111\n",
      "epoch 1313: loss = 0.0371\n",
      "epoch 1314: loss = 0.1309\n",
      "epoch 1315: loss = 0.1344\n",
      "epoch 1316: loss = 0.1542\n",
      "epoch 1317: loss = 0.1231\n",
      "epoch 1318: loss = 0.0481\n",
      "epoch 1319: loss = 0.1166\n",
      "epoch 1320: loss = 0.0907\n",
      "epoch 1321: loss = 0.0985\n",
      "epoch 1322: loss = 0.0678\n",
      "epoch 1323: loss = 0.0791\n",
      "epoch 1324: loss = 0.1331\n",
      "epoch 1325: loss = 0.1896\n",
      "epoch 1326: loss = 0.0668\n",
      "epoch 1327: loss = 0.1475\n",
      "epoch 1328: loss = 0.0498\n",
      "epoch 1329: loss = 0.0317\n",
      "epoch 1330: loss = 0.1001\n",
      "epoch 1331: loss = 0.0974\n",
      "epoch 1332: loss = 0.0778\n",
      "epoch 1333: loss = 0.0223\n",
      "epoch 1334: loss = 0.0723\n",
      "epoch 1335: loss = 0.0974\n",
      "epoch 1336: loss = 0.0908\n",
      "epoch 1337: loss = 0.0356\n",
      "epoch 1338: loss = 0.0560\n",
      "epoch 1339: loss = 0.1710\n",
      "epoch 1340: loss = 0.1410\n",
      "epoch 1341: loss = 0.1190\n",
      "epoch 1342: loss = 0.1635\n",
      "epoch 1343: loss = 0.0730\n",
      "epoch 1344: loss = 0.0175\n",
      "epoch 1345: loss = 0.1086\n",
      "epoch 1346: loss = 0.0971\n",
      "epoch 1347: loss = 0.0967\n",
      "epoch 1348: loss = 0.1201\n",
      "epoch 1349: loss = 0.0942\n",
      "epoch 1350: loss = 0.1092\n",
      "epoch 1351: loss = 0.1112\n",
      "epoch 1352: loss = 0.0747\n",
      "epoch 1353: loss = 0.0783\n",
      "epoch 1354: loss = 0.1174\n",
      "epoch 1355: loss = 0.0707\n",
      "epoch 1356: loss = 0.1764\n",
      "epoch 1357: loss = 0.1859\n",
      "epoch 1358: loss = 0.1283\n",
      "epoch 1359: loss = 0.1070\n",
      "epoch 1360: loss = 0.1380\n",
      "epoch 1361: loss = 0.1727\n",
      "epoch 1362: loss = 0.1302\n",
      "epoch 1363: loss = 0.0885\n",
      "epoch 1364: loss = 0.0238\n",
      "epoch 1365: loss = 0.0597\n",
      "epoch 1366: loss = 0.1237\n",
      "epoch 1367: loss = 0.1592\n",
      "epoch 1368: loss = 0.0979\n",
      "epoch 1369: loss = 0.1324\n",
      "epoch 1370: loss = 0.1329\n",
      "epoch 1371: loss = 0.1002\n",
      "epoch 1372: loss = 0.1441\n",
      "epoch 1373: loss = 0.1001\n",
      "epoch 1374: loss = 0.1026\n",
      "epoch 1375: loss = 0.0840\n",
      "epoch 1376: loss = 0.1827\n",
      "epoch 1377: loss = 0.1051\n",
      "epoch 1378: loss = 0.1023\n",
      "epoch 1379: loss = 0.1350\n",
      "epoch 1380: loss = 0.1132\n",
      "epoch 1381: loss = 0.0916\n",
      "epoch 1382: loss = 0.1048\n",
      "epoch 1383: loss = 0.0539\n",
      "epoch 1384: loss = 0.1141\n",
      "epoch 1385: loss = 0.0489\n",
      "epoch 1386: loss = 0.1032\n",
      "epoch 1387: loss = 0.0825\n",
      "epoch 1388: loss = 0.1088\n",
      "epoch 1389: loss = 0.1054\n",
      "epoch 1390: loss = 0.0893\n",
      "epoch 1391: loss = 0.1599\n",
      "epoch 1392: loss = 0.0956\n",
      "epoch 1393: loss = 0.0870\n",
      "epoch 1394: loss = 0.0764\n",
      "epoch 1395: loss = 0.1022\n",
      "epoch 1396: loss = 0.0855\n",
      "epoch 1397: loss = 0.1370\n",
      "epoch 1398: loss = 0.1074\n",
      "epoch 1399: loss = 0.1308\n",
      "epoch 1400: loss = 0.1200\n",
      "epoch 1401: loss = 0.1134\n",
      "epoch 1402: loss = 0.1639\n",
      "epoch 1403: loss = 0.1565\n",
      "epoch 1404: loss = 0.0745\n",
      "epoch 1405: loss = 0.1274\n",
      "epoch 1406: loss = 0.1562\n",
      "epoch 1407: loss = 0.0835\n",
      "epoch 1408: loss = 0.1226\n",
      "epoch 1409: loss = 0.1403\n",
      "epoch 1410: loss = 0.1440\n",
      "epoch 1411: loss = 0.1413\n",
      "epoch 1412: loss = 0.1561\n",
      "epoch 1413: loss = 0.1507\n",
      "epoch 1414: loss = 0.1171\n",
      "epoch 1415: loss = 0.1269\n",
      "epoch 1416: loss = 0.0842\n",
      "epoch 1417: loss = 0.1463\n",
      "epoch 1418: loss = 0.1666\n",
      "epoch 1419: loss = 0.1224\n",
      "epoch 1420: loss = 0.1325\n",
      "epoch 1421: loss = 0.1452\n",
      "epoch 1422: loss = 0.1479\n",
      "epoch 1423: loss = 0.1610\n",
      "epoch 1424: loss = 0.0517\n",
      "epoch 1425: loss = 0.0713\n",
      "epoch 1426: loss = 0.1455\n",
      "epoch 1427: loss = 0.0641\n",
      "epoch 1428: loss = 0.1001\n",
      "epoch 1429: loss = 0.0915\n",
      "epoch 1430: loss = 0.0846\n",
      "epoch 1431: loss = 0.1218\n",
      "epoch 1432: loss = 0.0509\n",
      "epoch 1433: loss = 0.1158\n",
      "epoch 1434: loss = 0.1329\n",
      "epoch 1435: loss = 0.1093\n",
      "epoch 1436: loss = 0.1377\n",
      "epoch 1437: loss = 0.0502\n",
      "epoch 1438: loss = 0.1528\n",
      "epoch 1439: loss = 0.1463\n",
      "epoch 1440: loss = 0.0755\n",
      "epoch 1441: loss = 0.0797\n",
      "epoch 1442: loss = 0.1181\n",
      "epoch 1443: loss = 0.1148\n",
      "epoch 1444: loss = 0.1402\n",
      "epoch 1445: loss = 0.0932\n",
      "epoch 1446: loss = 0.0966\n",
      "epoch 1447: loss = 0.1493\n",
      "epoch 1448: loss = 0.1041\n",
      "epoch 1449: loss = 0.0892\n",
      "epoch 1450: loss = 0.1411\n",
      "epoch 1451: loss = 0.1394\n",
      "epoch 1452: loss = 0.1067\n",
      "epoch 1453: loss = 0.0663\n",
      "epoch 1454: loss = 0.0684\n",
      "epoch 1455: loss = 0.0418\n",
      "epoch 1456: loss = 0.1823\n",
      "epoch 1457: loss = 0.1464\n",
      "epoch 1458: loss = 0.0820\n",
      "epoch 1459: loss = 0.0397\n",
      "epoch 1460: loss = 0.1684\n",
      "epoch 1461: loss = 0.0986\n",
      "epoch 1462: loss = 0.1294\n",
      "epoch 1463: loss = 0.1466\n",
      "epoch 1464: loss = 0.1552\n",
      "epoch 1465: loss = 0.0428\n",
      "epoch 1466: loss = 0.0840\n",
      "epoch 1467: loss = 0.1179\n",
      "epoch 1468: loss = 0.0893\n",
      "epoch 1469: loss = 0.1288\n",
      "epoch 1470: loss = 0.1879\n",
      "epoch 1471: loss = 0.1429\n",
      "epoch 1472: loss = 0.1848\n",
      "epoch 1473: loss = 0.1600\n",
      "epoch 1474: loss = 0.1387\n",
      "epoch 1475: loss = 0.1197\n",
      "epoch 1476: loss = 0.1793\n",
      "epoch 1477: loss = 0.1797\n",
      "epoch 1478: loss = 0.0706\n",
      "epoch 1479: loss = 0.0734\n",
      "epoch 1480: loss = 0.1601\n",
      "epoch 1481: loss = 0.1301\n",
      "epoch 1482: loss = 0.1534\n",
      "epoch 1483: loss = 0.0733\n",
      "epoch 1484: loss = 0.1434\n",
      "epoch 1485: loss = 0.1686\n",
      "epoch 1486: loss = 0.1230\n",
      "epoch 1487: loss = 0.1634\n",
      "epoch 1488: loss = 0.1769\n",
      "epoch 1489: loss = 0.1385\n",
      "epoch 1490: loss = 0.0966\n",
      "epoch 1491: loss = 0.1614\n",
      "epoch 1492: loss = 0.1551\n",
      "epoch 1493: loss = 0.1017\n",
      "epoch 1494: loss = 0.0900\n",
      "epoch 1495: loss = 0.0224\n",
      "epoch 1496: loss = 0.1772\n",
      "epoch 1497: loss = 0.0980\n",
      "epoch 1498: loss = 0.0468\n",
      "epoch 1499: loss = 0.0886\n",
      "epoch 1500: loss = 0.1160\n",
      "epoch 1501: loss = 0.1422\n",
      "epoch 1502: loss = 0.1231\n",
      "epoch 1503: loss = 0.1194\n",
      "epoch 1504: loss = 0.1196\n",
      "epoch 1505: loss = 0.0881\n",
      "epoch 1506: loss = 0.1332\n",
      "epoch 1507: loss = 0.1133\n",
      "epoch 1508: loss = 0.0948\n",
      "epoch 1509: loss = 0.1068\n",
      "epoch 1510: loss = 0.0764\n",
      "epoch 1511: loss = 0.1336\n",
      "epoch 1512: loss = 0.1291\n",
      "epoch 1513: loss = 0.0992\n",
      "epoch 1514: loss = 0.0590\n",
      "epoch 1515: loss = 0.1193\n",
      "epoch 1516: loss = 0.1215\n",
      "epoch 1517: loss = 0.0414\n",
      "epoch 1518: loss = 0.0881\n",
      "epoch 1519: loss = 0.1687\n",
      "epoch 1520: loss = 0.0987\n",
      "epoch 1521: loss = 0.1453\n",
      "epoch 1522: loss = 0.1106\n",
      "epoch 1523: loss = 0.0618\n",
      "epoch 1524: loss = 0.0455\n",
      "epoch 1525: loss = 0.1158\n",
      "epoch 1526: loss = 0.1289\n",
      "epoch 1527: loss = 0.0772\n",
      "epoch 1528: loss = 0.1188\n",
      "epoch 1529: loss = 0.0869\n",
      "epoch 1530: loss = 0.1464\n",
      "epoch 1531: loss = 0.0809\n",
      "epoch 1532: loss = 0.1579\n",
      "epoch 1533: loss = 0.1218\n",
      "epoch 1534: loss = 0.0746\n",
      "epoch 1535: loss = 0.0601\n",
      "epoch 1536: loss = 0.1527\n",
      "epoch 1537: loss = 0.1501\n",
      "epoch 1538: loss = 0.1778\n",
      "epoch 1539: loss = 0.1516\n",
      "epoch 1540: loss = 0.1453\n",
      "epoch 1541: loss = 0.1088\n",
      "epoch 1542: loss = 0.1758\n",
      "epoch 1543: loss = 0.1591\n",
      "epoch 1544: loss = 0.1293\n",
      "epoch 1545: loss = 0.1388\n",
      "epoch 1546: loss = 0.1609\n",
      "epoch 1547: loss = 0.1022\n",
      "epoch 1548: loss = 0.0455\n",
      "epoch 1549: loss = 0.1551\n",
      "epoch 1550: loss = 0.1506\n",
      "epoch 1551: loss = 0.1048\n",
      "epoch 1552: loss = 0.1644\n",
      "epoch 1553: loss = 0.1283\n",
      "epoch 1554: loss = 0.1030\n",
      "epoch 1555: loss = 0.0640\n",
      "epoch 1556: loss = 0.1362\n",
      "epoch 1557: loss = 0.1610\n",
      "epoch 1558: loss = 0.0810\n",
      "epoch 1559: loss = 0.0507\n",
      "epoch 1560: loss = 0.1162\n",
      "epoch 1561: loss = 0.1405\n",
      "epoch 1562: loss = 0.0641\n",
      "epoch 1563: loss = 0.1330\n",
      "epoch 1564: loss = 0.1228\n",
      "epoch 1565: loss = 0.1233\n",
      "epoch 1566: loss = 0.1106\n",
      "epoch 1567: loss = 0.1308\n",
      "epoch 1568: loss = 0.0558\n",
      "epoch 1569: loss = 0.0988\n",
      "epoch 1570: loss = 0.0900\n",
      "epoch 1571: loss = 0.1128\n",
      "epoch 1572: loss = 0.0604\n",
      "epoch 1573: loss = 0.0134\n",
      "epoch 1574: loss = 0.0826\n",
      "epoch 1575: loss = 0.1611\n",
      "epoch 1576: loss = 0.1080\n",
      "epoch 1577: loss = 0.1349\n",
      "epoch 1578: loss = 0.1392\n",
      "epoch 1579: loss = 0.1194\n",
      "epoch 1580: loss = 0.1418\n",
      "epoch 1581: loss = 0.1151\n",
      "epoch 1582: loss = 0.0744\n",
      "epoch 1583: loss = 0.1481\n",
      "epoch 1584: loss = 0.0561\n",
      "epoch 1585: loss = 0.0918\n",
      "epoch 1586: loss = 0.1314\n",
      "epoch 1587: loss = 0.1224\n",
      "epoch 1588: loss = 0.0976\n",
      "epoch 1589: loss = 0.1561\n",
      "epoch 1590: loss = 0.1699\n",
      "epoch 1591: loss = 0.1271\n",
      "epoch 1592: loss = 0.1461\n",
      "epoch 1593: loss = 0.1196\n",
      "epoch 1594: loss = 0.1484\n",
      "epoch 1595: loss = 0.1341\n",
      "epoch 1596: loss = 0.1246\n",
      "epoch 1597: loss = 0.0773\n",
      "epoch 1598: loss = 0.1302\n",
      "epoch 1599: loss = 0.1048\n",
      "epoch 1600: loss = 0.0530\n",
      "epoch 1601: loss = 0.0796\n",
      "epoch 1602: loss = 0.0815\n",
      "epoch 1603: loss = 0.1158\n",
      "epoch 1604: loss = 0.1420\n",
      "epoch 1605: loss = 0.1881\n",
      "epoch 1606: loss = 0.1654\n",
      "epoch 1607: loss = 0.1435\n",
      "epoch 1608: loss = 0.1743\n",
      "epoch 1609: loss = 0.1410\n",
      "epoch 1610: loss = 0.0222\n",
      "epoch 1611: loss = 0.1133\n",
      "epoch 1612: loss = 0.0849\n",
      "epoch 1613: loss = 0.1478\n",
      "epoch 1614: loss = 0.1499\n",
      "epoch 1615: loss = 0.1329\n",
      "epoch 1616: loss = 0.0687\n",
      "epoch 1617: loss = 0.0973\n",
      "epoch 1618: loss = 0.0335\n",
      "epoch 1619: loss = 0.1138\n",
      "epoch 1620: loss = 0.1644\n",
      "epoch 1621: loss = 0.1739\n",
      "epoch 1622: loss = 0.1764\n",
      "epoch 1623: loss = 0.0904\n",
      "epoch 1624: loss = 0.0885\n",
      "epoch 1625: loss = 0.0890\n",
      "epoch 1626: loss = 0.1140\n",
      "epoch 1627: loss = 0.0271\n",
      "epoch 1628: loss = 0.0549\n",
      "epoch 1629: loss = 0.0645\n",
      "epoch 1630: loss = 0.0825\n",
      "epoch 1631: loss = 0.1120\n",
      "epoch 1632: loss = 0.1207\n",
      "epoch 1633: loss = 0.0674\n",
      "epoch 1634: loss = 0.1192\n",
      "epoch 1635: loss = 0.1636\n",
      "epoch 1636: loss = 0.1487\n",
      "epoch 1637: loss = 0.1398\n",
      "epoch 1638: loss = 0.1107\n",
      "epoch 1639: loss = 0.1014\n",
      "epoch 1640: loss = 0.0877\n",
      "epoch 1641: loss = 0.0712\n",
      "epoch 1642: loss = 0.1574\n",
      "epoch 1643: loss = 0.1528\n",
      "epoch 1644: loss = 0.1553\n",
      "epoch 1645: loss = 0.0858\n",
      "epoch 1646: loss = 0.1071\n",
      "epoch 1647: loss = 0.0991\n",
      "epoch 1648: loss = 0.1143\n",
      "epoch 1649: loss = 0.1834\n",
      "epoch 1650: loss = 0.1401\n",
      "epoch 1651: loss = 0.1780\n",
      "epoch 1652: loss = 0.1434\n",
      "epoch 1653: loss = 0.1473\n",
      "epoch 1654: loss = 0.1253\n",
      "epoch 1655: loss = 0.1100\n",
      "epoch 1656: loss = 0.1038\n",
      "epoch 1657: loss = 0.0599\n",
      "epoch 1658: loss = 0.1077\n",
      "epoch 1659: loss = 0.0962\n",
      "epoch 1660: loss = 0.0826\n",
      "epoch 1661: loss = 0.1326\n",
      "epoch 1662: loss = 0.0815\n",
      "epoch 1663: loss = 0.1588\n",
      "epoch 1664: loss = 0.1060\n",
      "epoch 1665: loss = 0.1036\n",
      "epoch 1666: loss = 0.0726\n",
      "epoch 1667: loss = 0.0861\n",
      "epoch 1668: loss = 0.0653\n",
      "epoch 1669: loss = 0.0764\n",
      "epoch 1670: loss = 0.1537\n",
      "epoch 1671: loss = 0.1620\n",
      "epoch 1672: loss = 0.1413\n",
      "epoch 1673: loss = 0.1423\n",
      "epoch 1674: loss = 0.0320\n",
      "epoch 1675: loss = 0.1149\n",
      "epoch 1676: loss = 0.1332\n",
      "epoch 1677: loss = 0.0924\n",
      "epoch 1678: loss = 0.1270\n",
      "epoch 1679: loss = 0.1534\n",
      "epoch 1680: loss = 0.1025\n",
      "epoch 1681: loss = 0.0353\n",
      "epoch 1682: loss = 0.1078\n",
      "epoch 1683: loss = 0.1304\n",
      "epoch 1684: loss = 0.0689\n",
      "epoch 1685: loss = 0.1337\n",
      "epoch 1686: loss = 0.1222\n",
      "epoch 1687: loss = 0.0729\n",
      "epoch 1688: loss = 0.0948\n",
      "epoch 1689: loss = 0.1495\n",
      "epoch 1690: loss = 0.0593\n",
      "epoch 1691: loss = 0.1093\n",
      "epoch 1692: loss = 0.0920\n",
      "epoch 1693: loss = 0.1240\n",
      "epoch 1694: loss = 0.0379\n",
      "epoch 1695: loss = 0.1018\n",
      "epoch 1696: loss = 0.1132\n",
      "epoch 1697: loss = 0.1282\n",
      "epoch 1698: loss = 0.1724\n",
      "epoch 1699: loss = 0.1370\n",
      "epoch 1700: loss = 0.0534\n",
      "epoch 1701: loss = 0.1414\n",
      "epoch 1702: loss = 0.1034\n",
      "epoch 1703: loss = 0.0344\n",
      "epoch 1704: loss = 0.0753\n",
      "epoch 1705: loss = 0.0522\n",
      "epoch 1706: loss = 0.0330\n",
      "epoch 1707: loss = 0.0790\n",
      "epoch 1708: loss = 0.1528\n",
      "epoch 1709: loss = 0.1384\n",
      "epoch 1710: loss = 0.1485\n",
      "epoch 1711: loss = 0.1209\n",
      "epoch 1712: loss = 0.1403\n",
      "epoch 1713: loss = 0.1104\n",
      "epoch 1714: loss = 0.0742\n",
      "epoch 1715: loss = 0.1244\n",
      "epoch 1716: loss = 0.1772\n",
      "epoch 1717: loss = 0.1224\n",
      "epoch 1718: loss = 0.0651\n",
      "epoch 1719: loss = 0.1149\n",
      "epoch 1720: loss = 0.1666\n",
      "epoch 1721: loss = 0.1927\n",
      "epoch 1722: loss = 0.1693\n",
      "epoch 1723: loss = 0.0599\n",
      "epoch 1724: loss = 0.1177\n",
      "epoch 1725: loss = 0.1012\n",
      "epoch 1726: loss = 0.0854\n",
      "epoch 1727: loss = 0.1374\n",
      "epoch 1728: loss = 0.0690\n",
      "epoch 1729: loss = 0.0551\n",
      "epoch 1730: loss = 0.0599\n",
      "epoch 1731: loss = 0.1596\n",
      "epoch 1732: loss = 0.1719\n",
      "epoch 1733: loss = 0.1723\n",
      "epoch 1734: loss = 0.1723\n",
      "epoch 1735: loss = 0.0822\n",
      "epoch 1736: loss = 0.1274\n",
      "epoch 1737: loss = 0.1773\n",
      "epoch 1738: loss = 0.0838\n",
      "epoch 1739: loss = 0.1745\n",
      "epoch 1740: loss = 0.1386\n",
      "epoch 1741: loss = 0.0586\n",
      "epoch 1742: loss = 0.0420\n",
      "epoch 1743: loss = 0.1044\n",
      "epoch 1744: loss = 0.1181\n",
      "epoch 1745: loss = 0.0288\n",
      "epoch 1746: loss = 0.0643\n",
      "epoch 1747: loss = 0.1145\n",
      "epoch 1748: loss = 0.1392\n",
      "epoch 1749: loss = 0.1245\n",
      "epoch 1750: loss = 0.1139\n",
      "epoch 1751: loss = 0.1159\n",
      "epoch 1752: loss = 0.1918\n",
      "epoch 1753: loss = 0.1317\n",
      "epoch 1754: loss = 0.0977\n",
      "epoch 1755: loss = 0.0465\n",
      "epoch 1756: loss = 0.1267\n",
      "epoch 1757: loss = 0.0864\n",
      "epoch 1758: loss = 0.1431\n",
      "epoch 1759: loss = 0.0558\n",
      "epoch 1760: loss = 0.1526\n",
      "epoch 1761: loss = 0.1141\n",
      "epoch 1762: loss = 0.0854\n",
      "epoch 1763: loss = 0.1094\n",
      "epoch 1764: loss = 0.1025\n",
      "epoch 1765: loss = 0.1070\n",
      "epoch 1766: loss = 0.1461\n",
      "epoch 1767: loss = 0.0874\n",
      "epoch 1768: loss = 0.0786\n",
      "epoch 1769: loss = 0.1049\n",
      "epoch 1770: loss = 0.0514\n",
      "epoch 1771: loss = 0.0512\n",
      "epoch 1772: loss = 0.1019\n",
      "epoch 1773: loss = 0.1748\n",
      "epoch 1774: loss = 0.1232\n",
      "epoch 1775: loss = 0.1730\n",
      "epoch 1776: loss = 0.1093\n",
      "epoch 1777: loss = 0.0608\n",
      "epoch 1778: loss = 0.1786\n",
      "epoch 1779: loss = 0.1645\n",
      "epoch 1780: loss = 0.1416\n",
      "epoch 1781: loss = 0.1410\n",
      "epoch 1782: loss = 0.0929\n",
      "epoch 1783: loss = 0.1738\n",
      "epoch 1784: loss = 0.1897\n",
      "epoch 1785: loss = 0.1442\n",
      "epoch 1786: loss = 0.1665\n",
      "epoch 1787: loss = 0.1099\n",
      "epoch 1788: loss = 0.0349\n",
      "epoch 1789: loss = 0.1020\n",
      "epoch 1790: loss = 0.0666\n",
      "epoch 1791: loss = 0.0972\n",
      "epoch 1792: loss = 0.0347\n",
      "epoch 1793: loss = 0.1633\n",
      "epoch 1794: loss = 0.0715\n",
      "epoch 1795: loss = 0.1745\n",
      "epoch 1796: loss = 0.1236\n",
      "epoch 1797: loss = 0.0953\n",
      "epoch 1798: loss = 0.1271\n",
      "epoch 1799: loss = 0.0246\n",
      "epoch 1800: loss = 0.1395\n",
      "epoch 1801: loss = 0.0625\n",
      "epoch 1802: loss = 0.1162\n",
      "epoch 1803: loss = 0.1257\n",
      "epoch 1804: loss = 0.0974\n",
      "epoch 1805: loss = 0.1362\n",
      "epoch 1806: loss = 0.1108\n",
      "epoch 1807: loss = 0.1122\n",
      "epoch 1808: loss = 0.1305\n",
      "epoch 1809: loss = 0.1489\n",
      "epoch 1810: loss = 0.1052\n",
      "epoch 1811: loss = 0.1146\n",
      "epoch 1812: loss = 0.1237\n",
      "epoch 1813: loss = 0.1051\n",
      "epoch 1814: loss = 0.0857\n",
      "epoch 1815: loss = 0.0724\n",
      "epoch 1816: loss = 0.1214\n",
      "epoch 1817: loss = 0.1315\n",
      "epoch 1818: loss = 0.1008\n",
      "epoch 1819: loss = 0.0939\n",
      "epoch 1820: loss = 0.1085\n",
      "epoch 1821: loss = 0.0971\n",
      "epoch 1822: loss = 0.1098\n",
      "epoch 1823: loss = 0.1747\n",
      "epoch 1824: loss = 0.1684\n",
      "epoch 1825: loss = 0.1237\n",
      "epoch 1826: loss = 0.0583\n",
      "epoch 1827: loss = 0.0598\n",
      "epoch 1828: loss = 0.0620\n",
      "epoch 1829: loss = 0.0854\n",
      "epoch 1830: loss = 0.0593\n",
      "epoch 1831: loss = 0.1837\n",
      "epoch 1832: loss = 0.1306\n",
      "epoch 1833: loss = 0.1354\n",
      "epoch 1834: loss = 0.1279\n",
      "epoch 1835: loss = 0.0878\n",
      "epoch 1836: loss = 0.0715\n",
      "epoch 1837: loss = 0.1428\n",
      "epoch 1838: loss = 0.1391\n",
      "epoch 1839: loss = 0.1162\n",
      "epoch 1840: loss = 0.0319\n",
      "epoch 1841: loss = 0.0900\n",
      "epoch 1842: loss = 0.1204\n",
      "epoch 1843: loss = 0.0889\n",
      "epoch 1844: loss = 0.0767\n",
      "epoch 1845: loss = 0.0771\n",
      "epoch 1846: loss = 0.1362\n",
      "epoch 1847: loss = 0.1589\n",
      "epoch 1848: loss = 0.0980\n",
      "epoch 1849: loss = 0.1011\n",
      "epoch 1850: loss = 0.1694\n",
      "epoch 1851: loss = 0.0560\n",
      "epoch 1852: loss = 0.1852\n",
      "epoch 1853: loss = 0.1120\n",
      "epoch 1854: loss = 0.0731\n",
      "epoch 1855: loss = 0.0926\n",
      "epoch 1856: loss = 0.0714\n",
      "epoch 1857: loss = 0.1078\n",
      "epoch 1858: loss = 0.1591\n",
      "epoch 1859: loss = 0.0169\n",
      "epoch 1860: loss = 0.1011\n",
      "epoch 1861: loss = 0.1631\n",
      "epoch 1862: loss = 0.1265\n",
      "epoch 1863: loss = 0.0783\n",
      "epoch 1864: loss = 0.0398\n",
      "epoch 1865: loss = 0.1152\n",
      "epoch 1866: loss = 0.1083\n",
      "epoch 1867: loss = 0.0436\n",
      "epoch 1868: loss = 0.0873\n",
      "epoch 1869: loss = 0.0631\n",
      "epoch 1870: loss = 0.0230\n",
      "epoch 1871: loss = 0.0946\n",
      "epoch 1872: loss = 0.0963\n",
      "epoch 1873: loss = 0.1582\n",
      "epoch 1874: loss = 0.1311\n",
      "epoch 1875: loss = 0.1521\n",
      "epoch 1876: loss = 0.0792\n",
      "epoch 1877: loss = 0.1227\n",
      "epoch 1878: loss = 0.0942\n",
      "epoch 1879: loss = 0.0713\n",
      "epoch 1880: loss = 0.0950\n",
      "epoch 1881: loss = 0.1101\n",
      "epoch 1882: loss = 0.0933\n",
      "epoch 1883: loss = 0.1432\n",
      "epoch 1884: loss = 0.1546\n",
      "epoch 1885: loss = 0.0950\n",
      "epoch 1886: loss = 0.1248\n",
      "epoch 1887: loss = 0.1939\n",
      "epoch 1888: loss = 0.1184\n",
      "epoch 1889: loss = 0.1895\n",
      "epoch 1890: loss = 0.1802\n",
      "epoch 1891: loss = 0.1798\n",
      "epoch 1892: loss = 0.1351\n",
      "epoch 1893: loss = 0.0840\n",
      "epoch 1894: loss = 0.0955\n",
      "epoch 1895: loss = 0.1618\n",
      "epoch 1896: loss = 0.1299\n",
      "epoch 1897: loss = 0.1358\n",
      "epoch 1898: loss = 0.1286\n",
      "epoch 1899: loss = 0.0932\n",
      "epoch 1900: loss = 0.0184\n",
      "epoch 1901: loss = 0.1314\n",
      "epoch 1902: loss = 0.1585\n",
      "epoch 1903: loss = 0.1528\n",
      "epoch 1904: loss = 0.1022\n",
      "epoch 1905: loss = 0.0793\n",
      "epoch 1906: loss = 0.1435\n",
      "epoch 1907: loss = 0.1131\n",
      "epoch 1908: loss = 0.0633\n",
      "epoch 1909: loss = 0.1285\n",
      "epoch 1910: loss = 0.0851\n",
      "epoch 1911: loss = 0.1282\n",
      "epoch 1912: loss = 0.1343\n",
      "epoch 1913: loss = 0.1088\n",
      "epoch 1914: loss = 0.1527\n",
      "epoch 1915: loss = 0.1182\n",
      "epoch 1916: loss = 0.1292\n",
      "epoch 1917: loss = 0.0515\n",
      "epoch 1918: loss = 0.1352\n",
      "epoch 1919: loss = 0.1720\n",
      "epoch 1920: loss = 0.1026\n",
      "epoch 1921: loss = 0.1409\n",
      "epoch 1922: loss = 0.0441\n",
      "epoch 1923: loss = 0.1175\n",
      "epoch 1924: loss = 0.1420\n",
      "epoch 1925: loss = 0.1722\n",
      "epoch 1926: loss = 0.1276\n",
      "epoch 1927: loss = 0.1457\n",
      "epoch 1928: loss = 0.0796\n",
      "epoch 1929: loss = 0.1258\n",
      "epoch 1930: loss = 0.1083\n",
      "epoch 1931: loss = 0.1881\n",
      "epoch 1932: loss = 0.1129\n",
      "epoch 1933: loss = 0.1463\n",
      "epoch 1934: loss = 0.1785\n",
      "epoch 1935: loss = 0.0457\n",
      "epoch 1936: loss = 0.1207\n",
      "epoch 1937: loss = 0.1736\n",
      "epoch 1938: loss = 0.1521\n",
      "epoch 1939: loss = 0.1166\n",
      "epoch 1940: loss = 0.0129\n",
      "epoch 1941: loss = 0.1054\n",
      "epoch 1942: loss = 0.1825\n",
      "epoch 1943: loss = 0.0187\n",
      "epoch 1944: loss = 0.0469\n",
      "epoch 1945: loss = 0.1934\n",
      "epoch 1946: loss = 0.1293\n",
      "epoch 1947: loss = 0.0622\n",
      "epoch 1948: loss = 0.1270\n",
      "epoch 1949: loss = 0.0466\n",
      "epoch 1950: loss = 0.0840\n",
      "epoch 1951: loss = 0.1463\n",
      "epoch 1952: loss = 0.1197\n",
      "epoch 1953: loss = 0.1502\n",
      "epoch 1954: loss = 0.1250\n",
      "epoch 1955: loss = 0.1646\n",
      "epoch 1956: loss = 0.0336\n",
      "epoch 1957: loss = 0.1429\n",
      "epoch 1958: loss = 0.1494\n",
      "epoch 1959: loss = 0.0752\n",
      "epoch 1960: loss = 0.1049\n",
      "epoch 1961: loss = 0.1121\n",
      "epoch 1962: loss = 0.1895\n",
      "epoch 1963: loss = 0.1500\n",
      "epoch 1964: loss = 0.0512\n",
      "epoch 1965: loss = 0.0678\n",
      "epoch 1966: loss = 0.0464\n",
      "epoch 1967: loss = 0.0667\n",
      "epoch 1968: loss = 0.0665\n",
      "epoch 1969: loss = 0.0708\n",
      "epoch 1970: loss = 0.1303\n",
      "epoch 1971: loss = 0.0899\n",
      "epoch 1972: loss = 0.1215\n",
      "epoch 1973: loss = 0.1350\n",
      "epoch 1974: loss = 0.0935\n",
      "epoch 1975: loss = 0.1735\n",
      "epoch 1976: loss = 0.1188\n",
      "epoch 1977: loss = 0.0956\n",
      "epoch 1978: loss = 0.0730\n",
      "epoch 1979: loss = 0.0660\n",
      "epoch 1980: loss = 0.0702\n",
      "epoch 1981: loss = 0.1370\n",
      "epoch 1982: loss = 0.0493\n",
      "epoch 1983: loss = 0.0871\n",
      "epoch 1984: loss = 0.0873\n",
      "epoch 1985: loss = 0.0714\n",
      "epoch 1986: loss = 0.1101\n",
      "epoch 1987: loss = 0.0903\n",
      "epoch 1988: loss = 0.1078\n",
      "epoch 1989: loss = 0.1199\n",
      "epoch 1990: loss = 0.1296\n",
      "epoch 1991: loss = 0.1075\n",
      "epoch 1992: loss = 0.1353\n",
      "epoch 1993: loss = 0.1480\n",
      "epoch 1994: loss = 0.1927\n",
      "epoch 1995: loss = 0.1550\n",
      "epoch 1996: loss = 0.1311\n",
      "epoch 1997: loss = 0.1146\n",
      "epoch 1998: loss = 0.1702\n",
      "epoch 1999: loss = 0.1355\n",
      "epoch 2000: loss = 0.1156\n",
      "epoch 2001: loss = 0.1088\n",
      "epoch 2002: loss = 0.1553\n",
      "epoch 2003: loss = 0.1286\n",
      "epoch 2004: loss = 0.0901\n",
      "epoch 2005: loss = 0.1155\n",
      "epoch 2006: loss = 0.1881\n",
      "epoch 2007: loss = 0.1109\n",
      "epoch 2008: loss = 0.1372\n",
      "epoch 2009: loss = 0.0875\n",
      "epoch 2010: loss = 0.1092\n",
      "epoch 2011: loss = 0.1991\n",
      "epoch 2012: loss = 0.1344\n",
      "epoch 2013: loss = 0.1345\n",
      "epoch 2014: loss = 0.0817\n",
      "epoch 2015: loss = 0.0403\n",
      "epoch 2016: loss = 0.1191\n",
      "epoch 2017: loss = 0.0500\n",
      "epoch 2018: loss = 0.0678\n",
      "epoch 2019: loss = 0.1197\n",
      "epoch 2020: loss = 0.1508\n",
      "epoch 2021: loss = 0.1117\n",
      "epoch 2022: loss = 0.1162\n",
      "epoch 2023: loss = 0.1699\n",
      "epoch 2024: loss = 0.1216\n",
      "epoch 2025: loss = 0.0643\n",
      "epoch 2026: loss = 0.1192\n",
      "epoch 2027: loss = 0.0920\n",
      "epoch 2028: loss = 0.0420\n",
      "epoch 2029: loss = 0.1314\n",
      "epoch 2030: loss = 0.1107\n",
      "epoch 2031: loss = 0.1381\n",
      "epoch 2032: loss = 0.1734\n",
      "epoch 2033: loss = 0.1470\n",
      "epoch 2034: loss = 0.1375\n",
      "epoch 2035: loss = 0.1731\n",
      "epoch 2036: loss = 0.1119\n",
      "epoch 2037: loss = 0.1272\n",
      "epoch 2038: loss = 0.1245\n",
      "epoch 2039: loss = 0.1138\n",
      "epoch 2040: loss = 0.1493\n",
      "epoch 2041: loss = 0.0851\n",
      "epoch 2042: loss = 0.1817\n",
      "epoch 2043: loss = 0.1056\n",
      "epoch 2044: loss = 0.1031\n",
      "epoch 2045: loss = 0.1484\n",
      "epoch 2046: loss = 0.1251\n",
      "epoch 2047: loss = 0.0236\n",
      "epoch 2048: loss = 0.0659\n",
      "epoch 2049: loss = 0.1383\n",
      "epoch 2050: loss = 0.0929\n",
      "epoch 2051: loss = 0.0905\n",
      "epoch 2052: loss = 0.1049\n",
      "epoch 2053: loss = 0.0223\n",
      "epoch 2054: loss = 0.1819\n",
      "epoch 2055: loss = 0.1836\n",
      "epoch 2056: loss = 0.1580\n",
      "epoch 2057: loss = 0.1455\n",
      "epoch 2058: loss = 0.1149\n",
      "epoch 2059: loss = 0.0792\n",
      "epoch 2060: loss = 0.1008\n",
      "epoch 2061: loss = 0.1096\n",
      "epoch 2062: loss = 0.1535\n",
      "epoch 2063: loss = 0.1433\n",
      "epoch 2064: loss = 0.1036\n",
      "epoch 2065: loss = 0.0882\n",
      "epoch 2066: loss = 0.1165\n",
      "epoch 2067: loss = 0.1677\n",
      "epoch 2068: loss = 0.1360\n",
      "epoch 2069: loss = 0.1076\n",
      "epoch 2070: loss = 0.1910\n",
      "epoch 2071: loss = 0.1316\n",
      "epoch 2072: loss = 0.0665\n",
      "epoch 2073: loss = 0.1143\n",
      "epoch 2074: loss = 0.1839\n",
      "epoch 2075: loss = 0.1259\n",
      "epoch 2076: loss = 0.0497\n",
      "epoch 2077: loss = 0.0886\n",
      "epoch 2078: loss = 0.1646\n",
      "epoch 2079: loss = 0.1312\n",
      "epoch 2080: loss = 0.0804\n",
      "epoch 2081: loss = 0.1434\n",
      "epoch 2082: loss = 0.1037\n",
      "epoch 2083: loss = 0.1035\n",
      "epoch 2084: loss = 0.0784\n",
      "epoch 2085: loss = 0.0862\n",
      "epoch 2086: loss = 0.1367\n",
      "epoch 2087: loss = 0.1211\n",
      "epoch 2088: loss = 0.0896\n",
      "epoch 2089: loss = 0.1545\n",
      "epoch 2090: loss = 0.0252\n",
      "epoch 2091: loss = 0.1778\n",
      "epoch 2092: loss = 0.1885\n",
      "epoch 2093: loss = 0.1591\n",
      "epoch 2094: loss = 0.0788\n",
      "epoch 2095: loss = 0.1197\n",
      "epoch 2096: loss = 0.0665\n",
      "epoch 2097: loss = 0.0915\n",
      "epoch 2098: loss = 0.0577\n",
      "epoch 2099: loss = 0.1281\n",
      "epoch 2100: loss = 0.1002\n",
      "epoch 2101: loss = 0.1687\n",
      "epoch 2102: loss = 0.1114\n",
      "epoch 2103: loss = 0.0838\n",
      "epoch 2104: loss = 0.1329\n",
      "epoch 2105: loss = 0.0891\n",
      "epoch 2106: loss = 0.0906\n",
      "epoch 2107: loss = 0.1027\n",
      "epoch 2108: loss = 0.1393\n",
      "epoch 2109: loss = 0.0653\n",
      "epoch 2110: loss = 0.0529\n",
      "epoch 2111: loss = 0.0853\n",
      "epoch 2112: loss = 0.1477\n",
      "epoch 2113: loss = 0.1556\n",
      "epoch 2114: loss = 0.1058\n",
      "epoch 2115: loss = 0.1325\n",
      "epoch 2116: loss = 0.1945\n",
      "epoch 2117: loss = 0.1153\n",
      "epoch 2118: loss = 0.1453\n",
      "epoch 2119: loss = 0.0843\n",
      "epoch 2120: loss = 0.0968\n",
      "epoch 2121: loss = 0.0885\n",
      "epoch 2122: loss = 0.1417\n",
      "epoch 2123: loss = 0.0322\n",
      "epoch 2124: loss = 0.1152\n",
      "epoch 2125: loss = 0.0823\n",
      "epoch 2126: loss = 0.1310\n",
      "epoch 2127: loss = 0.0881\n",
      "epoch 2128: loss = 0.1126\n",
      "epoch 2129: loss = 0.0928\n",
      "epoch 2130: loss = 0.0402\n",
      "epoch 2131: loss = 0.1068\n",
      "epoch 2132: loss = 0.0550\n",
      "epoch 2133: loss = 0.0448\n",
      "epoch 2134: loss = 0.1478\n",
      "epoch 2135: loss = 0.1125\n",
      "epoch 2136: loss = 0.0527\n",
      "epoch 2137: loss = 0.1730\n",
      "epoch 2138: loss = 0.0914\n",
      "epoch 2139: loss = 0.0948\n",
      "epoch 2140: loss = 0.0845\n",
      "epoch 2141: loss = 0.0914\n",
      "epoch 2142: loss = 0.1420\n",
      "epoch 2143: loss = 0.1067\n",
      "epoch 2144: loss = 0.1063\n",
      "epoch 2145: loss = 0.0979\n",
      "epoch 2146: loss = 0.1438\n",
      "epoch 2147: loss = 0.0844\n",
      "epoch 2148: loss = 0.1394\n",
      "epoch 2149: loss = 0.0912\n",
      "epoch 2150: loss = 0.1066\n",
      "epoch 2151: loss = 0.1790\n",
      "epoch 2152: loss = 0.1319\n",
      "epoch 2153: loss = 0.0846\n",
      "epoch 2154: loss = 0.1692\n",
      "epoch 2155: loss = 0.1021\n",
      "epoch 2156: loss = 0.1021\n",
      "epoch 2157: loss = 0.0820\n",
      "epoch 2158: loss = 0.1226\n",
      "epoch 2159: loss = 0.1198\n",
      "epoch 2160: loss = 0.0826\n",
      "epoch 2161: loss = 0.1278\n",
      "epoch 2162: loss = 0.1817\n",
      "epoch 2163: loss = 0.0957\n",
      "epoch 2164: loss = 0.1253\n",
      "epoch 2165: loss = 0.1368\n",
      "epoch 2166: loss = 0.0789\n",
      "epoch 2167: loss = 0.0897\n",
      "epoch 2168: loss = 0.0772\n",
      "epoch 2169: loss = 0.0696\n",
      "epoch 2170: loss = 0.0904\n",
      "epoch 2171: loss = 0.0534\n",
      "epoch 2172: loss = 0.1318\n",
      "epoch 2173: loss = 0.1044\n",
      "epoch 2174: loss = 0.1650\n",
      "epoch 2175: loss = 0.1509\n",
      "epoch 2176: loss = 0.0107\n",
      "epoch 2177: loss = 0.1264\n",
      "epoch 2178: loss = 0.1227\n",
      "epoch 2179: loss = 0.1389\n",
      "epoch 2180: loss = 0.0618\n",
      "epoch 2181: loss = 0.1338\n",
      "epoch 2182: loss = 0.1414\n",
      "epoch 2183: loss = 0.0709\n",
      "epoch 2184: loss = 0.1201\n",
      "epoch 2185: loss = 0.0776\n",
      "epoch 2186: loss = 0.0889\n",
      "epoch 2187: loss = 0.0469\n",
      "epoch 2188: loss = 0.0806\n",
      "epoch 2189: loss = 0.1020\n",
      "epoch 2190: loss = 0.0405\n",
      "epoch 2191: loss = 0.1585\n",
      "epoch 2192: loss = 0.1562\n",
      "epoch 2193: loss = 0.1180\n",
      "epoch 2194: loss = 0.0937\n",
      "epoch 2195: loss = 0.1014\n",
      "epoch 2196: loss = 0.1536\n",
      "epoch 2197: loss = 0.1856\n",
      "epoch 2198: loss = 0.1014\n",
      "epoch 2199: loss = 0.1755\n",
      "epoch 2200: loss = 0.1564\n",
      "epoch 2201: loss = 0.0912\n",
      "epoch 2202: loss = 0.1109\n",
      "epoch 2203: loss = 0.1080\n",
      "epoch 2204: loss = 0.1834\n",
      "epoch 2205: loss = 0.1339\n",
      "epoch 2206: loss = 0.1486\n",
      "epoch 2207: loss = 0.1518\n",
      "epoch 2208: loss = 0.1643\n",
      "epoch 2209: loss = 0.0719\n",
      "epoch 2210: loss = 0.1363\n",
      "epoch 2211: loss = 0.1334\n",
      "epoch 2212: loss = 0.1245\n",
      "epoch 2213: loss = 0.1367\n",
      "epoch 2214: loss = 0.0712\n",
      "epoch 2215: loss = 0.1545\n",
      "epoch 2216: loss = 0.1162\n",
      "epoch 2217: loss = 0.0727\n",
      "epoch 2218: loss = 0.1547\n",
      "epoch 2219: loss = 0.1413\n",
      "epoch 2220: loss = 0.1904\n",
      "epoch 2221: loss = 0.1102\n",
      "epoch 2222: loss = 0.0830\n",
      "epoch 2223: loss = 0.1270\n",
      "epoch 2224: loss = 0.1204\n",
      "epoch 2225: loss = 0.1325\n",
      "epoch 2226: loss = 0.0774\n",
      "epoch 2227: loss = 0.0889\n",
      "epoch 2228: loss = 0.1529\n",
      "epoch 2229: loss = 0.1507\n",
      "epoch 2230: loss = 0.1226\n",
      "epoch 2231: loss = 0.0945\n",
      "epoch 2232: loss = 0.0677\n",
      "epoch 2233: loss = 0.0712\n",
      "epoch 2234: loss = 0.1097\n",
      "epoch 2235: loss = 0.1396\n",
      "epoch 2236: loss = 0.1556\n",
      "epoch 2237: loss = 0.1147\n",
      "epoch 2238: loss = 0.0217\n",
      "epoch 2239: loss = 0.0844\n",
      "epoch 2240: loss = 0.1380\n",
      "epoch 2241: loss = 0.0849\n",
      "epoch 2242: loss = 0.1293\n",
      "epoch 2243: loss = 0.1059\n",
      "epoch 2244: loss = 0.0642\n",
      "epoch 2245: loss = 0.1828\n",
      "epoch 2246: loss = 0.1330\n",
      "epoch 2247: loss = 0.0717\n",
      "epoch 2248: loss = 0.1263\n",
      "epoch 2249: loss = 0.1131\n",
      "epoch 2250: loss = 0.0801\n",
      "epoch 2251: loss = 0.1414\n",
      "epoch 2252: loss = 0.0438\n",
      "epoch 2253: loss = 0.1275\n",
      "epoch 2254: loss = 0.0639\n",
      "epoch 2255: loss = 0.1497\n",
      "epoch 2256: loss = 0.1033\n",
      "epoch 2257: loss = 0.1283\n",
      "epoch 2258: loss = 0.0792\n",
      "epoch 2259: loss = 0.1077\n",
      "epoch 2260: loss = 0.1375\n",
      "epoch 2261: loss = 0.0685\n",
      "epoch 2262: loss = 0.1043\n",
      "epoch 2263: loss = 0.0918\n",
      "epoch 2264: loss = 0.1062\n",
      "epoch 2265: loss = 0.1062\n",
      "epoch 2266: loss = 0.0697\n",
      "epoch 2267: loss = 0.0863\n",
      "epoch 2268: loss = 0.1089\n",
      "epoch 2269: loss = 0.1422\n",
      "epoch 2270: loss = 0.1079\n",
      "epoch 2271: loss = 0.0368\n",
      "epoch 2272: loss = 0.1356\n",
      "epoch 2273: loss = 0.1125\n",
      "epoch 2274: loss = 0.1678\n",
      "epoch 2275: loss = 0.1216\n",
      "epoch 2276: loss = 0.0747\n",
      "epoch 2277: loss = 0.1344\n",
      "epoch 2278: loss = 0.1262\n",
      "epoch 2279: loss = 0.1098\n",
      "epoch 2280: loss = 0.0294\n",
      "epoch 2281: loss = 0.1390\n",
      "epoch 2282: loss = 0.1917\n",
      "epoch 2283: loss = 0.1000\n",
      "epoch 2284: loss = 0.1026\n",
      "epoch 2285: loss = 0.0947\n",
      "epoch 2286: loss = 0.0904\n",
      "epoch 2287: loss = 0.1405\n",
      "epoch 2288: loss = 0.1718\n",
      "epoch 2289: loss = 0.1157\n",
      "epoch 2290: loss = 0.0410\n",
      "epoch 2291: loss = 0.0513\n",
      "epoch 2292: loss = 0.0463\n",
      "epoch 2293: loss = 0.0878\n",
      "epoch 2294: loss = 0.1221\n",
      "epoch 2295: loss = 0.1283\n",
      "epoch 2296: loss = 0.1669\n",
      "epoch 2297: loss = 0.1037\n",
      "epoch 2298: loss = 0.1030\n",
      "epoch 2299: loss = 0.0805\n",
      "epoch 2300: loss = 0.0979\n",
      "epoch 2301: loss = 0.1786\n",
      "epoch 2302: loss = 0.0814\n",
      "epoch 2303: loss = 0.1037\n",
      "epoch 2304: loss = 0.1634\n",
      "epoch 2305: loss = 0.0858\n",
      "epoch 2306: loss = 0.1114\n",
      "epoch 2307: loss = 0.1072\n",
      "epoch 2308: loss = 0.1071\n",
      "epoch 2309: loss = 0.0975\n",
      "epoch 2310: loss = 0.1127\n",
      "epoch 2311: loss = 0.1472\n",
      "epoch 2312: loss = 0.1363\n",
      "epoch 2313: loss = 0.1137\n",
      "epoch 2314: loss = 0.1561\n",
      "epoch 2315: loss = 0.1700\n",
      "epoch 2316: loss = 0.0759\n",
      "epoch 2317: loss = 0.0862\n",
      "epoch 2318: loss = 0.1153\n",
      "epoch 2319: loss = 0.0627\n",
      "epoch 2320: loss = 0.1475\n",
      "epoch 2321: loss = 0.1107\n",
      "epoch 2322: loss = 0.1005\n",
      "epoch 2323: loss = 0.1172\n",
      "epoch 2324: loss = 0.1505\n",
      "epoch 2325: loss = 0.0930\n",
      "epoch 2326: loss = 0.1070\n",
      "epoch 2327: loss = 0.1314\n",
      "epoch 2328: loss = 0.1070\n",
      "epoch 2329: loss = 0.0995\n",
      "epoch 2330: loss = 0.0939\n",
      "epoch 2331: loss = 0.0587\n",
      "epoch 2332: loss = 0.1183\n",
      "epoch 2333: loss = 0.1078\n",
      "epoch 2334: loss = 0.0883\n",
      "epoch 2335: loss = 0.0879\n",
      "epoch 2336: loss = 0.0615\n",
      "epoch 2337: loss = 0.1499\n",
      "epoch 2338: loss = 0.1795\n",
      "epoch 2339: loss = 0.0951\n",
      "epoch 2340: loss = 0.1379\n",
      "epoch 2341: loss = 0.1434\n",
      "epoch 2342: loss = 0.1507\n",
      "epoch 2343: loss = 0.0643\n",
      "epoch 2344: loss = 0.0611\n",
      "epoch 2345: loss = 0.1554\n",
      "epoch 2346: loss = 0.1777\n",
      "epoch 2347: loss = 0.1464\n",
      "epoch 2348: loss = 0.1495\n",
      "epoch 2349: loss = 0.0591\n",
      "epoch 2350: loss = 0.0884\n",
      "epoch 2351: loss = 0.0803\n",
      "epoch 2352: loss = 0.1360\n",
      "epoch 2353: loss = 0.1131\n",
      "epoch 2354: loss = 0.1638\n",
      "epoch 2355: loss = 0.0647\n",
      "epoch 2356: loss = 0.0653\n",
      "epoch 2357: loss = 0.0983\n",
      "epoch 2358: loss = 0.1918\n",
      "epoch 2359: loss = 0.1176\n",
      "epoch 2360: loss = 0.0612\n",
      "epoch 2361: loss = 0.0490\n",
      "epoch 2362: loss = 0.1441\n",
      "epoch 2363: loss = 0.1012\n",
      "epoch 2364: loss = 0.1409\n",
      "epoch 2365: loss = 0.1837\n",
      "epoch 2366: loss = 0.1599\n",
      "epoch 2367: loss = 0.1566\n",
      "epoch 2368: loss = 0.1589\n",
      "epoch 2369: loss = 0.1396\n",
      "epoch 2370: loss = 0.0865\n",
      "epoch 2371: loss = 0.0929\n",
      "epoch 2372: loss = 0.1290\n",
      "epoch 2373: loss = 0.1606\n",
      "epoch 2374: loss = 0.0561\n",
      "epoch 2375: loss = 0.1741\n",
      "epoch 2376: loss = 0.0537\n",
      "epoch 2377: loss = 0.0726\n",
      "epoch 2378: loss = 0.0492\n",
      "epoch 2379: loss = 0.1503\n",
      "epoch 2380: loss = 0.1851\n",
      "epoch 2381: loss = 0.1155\n",
      "epoch 2382: loss = 0.0478\n",
      "epoch 2383: loss = 0.0739\n",
      "epoch 2384: loss = 0.0470\n",
      "epoch 2385: loss = 0.1602\n",
      "epoch 2386: loss = 0.1091\n",
      "epoch 2387: loss = 0.0669\n",
      "epoch 2388: loss = 0.1125\n",
      "epoch 2389: loss = 0.0772\n",
      "epoch 2390: loss = 0.0583\n",
      "epoch 2391: loss = 0.0843\n",
      "epoch 2392: loss = 0.1819\n",
      "epoch 2393: loss = 0.1101\n",
      "epoch 2394: loss = 0.1308\n",
      "epoch 2395: loss = 0.1337\n",
      "epoch 2396: loss = 0.1565\n",
      "epoch 2397: loss = 0.1580\n",
      "epoch 2398: loss = 0.0996\n",
      "epoch 2399: loss = 0.0573\n",
      "epoch 2400: loss = 0.0836\n",
      "epoch 2401: loss = 0.0969\n",
      "epoch 2402: loss = 0.1513\n",
      "epoch 2403: loss = 0.1830\n",
      "epoch 2404: loss = 0.1582\n",
      "epoch 2405: loss = 0.1118\n",
      "epoch 2406: loss = 0.1452\n",
      "epoch 2407: loss = 0.1350\n",
      "epoch 2408: loss = 0.1393\n",
      "epoch 2409: loss = 0.0908\n",
      "epoch 2410: loss = 0.1531\n",
      "epoch 2411: loss = 0.0776\n",
      "epoch 2412: loss = 0.0836\n",
      "epoch 2413: loss = 0.1557\n",
      "epoch 2414: loss = 0.0906\n",
      "epoch 2415: loss = 0.1506\n",
      "epoch 2416: loss = 0.1890\n",
      "epoch 2417: loss = 0.1578\n",
      "epoch 2418: loss = 0.1625\n",
      "epoch 2419: loss = 0.0187\n",
      "epoch 2420: loss = 0.0917\n",
      "epoch 2421: loss = 0.0484\n",
      "epoch 2422: loss = 0.1083\n",
      "epoch 2423: loss = 0.1178\n",
      "epoch 2424: loss = 0.1444\n",
      "epoch 2425: loss = 0.0432\n",
      "epoch 2426: loss = 0.1151\n",
      "epoch 2427: loss = 0.1127\n",
      "epoch 2428: loss = 0.1406\n",
      "epoch 2429: loss = 0.1169\n",
      "epoch 2430: loss = 0.0496\n",
      "epoch 2431: loss = 0.1370\n",
      "epoch 2432: loss = 0.1476\n",
      "epoch 2433: loss = 0.1078\n",
      "epoch 2434: loss = 0.0407\n",
      "epoch 2435: loss = 0.1299\n",
      "epoch 2436: loss = 0.1684\n",
      "epoch 2437: loss = 0.1628\n",
      "epoch 2438: loss = 0.1535\n",
      "epoch 2439: loss = 0.1320\n",
      "epoch 2440: loss = 0.1119\n",
      "epoch 2441: loss = 0.1311\n",
      "epoch 2442: loss = 0.1628\n",
      "epoch 2443: loss = 0.1763\n",
      "epoch 2444: loss = 0.1422\n",
      "epoch 2445: loss = 0.1232\n",
      "epoch 2446: loss = 0.1074\n",
      "epoch 2447: loss = 0.0565\n",
      "epoch 2448: loss = 0.1067\n",
      "epoch 2449: loss = 0.0927\n",
      "epoch 2450: loss = 0.1232\n",
      "epoch 2451: loss = 0.1125\n",
      "epoch 2452: loss = 0.1463\n",
      "epoch 2453: loss = 0.1479\n",
      "epoch 2454: loss = 0.1906\n",
      "epoch 2455: loss = 0.1007\n",
      "epoch 2456: loss = 0.1455\n",
      "epoch 2457: loss = 0.0707\n",
      "epoch 2458: loss = 0.0682\n",
      "epoch 2459: loss = 0.0172\n",
      "epoch 2460: loss = 0.1292\n",
      "epoch 2461: loss = 0.0624\n",
      "epoch 2462: loss = 0.1757\n",
      "epoch 2463: loss = 0.1639\n",
      "epoch 2464: loss = 0.2018\n",
      "epoch 2465: loss = 0.1653\n",
      "epoch 2466: loss = 0.0742\n",
      "epoch 2467: loss = 0.1663\n",
      "epoch 2468: loss = 0.1389\n",
      "epoch 2469: loss = 0.0861\n",
      "epoch 2470: loss = 0.0216\n",
      "epoch 2471: loss = 0.0965\n",
      "epoch 2472: loss = 0.1429\n",
      "epoch 2473: loss = 0.1547\n",
      "epoch 2474: loss = 0.1441\n",
      "epoch 2475: loss = 0.1864\n",
      "epoch 2476: loss = 0.1670\n",
      "epoch 2477: loss = 0.0977\n",
      "epoch 2478: loss = 0.0517\n",
      "epoch 2479: loss = 0.0367\n",
      "epoch 2480: loss = 0.1308\n",
      "epoch 2481: loss = 0.1923\n",
      "epoch 2482: loss = 0.1583\n",
      "epoch 2483: loss = 0.1183\n",
      "epoch 2484: loss = 0.1114\n",
      "epoch 2485: loss = 0.0725\n",
      "epoch 2486: loss = 0.0613\n",
      "epoch 2487: loss = 0.0762\n",
      "epoch 2488: loss = 0.1707\n",
      "epoch 2489: loss = 0.1103\n",
      "epoch 2490: loss = 0.1349\n",
      "epoch 2491: loss = 0.0920\n",
      "epoch 2492: loss = 0.1476\n",
      "epoch 2493: loss = 0.1339\n",
      "epoch 2494: loss = 0.1517\n",
      "epoch 2495: loss = 0.0962\n",
      "epoch 2496: loss = 0.0918\n",
      "epoch 2497: loss = 0.1370\n",
      "epoch 2498: loss = 0.1263\n",
      "epoch 2499: loss = 0.0212\n",
      "epoch 2500: loss = 0.1294\n",
      "epoch 2501: loss = 0.1240\n",
      "epoch 2502: loss = 0.0631\n",
      "epoch 2503: loss = 0.1066\n",
      "epoch 2504: loss = 0.1439\n",
      "epoch 2505: loss = 0.0320\n",
      "epoch 2506: loss = 0.1075\n",
      "epoch 2507: loss = 0.1525\n",
      "epoch 2508: loss = 0.1248\n",
      "epoch 2509: loss = 0.1354\n",
      "epoch 2510: loss = 0.1648\n",
      "epoch 2511: loss = 0.0826\n",
      "epoch 2512: loss = 0.1050\n",
      "epoch 2513: loss = 0.0622\n",
      "epoch 2514: loss = 0.0808\n",
      "epoch 2515: loss = 0.0666\n",
      "epoch 2516: loss = 0.1967\n",
      "epoch 2517: loss = 0.0536\n",
      "epoch 2518: loss = 0.1391\n",
      "epoch 2519: loss = 0.1569\n",
      "epoch 2520: loss = 0.1829\n",
      "epoch 2521: loss = 0.0544\n",
      "epoch 2522: loss = 0.1166\n",
      "epoch 2523: loss = 0.0965\n",
      "epoch 2524: loss = 0.1377\n",
      "epoch 2525: loss = 0.1242\n",
      "epoch 2526: loss = 0.1624\n",
      "epoch 2527: loss = 0.1130\n",
      "epoch 2528: loss = 0.0218\n",
      "epoch 2529: loss = 0.0752\n",
      "epoch 2530: loss = 0.1630\n",
      "epoch 2531: loss = 0.1726\n",
      "epoch 2532: loss = 0.1393\n",
      "epoch 2533: loss = 0.1101\n",
      "epoch 2534: loss = 0.0795\n",
      "epoch 2535: loss = 0.1398\n",
      "epoch 2536: loss = 0.0871\n",
      "epoch 2537: loss = 0.0741\n",
      "epoch 2538: loss = 0.1264\n",
      "epoch 2539: loss = 0.1098\n",
      "epoch 2540: loss = 0.1897\n",
      "epoch 2541: loss = 0.0620\n",
      "epoch 2542: loss = 0.1129\n",
      "epoch 2543: loss = 0.0814\n",
      "epoch 2544: loss = 0.0736\n",
      "epoch 2545: loss = 0.1166\n",
      "epoch 2546: loss = 0.1116\n",
      "epoch 2547: loss = 0.0912\n",
      "epoch 2548: loss = 0.1297\n",
      "epoch 2549: loss = 0.0692\n",
      "epoch 2550: loss = 0.1571\n",
      "epoch 2551: loss = 0.1650\n",
      "epoch 2552: loss = 0.0807\n",
      "epoch 2553: loss = 0.1765\n",
      "epoch 2554: loss = 0.1842\n",
      "epoch 2555: loss = 0.0706\n",
      "epoch 2556: loss = 0.1021\n",
      "epoch 2557: loss = 0.0605\n",
      "epoch 2558: loss = 0.1228\n",
      "epoch 2559: loss = 0.1302\n",
      "epoch 2560: loss = 0.1003\n",
      "epoch 2561: loss = 0.0407\n",
      "epoch 2562: loss = 0.0563\n",
      "epoch 2563: loss = 0.0827\n",
      "epoch 2564: loss = 0.0734\n",
      "epoch 2565: loss = 0.0606\n",
      "epoch 2566: loss = 0.0817\n",
      "epoch 2567: loss = 0.1132\n",
      "epoch 2568: loss = 0.1455\n",
      "epoch 2569: loss = 0.0910\n",
      "epoch 2570: loss = 0.0865\n",
      "epoch 2571: loss = 0.1248\n",
      "epoch 2572: loss = 0.0293\n",
      "epoch 2573: loss = 0.0384\n",
      "epoch 2574: loss = 0.0593\n",
      "epoch 2575: loss = 0.1379\n",
      "epoch 2576: loss = 0.0701\n",
      "epoch 2577: loss = 0.1196\n",
      "epoch 2578: loss = 0.1055\n",
      "epoch 2579: loss = 0.0870\n",
      "epoch 2580: loss = 0.1050\n",
      "epoch 2581: loss = 0.1049\n",
      "epoch 2582: loss = 0.0995\n",
      "epoch 2583: loss = 0.0717\n",
      "epoch 2584: loss = 0.0415\n",
      "epoch 2585: loss = 0.1312\n",
      "epoch 2586: loss = 0.0844\n",
      "epoch 2587: loss = 0.0903\n",
      "epoch 2588: loss = 0.0793\n",
      "epoch 2589: loss = 0.0999\n",
      "epoch 2590: loss = 0.1063\n",
      "epoch 2591: loss = 0.1699\n",
      "epoch 2592: loss = 0.1674\n",
      "epoch 2593: loss = 0.1051\n",
      "epoch 2594: loss = 0.1496\n",
      "epoch 2595: loss = 0.0818\n",
      "epoch 2596: loss = 0.0838\n",
      "epoch 2597: loss = 0.0579\n",
      "epoch 2598: loss = 0.0821\n",
      "epoch 2599: loss = 0.0591\n",
      "epoch 2600: loss = 0.1130\n",
      "epoch 2601: loss = 0.1146\n",
      "epoch 2602: loss = 0.0635\n",
      "epoch 2603: loss = 0.0859\n",
      "epoch 2604: loss = 0.1378\n",
      "epoch 2605: loss = 0.0733\n",
      "epoch 2606: loss = 0.1160\n",
      "epoch 2607: loss = 0.1419\n",
      "epoch 2608: loss = 0.0804\n",
      "epoch 2609: loss = 0.0818\n",
      "epoch 2610: loss = 0.0646\n",
      "epoch 2611: loss = 0.1870\n",
      "epoch 2612: loss = 0.1583\n",
      "epoch 2613: loss = 0.1609\n",
      "epoch 2614: loss = 0.1533\n",
      "epoch 2615: loss = 0.1011\n",
      "epoch 2616: loss = 0.0812\n",
      "epoch 2617: loss = 0.0848\n",
      "epoch 2618: loss = 0.0585\n",
      "epoch 2619: loss = 0.0868\n",
      "epoch 2620: loss = 0.1101\n",
      "epoch 2621: loss = 0.1039\n",
      "epoch 2622: loss = 0.1745\n",
      "epoch 2623: loss = 0.1070\n",
      "epoch 2624: loss = 0.0780\n",
      "epoch 2625: loss = 0.0635\n",
      "epoch 2626: loss = 0.1193\n",
      "epoch 2627: loss = 0.1476\n",
      "epoch 2628: loss = 0.1215\n",
      "epoch 2629: loss = 0.0361\n",
      "epoch 2630: loss = 0.1576\n",
      "epoch 2631: loss = 0.1244\n",
      "epoch 2632: loss = 0.1467\n",
      "epoch 2633: loss = 0.1543\n",
      "epoch 2634: loss = 0.1527\n",
      "epoch 2635: loss = 0.1640\n",
      "epoch 2636: loss = 0.0738\n",
      "epoch 2637: loss = 0.0383\n",
      "epoch 2638: loss = 0.0999\n",
      "epoch 2639: loss = 0.1693\n",
      "epoch 2640: loss = 0.0813\n",
      "epoch 2641: loss = 0.1697\n",
      "epoch 2642: loss = 0.1581\n",
      "epoch 2643: loss = 0.1390\n",
      "epoch 2644: loss = 0.0749\n",
      "epoch 2645: loss = 0.1170\n",
      "epoch 2646: loss = 0.1225\n",
      "epoch 2647: loss = 0.1282\n",
      "epoch 2648: loss = 0.0822\n",
      "epoch 2649: loss = 0.0675\n",
      "epoch 2650: loss = 0.1399\n",
      "epoch 2651: loss = 0.1365\n",
      "epoch 2652: loss = 0.1243\n",
      "epoch 2653: loss = 0.1745\n",
      "epoch 2654: loss = 0.0916\n",
      "epoch 2655: loss = 0.0831\n",
      "epoch 2656: loss = 0.0805\n",
      "epoch 2657: loss = 0.0717\n",
      "epoch 2658: loss = 0.1011\n",
      "epoch 2659: loss = 0.0903\n",
      "epoch 2660: loss = 0.0917\n",
      "epoch 2661: loss = 0.0868\n",
      "epoch 2662: loss = 0.1285\n",
      "epoch 2663: loss = 0.0893\n",
      "epoch 2664: loss = 0.1405\n",
      "epoch 2665: loss = 0.1266\n",
      "epoch 2666: loss = 0.1194\n",
      "epoch 2667: loss = 0.0656\n",
      "epoch 2668: loss = 0.1205\n",
      "epoch 2669: loss = 0.0992\n",
      "epoch 2670: loss = 0.0609\n",
      "epoch 2671: loss = 0.1219\n",
      "epoch 2672: loss = 0.0541\n",
      "epoch 2673: loss = 0.1224\n",
      "epoch 2674: loss = 0.0832\n",
      "epoch 2675: loss = 0.1461\n",
      "epoch 2676: loss = 0.1892\n",
      "epoch 2677: loss = 0.1079\n",
      "epoch 2678: loss = 0.1355\n",
      "epoch 2679: loss = 0.1006\n",
      "epoch 2680: loss = 0.1313\n",
      "epoch 2681: loss = 0.1004\n",
      "epoch 2682: loss = 0.0807\n",
      "epoch 2683: loss = 0.1149\n",
      "epoch 2684: loss = 0.1101\n",
      "epoch 2685: loss = 0.0747\n",
      "epoch 2686: loss = 0.1490\n",
      "epoch 2687: loss = 0.0827\n",
      "epoch 2688: loss = 0.0996\n",
      "epoch 2689: loss = 0.1054\n",
      "epoch 2690: loss = 0.1371\n",
      "epoch 2691: loss = 0.1077\n",
      "epoch 2692: loss = 0.0033\n",
      "epoch 2693: loss = 0.0813\n",
      "epoch 2694: loss = 0.0854\n",
      "epoch 2695: loss = 0.0684\n",
      "epoch 2696: loss = 0.1015\n",
      "epoch 2697: loss = 0.1174\n",
      "epoch 2698: loss = 0.1136\n",
      "epoch 2699: loss = 0.1118\n",
      "epoch 2700: loss = 0.0414\n",
      "epoch 2701: loss = 0.1045\n",
      "epoch 2702: loss = 0.1311\n",
      "epoch 2703: loss = 0.1132\n",
      "epoch 2704: loss = 0.1460\n",
      "epoch 2705: loss = 0.1130\n",
      "epoch 2706: loss = 0.1170\n",
      "epoch 2707: loss = 0.0807\n",
      "epoch 2708: loss = 0.0756\n",
      "epoch 2709: loss = 0.1332\n",
      "epoch 2710: loss = 0.1489\n",
      "epoch 2711: loss = 0.0915\n",
      "epoch 2712: loss = 0.0408\n",
      "epoch 2713: loss = 0.1185\n",
      "epoch 2714: loss = 0.1609\n",
      "epoch 2715: loss = 0.1449\n",
      "epoch 2716: loss = 0.0598\n",
      "epoch 2717: loss = 0.1256\n",
      "epoch 2718: loss = 0.0894\n",
      "epoch 2719: loss = 0.0935\n",
      "epoch 2720: loss = 0.0807\n",
      "epoch 2721: loss = 0.1199\n",
      "epoch 2722: loss = 0.1138\n",
      "epoch 2723: loss = 0.1186\n",
      "epoch 2724: loss = 0.1015\n",
      "epoch 2725: loss = 0.0474\n",
      "epoch 2726: loss = 0.1036\n",
      "epoch 2727: loss = 0.1335\n",
      "epoch 2728: loss = 0.1056\n",
      "epoch 2729: loss = 0.0686\n",
      "epoch 2730: loss = 0.1671\n",
      "epoch 2731: loss = 0.1636\n",
      "epoch 2732: loss = 0.0419\n",
      "epoch 2733: loss = 0.1091\n",
      "epoch 2734: loss = 0.1718\n",
      "epoch 2735: loss = 0.1549\n",
      "epoch 2736: loss = 0.1249\n",
      "epoch 2737: loss = 0.0525\n",
      "epoch 2738: loss = 0.1731\n",
      "epoch 2739: loss = 0.0175\n",
      "epoch 2740: loss = 0.0979\n",
      "epoch 2741: loss = 0.1154\n",
      "epoch 2742: loss = 0.1119\n",
      "epoch 2743: loss = 0.1482\n",
      "epoch 2744: loss = 0.1242\n",
      "epoch 2745: loss = 0.1378\n",
      "epoch 2746: loss = 0.0893\n",
      "epoch 2747: loss = 0.0763\n",
      "epoch 2748: loss = 0.0845\n",
      "epoch 2749: loss = 0.0205\n",
      "epoch 2750: loss = 0.0628\n",
      "epoch 2751: loss = 0.0663\n",
      "epoch 2752: loss = 0.0765\n",
      "epoch 2753: loss = 0.1550\n",
      "epoch 2754: loss = 0.0561\n",
      "epoch 2755: loss = 0.0779\n",
      "epoch 2756: loss = 0.1364\n",
      "epoch 2757: loss = 0.0808\n",
      "epoch 2758: loss = 0.0818\n",
      "epoch 2759: loss = 0.1544\n",
      "epoch 2760: loss = 0.1201\n",
      "epoch 2761: loss = 0.1523\n",
      "epoch 2762: loss = 0.1338\n",
      "epoch 2763: loss = 0.1796\n",
      "epoch 2764: loss = 0.1524\n",
      "epoch 2765: loss = 0.0899\n",
      "epoch 2766: loss = 0.1032\n",
      "epoch 2767: loss = 0.0859\n",
      "epoch 2768: loss = 0.1655\n",
      "epoch 2769: loss = 0.1513\n",
      "epoch 2770: loss = 0.1176\n",
      "epoch 2771: loss = 0.1186\n",
      "epoch 2772: loss = 0.1295\n",
      "epoch 2773: loss = 0.1033\n",
      "epoch 2774: loss = 0.1350\n",
      "epoch 2775: loss = 0.1597\n",
      "epoch 2776: loss = 0.1078\n",
      "epoch 2777: loss = 0.1698\n",
      "epoch 2778: loss = 0.0714\n",
      "epoch 2779: loss = 0.0813\n",
      "epoch 2780: loss = 0.0433\n",
      "epoch 2781: loss = 0.1054\n",
      "epoch 2782: loss = 0.0834\n",
      "epoch 2783: loss = 0.0674\n",
      "epoch 2784: loss = 0.1044\n",
      "epoch 2785: loss = 0.1195\n",
      "epoch 2786: loss = 0.1209\n",
      "epoch 2787: loss = 0.1763\n",
      "epoch 2788: loss = 0.1089\n",
      "epoch 2789: loss = 0.0975\n",
      "epoch 2790: loss = 0.0894\n",
      "epoch 2791: loss = 0.0992\n",
      "epoch 2792: loss = 0.0754\n",
      "epoch 2793: loss = 0.0932\n",
      "epoch 2794: loss = 0.0627\n",
      "epoch 2795: loss = 0.0932\n",
      "epoch 2796: loss = 0.1509\n",
      "epoch 2797: loss = 0.1443\n",
      "epoch 2798: loss = 0.1274\n",
      "epoch 2799: loss = 0.0803\n",
      "epoch 2800: loss = 0.1329\n",
      "epoch 2801: loss = 0.1728\n",
      "epoch 2802: loss = 0.1115\n",
      "epoch 2803: loss = 0.0775\n",
      "epoch 2804: loss = 0.0235\n",
      "epoch 2805: loss = 0.1467\n",
      "epoch 2806: loss = 0.1168\n",
      "epoch 2807: loss = 0.1324\n",
      "epoch 2808: loss = 0.1528\n",
      "epoch 2809: loss = 0.0923\n",
      "epoch 2810: loss = 0.1349\n",
      "epoch 2811: loss = 0.1074\n",
      "epoch 2812: loss = 0.1219\n",
      "epoch 2813: loss = 0.0414\n",
      "epoch 2814: loss = 0.0393\n",
      "epoch 2815: loss = 0.0368\n",
      "epoch 2816: loss = 0.1107\n",
      "epoch 2817: loss = 0.1058\n",
      "epoch 2818: loss = 0.1486\n",
      "epoch 2819: loss = 0.0974\n",
      "epoch 2820: loss = 0.1174\n",
      "epoch 2821: loss = 0.0496\n",
      "epoch 2822: loss = 0.0689\n",
      "epoch 2823: loss = 0.0936\n",
      "epoch 2824: loss = 0.0998\n",
      "epoch 2825: loss = 0.1390\n",
      "epoch 2826: loss = 0.0746\n",
      "epoch 2827: loss = 0.0641\n",
      "epoch 2828: loss = 0.0864\n",
      "epoch 2829: loss = 0.1322\n",
      "epoch 2830: loss = 0.0887\n",
      "epoch 2831: loss = 0.1323\n",
      "epoch 2832: loss = 0.0654\n",
      "epoch 2833: loss = 0.1668\n",
      "epoch 2834: loss = 0.1217\n",
      "epoch 2835: loss = 0.1065\n",
      "epoch 2836: loss = 0.0504\n",
      "epoch 2837: loss = 0.0889\n",
      "epoch 2838: loss = 0.1685\n",
      "epoch 2839: loss = 0.1131\n",
      "epoch 2840: loss = 0.1485\n",
      "epoch 2841: loss = 0.1073\n",
      "epoch 2842: loss = 0.1202\n",
      "epoch 2843: loss = 0.0709\n",
      "epoch 2844: loss = 0.1628\n",
      "epoch 2845: loss = 0.0996\n",
      "epoch 2846: loss = 0.0689\n",
      "epoch 2847: loss = 0.0282\n",
      "epoch 2848: loss = 0.1635\n",
      "epoch 2849: loss = 0.0425\n",
      "epoch 2850: loss = 0.1267\n",
      "epoch 2851: loss = 0.1153\n",
      "epoch 2852: loss = 0.1233\n",
      "epoch 2853: loss = 0.0858\n",
      "epoch 2854: loss = 0.1281\n",
      "epoch 2855: loss = 0.1623\n",
      "epoch 2856: loss = 0.1024\n",
      "epoch 2857: loss = 0.0926\n",
      "epoch 2858: loss = 0.1082\n",
      "epoch 2859: loss = 0.1091\n",
      "epoch 2860: loss = 0.1498\n",
      "epoch 2861: loss = 0.1418\n",
      "epoch 2862: loss = 0.1594\n",
      "epoch 2863: loss = 0.1688\n",
      "epoch 2864: loss = 0.1185\n",
      "epoch 2865: loss = 0.0790\n",
      "epoch 2866: loss = 0.0951\n",
      "epoch 2867: loss = 0.1589\n",
      "epoch 2868: loss = 0.0757\n",
      "epoch 2869: loss = 0.1341\n",
      "epoch 2870: loss = 0.0268\n",
      "epoch 2871: loss = 0.1367\n",
      "epoch 2872: loss = 0.0493\n",
      "epoch 2873: loss = 0.0938\n",
      "epoch 2874: loss = 0.1537\n",
      "epoch 2875: loss = 0.0966\n",
      "epoch 2876: loss = 0.0809\n",
      "epoch 2877: loss = 0.1576\n",
      "epoch 2878: loss = 0.1160\n",
      "epoch 2879: loss = 0.1405\n",
      "epoch 2880: loss = 0.0685\n",
      "epoch 2881: loss = 0.0843\n",
      "epoch 2882: loss = 0.1859\n",
      "epoch 2883: loss = 0.1504\n",
      "epoch 2884: loss = 0.0713\n",
      "epoch 2885: loss = 0.0906\n",
      "epoch 2886: loss = 0.1380\n",
      "epoch 2887: loss = 0.1693\n",
      "epoch 2888: loss = 0.1041\n",
      "epoch 2889: loss = 0.1140\n",
      "epoch 2890: loss = 0.0498\n",
      "epoch 2891: loss = 0.0995\n",
      "epoch 2892: loss = 0.0453\n",
      "epoch 2893: loss = 0.0790\n",
      "epoch 2894: loss = 0.1038\n",
      "epoch 2895: loss = 0.1815\n",
      "epoch 2896: loss = 0.0907\n",
      "epoch 2897: loss = 0.1040\n",
      "epoch 2898: loss = 0.1388\n",
      "epoch 2899: loss = 0.0909\n",
      "epoch 2900: loss = 0.1821\n",
      "epoch 2901: loss = 0.0240\n",
      "epoch 2902: loss = 0.1675\n",
      "epoch 2903: loss = 0.1210\n",
      "epoch 2904: loss = 0.0369\n",
      "epoch 2905: loss = 0.1281\n",
      "epoch 2906: loss = 0.0685\n",
      "epoch 2907: loss = 0.0595\n",
      "epoch 2908: loss = 0.0266\n",
      "epoch 2909: loss = 0.0601\n",
      "epoch 2910: loss = 0.1403\n",
      "epoch 2911: loss = 0.1540\n",
      "epoch 2912: loss = 0.0732\n",
      "epoch 2913: loss = 0.0854\n",
      "epoch 2914: loss = 0.1604\n",
      "epoch 2915: loss = 0.0763\n",
      "epoch 2916: loss = 0.0921\n",
      "epoch 2917: loss = 0.0857\n",
      "epoch 2918: loss = 0.1290\n",
      "epoch 2919: loss = 0.0947\n",
      "epoch 2920: loss = 0.0627\n",
      "epoch 2921: loss = 0.1118\n",
      "epoch 2922: loss = 0.1371\n",
      "epoch 2923: loss = 0.1975\n",
      "epoch 2924: loss = 0.0665\n",
      "epoch 2925: loss = 0.1743\n",
      "epoch 2926: loss = 0.0794\n",
      "epoch 2927: loss = 0.0656\n",
      "epoch 2928: loss = 0.1620\n",
      "epoch 2929: loss = 0.0289\n",
      "epoch 2930: loss = 0.0898\n",
      "epoch 2931: loss = 0.1557\n",
      "epoch 2932: loss = 0.0873\n",
      "epoch 2933: loss = 0.1099\n",
      "epoch 2934: loss = 0.0640\n",
      "epoch 2935: loss = 0.0977\n",
      "epoch 2936: loss = 0.1899\n",
      "epoch 2937: loss = 0.1310\n",
      "epoch 2938: loss = 0.1704\n",
      "epoch 2939: loss = 0.1485\n",
      "epoch 2940: loss = 0.1124\n",
      "epoch 2941: loss = 0.0175\n",
      "epoch 2942: loss = 0.0825\n",
      "epoch 2943: loss = 0.1000\n",
      "epoch 2944: loss = 0.1497\n",
      "epoch 2945: loss = 0.1291\n",
      "epoch 2946: loss = 0.1800\n",
      "epoch 2947: loss = 0.0601\n",
      "epoch 2948: loss = 0.1579\n",
      "epoch 2949: loss = 0.1335\n",
      "epoch 2950: loss = 0.0242\n",
      "epoch 2951: loss = 0.0540\n",
      "epoch 2952: loss = 0.1351\n",
      "epoch 2953: loss = 0.1436\n",
      "epoch 2954: loss = 0.1151\n",
      "epoch 2955: loss = 0.1063\n",
      "epoch 2956: loss = 0.0951\n",
      "epoch 2957: loss = 0.1142\n",
      "epoch 2958: loss = 0.1483\n",
      "epoch 2959: loss = 0.1610\n",
      "epoch 2960: loss = 0.0809\n",
      "epoch 2961: loss = 0.1410\n",
      "epoch 2962: loss = 0.1773\n",
      "epoch 2963: loss = 0.1649\n",
      "epoch 2964: loss = 0.0867\n",
      "epoch 2965: loss = 0.1045\n",
      "epoch 2966: loss = 0.1202\n",
      "epoch 2967: loss = 0.0676\n",
      "epoch 2968: loss = 0.1485\n",
      "epoch 2969: loss = 0.1509\n",
      "epoch 2970: loss = 0.1062\n",
      "epoch 2971: loss = 0.0958\n",
      "epoch 2972: loss = 0.1087\n",
      "epoch 2973: loss = 0.1978\n",
      "epoch 2974: loss = 0.1727\n",
      "epoch 2975: loss = 0.1651\n",
      "epoch 2976: loss = 0.1527\n",
      "epoch 2977: loss = 0.1252\n",
      "epoch 2978: loss = 0.0946\n",
      "epoch 2979: loss = 0.1019\n",
      "epoch 2980: loss = 0.1141\n",
      "epoch 2981: loss = 0.0604\n",
      "epoch 2982: loss = 0.1394\n",
      "epoch 2983: loss = 0.1490\n",
      "epoch 2984: loss = 0.0291\n",
      "epoch 2985: loss = 0.1458\n",
      "epoch 2986: loss = 0.1466\n",
      "epoch 2987: loss = 0.1627\n",
      "epoch 2988: loss = 0.1223\n",
      "epoch 2989: loss = 0.1273\n",
      "epoch 2990: loss = 0.1260\n",
      "epoch 2991: loss = 0.1550\n",
      "epoch 2992: loss = 0.1407\n",
      "epoch 2993: loss = 0.1132\n",
      "epoch 2994: loss = 0.1234\n",
      "epoch 2995: loss = 0.0737\n",
      "epoch 2996: loss = 0.1434\n",
      "epoch 2997: loss = 0.1605\n",
      "epoch 2998: loss = 0.0233\n",
      "epoch 2999: loss = 0.1198\n",
      "epoch 3000: loss = 0.1103\n",
      "epoch 3001: loss = 0.0486\n",
      "epoch 3002: loss = 0.0768\n",
      "epoch 3003: loss = 0.0990\n",
      "epoch 3004: loss = 0.0651\n",
      "epoch 3005: loss = 0.1089\n",
      "epoch 3006: loss = 0.0979\n",
      "epoch 3007: loss = 0.1601\n",
      "epoch 3008: loss = 0.1286\n",
      "epoch 3009: loss = 0.0779\n",
      "epoch 3010: loss = 0.0929\n",
      "epoch 3011: loss = 0.1246\n",
      "epoch 3012: loss = 0.1269\n",
      "epoch 3013: loss = 0.0830\n",
      "epoch 3014: loss = 0.1039\n",
      "epoch 3015: loss = 0.1008\n",
      "epoch 3016: loss = 0.1096\n",
      "epoch 3017: loss = 0.0884\n",
      "epoch 3018: loss = 0.0751\n",
      "epoch 3019: loss = 0.1210\n",
      "epoch 3020: loss = 0.1433\n",
      "epoch 3021: loss = 0.0959\n",
      "epoch 3022: loss = 0.1009\n",
      "epoch 3023: loss = 0.1172\n",
      "epoch 3024: loss = 0.1395\n",
      "epoch 3025: loss = 0.1606\n",
      "epoch 3026: loss = 0.1037\n",
      "epoch 3027: loss = 0.1384\n",
      "epoch 3028: loss = 0.1009\n",
      "epoch 3029: loss = 0.1066\n",
      "epoch 3030: loss = 0.0901\n",
      "epoch 3031: loss = 0.1012\n",
      "epoch 3032: loss = 0.1017\n",
      "epoch 3033: loss = 0.1575\n",
      "epoch 3034: loss = 0.1256\n",
      "epoch 3035: loss = 0.0782\n",
      "epoch 3036: loss = 0.1201\n",
      "epoch 3037: loss = 0.1207\n",
      "epoch 3038: loss = 0.1057\n",
      "epoch 3039: loss = 0.1414\n",
      "epoch 3040: loss = 0.0881\n",
      "epoch 3041: loss = 0.0787\n",
      "epoch 3042: loss = 0.1059\n",
      "epoch 3043: loss = 0.0563\n",
      "epoch 3044: loss = 0.1572\n",
      "epoch 3045: loss = 0.1145\n",
      "epoch 3046: loss = 0.0975\n",
      "epoch 3047: loss = 0.1420\n",
      "epoch 3048: loss = 0.0701\n",
      "epoch 3049: loss = 0.0927\n",
      "epoch 3050: loss = 0.1547\n",
      "epoch 3051: loss = 0.1404\n",
      "epoch 3052: loss = 0.1677\n",
      "epoch 3053: loss = 0.1475\n",
      "epoch 3054: loss = 0.1271\n",
      "epoch 3055: loss = 0.1276\n",
      "epoch 3056: loss = 0.1600\n",
      "epoch 3057: loss = 0.0480\n",
      "epoch 3058: loss = 0.1264\n",
      "epoch 3059: loss = 0.1123\n",
      "epoch 3060: loss = 0.1454\n",
      "epoch 3061: loss = 0.0947\n",
      "epoch 3062: loss = 0.0995\n",
      "epoch 3063: loss = 0.1819\n",
      "epoch 3064: loss = 0.1599\n",
      "epoch 3065: loss = 0.0946\n",
      "epoch 3066: loss = 0.0301\n",
      "epoch 3067: loss = 0.0967\n",
      "epoch 3068: loss = 0.1010\n",
      "epoch 3069: loss = 0.1046\n",
      "epoch 3070: loss = 0.1655\n",
      "epoch 3071: loss = 0.0669\n",
      "epoch 3072: loss = 0.0095\n",
      "epoch 3073: loss = 0.1450\n",
      "epoch 3074: loss = 0.1374\n",
      "epoch 3075: loss = 0.0800\n",
      "epoch 3076: loss = 0.0892\n",
      "epoch 3077: loss = 0.0140\n",
      "epoch 3078: loss = 0.0726\n",
      "epoch 3079: loss = 0.1082\n",
      "epoch 3080: loss = 0.0436\n",
      "epoch 3081: loss = 0.0512\n",
      "epoch 3082: loss = 0.0850\n",
      "epoch 3083: loss = 0.0219\n",
      "epoch 3084: loss = 0.0912\n",
      "epoch 3085: loss = 0.1096\n",
      "epoch 3086: loss = 0.0729\n",
      "epoch 3087: loss = 0.0707\n",
      "epoch 3088: loss = 0.1517\n",
      "epoch 3089: loss = 0.0274\n",
      "epoch 3090: loss = 0.0221\n",
      "epoch 3091: loss = 0.1169\n",
      "epoch 3092: loss = 0.1546\n",
      "epoch 3093: loss = 0.0256\n",
      "epoch 3094: loss = 0.0877\n",
      "epoch 3095: loss = 0.0251\n",
      "epoch 3096: loss = 0.0905\n",
      "epoch 3097: loss = 0.0681\n",
      "epoch 3098: loss = 0.0797\n",
      "epoch 3099: loss = 0.1540\n",
      "epoch 3100: loss = 0.0995\n",
      "epoch 3101: loss = 0.0876\n",
      "epoch 3102: loss = 0.1437\n",
      "epoch 3103: loss = 0.0460\n",
      "epoch 3104: loss = 0.1709\n",
      "epoch 3105: loss = 0.1560\n",
      "epoch 3106: loss = 0.0741\n",
      "epoch 3107: loss = 0.1243\n",
      "epoch 3108: loss = 0.0808\n",
      "epoch 3109: loss = 0.0913\n",
      "epoch 3110: loss = 0.0310\n",
      "epoch 3111: loss = 0.1218\n",
      "epoch 3112: loss = 0.0956\n",
      "epoch 3113: loss = 0.1035\n",
      "epoch 3114: loss = 0.0780\n",
      "epoch 3115: loss = 0.0976\n",
      "epoch 3116: loss = 0.1041\n",
      "epoch 3117: loss = 0.1698\n",
      "epoch 3118: loss = 0.1798\n",
      "epoch 3119: loss = 0.1257\n",
      "epoch 3120: loss = 0.0605\n",
      "epoch 3121: loss = 0.0918\n",
      "epoch 3122: loss = 0.0990\n",
      "epoch 3123: loss = 0.1171\n",
      "epoch 3124: loss = 0.1329\n",
      "epoch 3125: loss = 0.1410\n",
      "epoch 3126: loss = 0.1231\n",
      "epoch 3127: loss = 0.1634\n",
      "epoch 3128: loss = 0.1752\n",
      "epoch 3129: loss = 0.0994\n",
      "epoch 3130: loss = 0.1142\n",
      "epoch 3131: loss = 0.0191\n",
      "epoch 3132: loss = 0.1225\n",
      "epoch 3133: loss = 0.1646\n",
      "epoch 3134: loss = 0.1695\n",
      "epoch 3135: loss = 0.0670\n",
      "epoch 3136: loss = 0.1050\n",
      "epoch 3137: loss = 0.1441\n",
      "epoch 3138: loss = 0.1754\n",
      "epoch 3139: loss = 0.1593\n",
      "epoch 3140: loss = 0.0552\n",
      "epoch 3141: loss = 0.0519\n",
      "epoch 3142: loss = 0.0923\n",
      "epoch 3143: loss = 0.1658\n",
      "epoch 3144: loss = 0.1705\n",
      "epoch 3145: loss = 0.1760\n",
      "epoch 3146: loss = 0.0402\n",
      "epoch 3147: loss = 0.1573\n",
      "epoch 3148: loss = 0.0838\n",
      "epoch 3149: loss = 0.1689\n",
      "epoch 3150: loss = 0.1201\n",
      "epoch 3151: loss = 0.1725\n",
      "epoch 3152: loss = 0.1610\n",
      "epoch 3153: loss = 0.0746\n",
      "epoch 3154: loss = 0.1169\n",
      "epoch 3155: loss = 0.1045\n",
      "epoch 3156: loss = 0.1000\n",
      "epoch 3157: loss = 0.1750\n",
      "epoch 3158: loss = 0.1484\n",
      "epoch 3159: loss = 0.1467\n",
      "epoch 3160: loss = 0.1169\n",
      "epoch 3161: loss = 0.0518\n",
      "epoch 3162: loss = 0.1305\n",
      "epoch 3163: loss = 0.1426\n",
      "epoch 3164: loss = 0.1729\n",
      "epoch 3165: loss = 0.1135\n",
      "epoch 3166: loss = 0.1128\n",
      "epoch 3167: loss = 0.1854\n",
      "epoch 3168: loss = 0.1196\n",
      "epoch 3169: loss = 0.1576\n",
      "epoch 3170: loss = 0.1010\n",
      "epoch 3171: loss = 0.0539\n",
      "epoch 3172: loss = 0.0888\n",
      "epoch 3173: loss = 0.1020\n",
      "epoch 3174: loss = 0.1443\n",
      "epoch 3175: loss = 0.1169\n",
      "epoch 3176: loss = 0.0210\n",
      "epoch 3177: loss = 0.0842\n",
      "epoch 3178: loss = 0.1292\n",
      "epoch 3179: loss = 0.1455\n",
      "epoch 3180: loss = 0.0479\n",
      "epoch 3181: loss = 0.1465\n",
      "epoch 3182: loss = 0.0704\n",
      "epoch 3183: loss = 0.1888\n",
      "epoch 3184: loss = 0.1684\n",
      "epoch 3185: loss = 0.0743\n",
      "epoch 3186: loss = 0.1598\n",
      "epoch 3187: loss = 0.1041\n",
      "epoch 3188: loss = 0.1192\n",
      "epoch 3189: loss = 0.1404\n",
      "epoch 3190: loss = 0.0347\n",
      "epoch 3191: loss = 0.1152\n",
      "epoch 3192: loss = 0.1671\n",
      "epoch 3193: loss = 0.1527\n",
      "epoch 3194: loss = 0.1978\n",
      "epoch 3195: loss = 0.1111\n",
      "epoch 3196: loss = 0.1246\n",
      "epoch 3197: loss = 0.1440\n",
      "epoch 3198: loss = 0.0828\n",
      "epoch 3199: loss = 0.0761\n",
      "epoch 3200: loss = 0.0847\n",
      "epoch 3201: loss = 0.1175\n",
      "epoch 3202: loss = 0.1253\n",
      "epoch 3203: loss = 0.0497\n",
      "epoch 3204: loss = 0.1585\n",
      "epoch 3205: loss = 0.1605\n",
      "epoch 3206: loss = 0.1545\n",
      "epoch 3207: loss = 0.0546\n",
      "epoch 3208: loss = 0.1184\n",
      "epoch 3209: loss = 0.1531\n",
      "epoch 3210: loss = 0.0642\n",
      "epoch 3211: loss = 0.1225\n",
      "epoch 3212: loss = 0.1067\n",
      "epoch 3213: loss = 0.1170\n",
      "epoch 3214: loss = 0.1167\n",
      "epoch 3215: loss = 0.0590\n",
      "epoch 3216: loss = 0.0923\n",
      "epoch 3217: loss = 0.0157\n",
      "epoch 3218: loss = 0.1552\n",
      "epoch 3219: loss = 0.0909\n",
      "epoch 3220: loss = 0.1574\n",
      "epoch 3221: loss = 0.1452\n",
      "epoch 3222: loss = 0.1754\n",
      "epoch 3223: loss = 0.0932\n",
      "epoch 3224: loss = 0.1347\n",
      "epoch 3225: loss = 0.0364\n",
      "epoch 3226: loss = 0.0989\n",
      "epoch 3227: loss = 0.1863\n",
      "epoch 3228: loss = 0.1180\n",
      "epoch 3229: loss = 0.1453\n",
      "epoch 3230: loss = 0.1194\n",
      "epoch 3231: loss = 0.0363\n",
      "epoch 3232: loss = 0.0461\n",
      "epoch 3233: loss = 0.1381\n",
      "epoch 3234: loss = 0.0636\n",
      "epoch 3235: loss = 0.1121\n",
      "epoch 3236: loss = 0.1627\n",
      "epoch 3237: loss = 0.0691\n",
      "epoch 3238: loss = 0.0813\n",
      "epoch 3239: loss = 0.1101\n",
      "epoch 3240: loss = 0.1649\n",
      "epoch 3241: loss = 0.1473\n",
      "epoch 3242: loss = 0.1547\n",
      "epoch 3243: loss = 0.1142\n",
      "epoch 3244: loss = 0.1005\n",
      "epoch 3245: loss = 0.0958\n",
      "epoch 3246: loss = 0.0891\n",
      "epoch 3247: loss = 0.1262\n",
      "epoch 3248: loss = 0.1755\n",
      "epoch 3249: loss = 0.1292\n",
      "epoch 3250: loss = 0.1208\n",
      "epoch 3251: loss = 0.1437\n",
      "epoch 3252: loss = 0.1186\n",
      "epoch 3253: loss = 0.0703\n",
      "epoch 3254: loss = 0.1152\n",
      "epoch 3255: loss = 0.1183\n",
      "epoch 3256: loss = 0.1186\n",
      "epoch 3257: loss = 0.0804\n",
      "epoch 3258: loss = 0.1230\n",
      "epoch 3259: loss = 0.0536\n",
      "epoch 3260: loss = 0.1790\n",
      "epoch 3261: loss = 0.1413\n",
      "epoch 3262: loss = 0.1477\n",
      "epoch 3263: loss = 0.1366\n",
      "epoch 3264: loss = 0.1316\n",
      "epoch 3265: loss = 0.1154\n",
      "epoch 3266: loss = 0.1380\n",
      "epoch 3267: loss = 0.0138\n",
      "epoch 3268: loss = 0.1044\n",
      "epoch 3269: loss = 0.0674\n",
      "epoch 3270: loss = 0.0934\n",
      "epoch 3271: loss = 0.0533\n",
      "epoch 3272: loss = 0.1297\n",
      "epoch 3273: loss = 0.0088\n",
      "epoch 3274: loss = 0.1398\n",
      "epoch 3275: loss = 0.1382\n",
      "epoch 3276: loss = 0.1124\n",
      "epoch 3277: loss = 0.0671\n",
      "epoch 3278: loss = 0.0822\n",
      "epoch 3279: loss = 0.1882\n",
      "epoch 3280: loss = 0.1744\n",
      "epoch 3281: loss = 0.0071\n",
      "epoch 3282: loss = 0.1505\n",
      "epoch 3283: loss = 0.1252\n",
      "epoch 3284: loss = 0.1083\n",
      "epoch 3285: loss = 0.0827\n",
      "epoch 3286: loss = 0.1709\n",
      "epoch 3287: loss = 0.0998\n",
      "epoch 3288: loss = 0.1209\n",
      "epoch 3289: loss = 0.1274\n",
      "epoch 3290: loss = 0.1143\n",
      "epoch 3291: loss = 0.0856\n",
      "epoch 3292: loss = 0.1436\n",
      "epoch 3293: loss = 0.0969\n",
      "epoch 3294: loss = 0.1140\n",
      "epoch 3295: loss = 0.1091\n",
      "epoch 3296: loss = 0.1003\n",
      "epoch 3297: loss = 0.1846\n",
      "epoch 3298: loss = 0.1816\n",
      "epoch 3299: loss = 0.1456\n",
      "epoch 3300: loss = 0.1337\n",
      "epoch 3301: loss = 0.0890\n",
      "epoch 3302: loss = 0.0966\n",
      "epoch 3303: loss = 0.0746\n",
      "epoch 3304: loss = 0.0763\n",
      "epoch 3305: loss = 0.0560\n",
      "epoch 3306: loss = 0.1306\n",
      "epoch 3307: loss = 0.0987\n",
      "epoch 3308: loss = 0.0724\n",
      "epoch 3309: loss = 0.1271\n",
      "epoch 3310: loss = 0.1150\n",
      "epoch 3311: loss = 0.1168\n",
      "epoch 3312: loss = 0.0834\n",
      "epoch 3313: loss = 0.0822\n",
      "epoch 3314: loss = 0.1516\n",
      "epoch 3315: loss = 0.1610\n",
      "epoch 3316: loss = 0.1309\n",
      "epoch 3317: loss = 0.1071\n",
      "epoch 3318: loss = 0.1352\n",
      "epoch 3319: loss = 0.0627\n",
      "epoch 3320: loss = 0.1113\n",
      "epoch 3321: loss = 0.0652\n",
      "epoch 3322: loss = 0.0957\n",
      "epoch 3323: loss = 0.0868\n",
      "epoch 3324: loss = 0.0638\n",
      "epoch 3325: loss = 0.1055\n",
      "epoch 3326: loss = 0.0794\n",
      "epoch 3327: loss = 0.0723\n",
      "epoch 3328: loss = 0.1672\n",
      "epoch 3329: loss = 0.1613\n",
      "epoch 3330: loss = 0.1091\n",
      "epoch 3331: loss = 0.0918\n",
      "epoch 3332: loss = 0.1183\n",
      "epoch 3333: loss = 0.1439\n",
      "epoch 3334: loss = 0.0969\n",
      "epoch 3335: loss = 0.1096\n",
      "epoch 3336: loss = 0.1009\n",
      "epoch 3337: loss = 0.1295\n",
      "epoch 3338: loss = 0.1122\n",
      "epoch 3339: loss = 0.1136\n",
      "epoch 3340: loss = 0.1918\n",
      "epoch 3341: loss = 0.1382\n",
      "epoch 3342: loss = 0.1135\n",
      "epoch 3343: loss = 0.0650\n",
      "epoch 3344: loss = 0.0766\n",
      "epoch 3345: loss = 0.1586\n",
      "epoch 3346: loss = 0.1245\n",
      "epoch 3347: loss = 0.1437\n",
      "epoch 3348: loss = 0.1285\n",
      "epoch 3349: loss = 0.1011\n",
      "epoch 3350: loss = 0.0650\n",
      "epoch 3351: loss = 0.1558\n",
      "epoch 3352: loss = 0.0962\n",
      "epoch 3353: loss = 0.0843\n",
      "epoch 3354: loss = 0.0917\n",
      "epoch 3355: loss = 0.1433\n",
      "epoch 3356: loss = 0.1045\n",
      "epoch 3357: loss = 0.0573\n",
      "epoch 3358: loss = 0.0499\n",
      "epoch 3359: loss = 0.1011\n",
      "epoch 3360: loss = 0.0966\n",
      "epoch 3361: loss = 0.1141\n",
      "epoch 3362: loss = 0.1508\n",
      "epoch 3363: loss = 0.1263\n",
      "epoch 3364: loss = 0.0906\n",
      "epoch 3365: loss = 0.0951\n",
      "epoch 3366: loss = 0.0706\n",
      "epoch 3367: loss = 0.0920\n",
      "epoch 3368: loss = 0.0880\n",
      "epoch 3369: loss = 0.0917\n",
      "epoch 3370: loss = 0.0862\n",
      "epoch 3371: loss = 0.1644\n",
      "epoch 3372: loss = 0.1366\n",
      "epoch 3373: loss = 0.1035\n",
      "epoch 3374: loss = 0.1285\n",
      "epoch 3375: loss = 0.1266\n",
      "epoch 3376: loss = 0.0906\n",
      "epoch 3377: loss = 0.1225\n",
      "epoch 3378: loss = 0.0634\n",
      "epoch 3379: loss = 0.0503\n",
      "epoch 3380: loss = 0.0628\n",
      "epoch 3381: loss = 0.0910\n",
      "epoch 3382: loss = 0.1512\n",
      "epoch 3383: loss = 0.1719\n",
      "epoch 3384: loss = 0.1375\n",
      "epoch 3385: loss = 0.0841\n",
      "epoch 3386: loss = 0.0938\n",
      "epoch 3387: loss = 0.0745\n",
      "epoch 3388: loss = 0.1456\n",
      "epoch 3389: loss = 0.0512\n",
      "epoch 3390: loss = 0.1301\n",
      "epoch 3391: loss = 0.1547\n",
      "epoch 3392: loss = 0.1061\n",
      "epoch 3393: loss = 0.1326\n",
      "epoch 3394: loss = 0.0569\n",
      "epoch 3395: loss = 0.0980\n",
      "epoch 3396: loss = 0.1154\n",
      "epoch 3397: loss = 0.1178\n",
      "epoch 3398: loss = 0.1158\n",
      "epoch 3399: loss = 0.0736\n",
      "epoch 3400: loss = 0.0904\n",
      "epoch 3401: loss = 0.0945\n",
      "epoch 3402: loss = 0.0874\n",
      "epoch 3403: loss = 0.1117\n",
      "epoch 3404: loss = 0.1246\n",
      "epoch 3405: loss = 0.1475\n",
      "epoch 3406: loss = 0.1064\n",
      "epoch 3407: loss = 0.1365\n",
      "epoch 3408: loss = 0.0942\n",
      "epoch 3409: loss = 0.1808\n",
      "epoch 3410: loss = 0.1770\n",
      "epoch 3411: loss = 0.1598\n",
      "epoch 3412: loss = 0.1081\n",
      "epoch 3413: loss = 0.1080\n",
      "epoch 3414: loss = 0.0701\n",
      "epoch 3415: loss = 0.0991\n",
      "epoch 3416: loss = 0.0276\n",
      "epoch 3417: loss = 0.0204\n",
      "epoch 3418: loss = 0.1454\n",
      "epoch 3419: loss = 0.1583\n",
      "epoch 3420: loss = 0.1099\n",
      "epoch 3421: loss = 0.1418\n",
      "epoch 3422: loss = 0.1028\n",
      "epoch 3423: loss = 0.0253\n",
      "epoch 3424: loss = 0.0551\n",
      "epoch 3425: loss = 0.1289\n",
      "epoch 3426: loss = 0.1224\n",
      "epoch 3427: loss = 0.1017\n",
      "epoch 3428: loss = 0.1057\n",
      "epoch 3429: loss = 0.1576\n",
      "epoch 3430: loss = 0.2011\n",
      "epoch 3431: loss = 0.0885\n",
      "epoch 3432: loss = 0.0499\n",
      "epoch 3433: loss = 0.0737\n",
      "epoch 3434: loss = 0.0942\n",
      "epoch 3435: loss = 0.0934\n",
      "epoch 3436: loss = 0.0722\n",
      "epoch 3437: loss = 0.0791\n",
      "epoch 3438: loss = 0.0698\n",
      "epoch 3439: loss = 0.0427\n",
      "epoch 3440: loss = 0.1630\n",
      "epoch 3441: loss = 0.1362\n",
      "epoch 3442: loss = 0.0726\n",
      "epoch 3443: loss = 0.1032\n",
      "epoch 3444: loss = 0.1052\n",
      "epoch 3445: loss = 0.0408\n",
      "epoch 3446: loss = 0.1655\n",
      "epoch 3447: loss = 0.0544\n",
      "epoch 3448: loss = 0.1566\n",
      "epoch 3449: loss = 0.1647\n",
      "epoch 3450: loss = 0.1106\n",
      "epoch 3451: loss = 0.1436\n",
      "epoch 3452: loss = 0.0866\n",
      "epoch 3453: loss = 0.1023\n",
      "epoch 3454: loss = 0.1169\n",
      "epoch 3455: loss = 0.1060\n",
      "epoch 3456: loss = 0.0864\n",
      "epoch 3457: loss = 0.0973\n",
      "epoch 3458: loss = 0.1213\n",
      "epoch 3459: loss = 0.0538\n",
      "epoch 3460: loss = 0.1148\n",
      "epoch 3461: loss = 0.1255\n",
      "epoch 3462: loss = 0.0898\n",
      "epoch 3463: loss = 0.1252\n",
      "epoch 3464: loss = 0.1360\n",
      "epoch 3465: loss = 0.0817\n",
      "epoch 3466: loss = 0.1145\n",
      "epoch 3467: loss = 0.1826\n",
      "epoch 3468: loss = 0.1035\n",
      "epoch 3469: loss = 0.1405\n",
      "epoch 3470: loss = 0.1307\n",
      "epoch 3471: loss = 0.1220\n",
      "epoch 3472: loss = 0.1856\n",
      "epoch 3473: loss = 0.0543\n",
      "epoch 3474: loss = 0.0750\n",
      "epoch 3475: loss = 0.0632\n",
      "epoch 3476: loss = 0.0474\n",
      "epoch 3477: loss = 0.0968\n",
      "epoch 3478: loss = 0.0999\n",
      "epoch 3479: loss = 0.0399\n",
      "epoch 3480: loss = 0.0897\n",
      "epoch 3481: loss = 0.1427\n",
      "epoch 3482: loss = 0.0695\n",
      "epoch 3483: loss = 0.1515\n",
      "epoch 3484: loss = 0.0444\n",
      "epoch 3485: loss = 0.1106\n",
      "epoch 3486: loss = 0.0864\n",
      "epoch 3487: loss = 0.0252\n",
      "epoch 3488: loss = 0.0491\n",
      "epoch 3489: loss = 0.1014\n",
      "epoch 3490: loss = 0.1901\n",
      "epoch 3491: loss = 0.1891\n",
      "epoch 3492: loss = 0.1231\n",
      "epoch 3493: loss = 0.1074\n",
      "epoch 3494: loss = 0.1821\n",
      "epoch 3495: loss = 0.1165\n",
      "epoch 3496: loss = 0.1787\n",
      "epoch 3497: loss = 0.1151\n",
      "epoch 3498: loss = 0.0996\n",
      "epoch 3499: loss = 0.1150\n",
      "epoch 3500: loss = 0.1506\n",
      "epoch 3501: loss = 0.0841\n",
      "epoch 3502: loss = 0.1203\n",
      "epoch 3503: loss = 0.0316\n",
      "epoch 3504: loss = 0.1056\n",
      "epoch 3505: loss = 0.1032\n",
      "epoch 3506: loss = 0.1476\n",
      "epoch 3507: loss = 0.1651\n",
      "epoch 3508: loss = 0.1610\n",
      "epoch 3509: loss = 0.1038\n",
      "epoch 3510: loss = 0.0824\n",
      "epoch 3511: loss = 0.1058\n",
      "epoch 3512: loss = 0.1796\n",
      "epoch 3513: loss = 0.1081\n",
      "epoch 3514: loss = 0.1028\n",
      "epoch 3515: loss = 0.1447\n",
      "epoch 3516: loss = 0.1769\n",
      "epoch 3517: loss = 0.1716\n",
      "epoch 3518: loss = 0.1375\n",
      "epoch 3519: loss = 0.1401\n",
      "epoch 3520: loss = 0.1165\n",
      "epoch 3521: loss = 0.0855\n",
      "epoch 3522: loss = 0.1216\n",
      "epoch 3523: loss = 0.0662\n",
      "epoch 3524: loss = 0.1032\n",
      "epoch 3525: loss = 0.0919\n",
      "epoch 3526: loss = 0.1754\n",
      "epoch 3527: loss = 0.1593\n",
      "epoch 3528: loss = 0.1478\n",
      "epoch 3529: loss = 0.1503\n",
      "epoch 3530: loss = 0.0843\n",
      "epoch 3531: loss = 0.1879\n",
      "epoch 3532: loss = 0.1604\n",
      "epoch 3533: loss = 0.0939\n",
      "epoch 3534: loss = 0.0468\n",
      "epoch 3535: loss = 0.0968\n",
      "epoch 3536: loss = 0.0639\n",
      "epoch 3537: loss = 0.1401\n",
      "epoch 3538: loss = 0.1525\n",
      "epoch 3539: loss = 0.1137\n",
      "epoch 3540: loss = 0.0668\n",
      "epoch 3541: loss = 0.1286\n",
      "epoch 3542: loss = 0.0767\n",
      "epoch 3543: loss = 0.1083\n",
      "epoch 3544: loss = 0.1234\n",
      "epoch 3545: loss = 0.0923\n",
      "epoch 3546: loss = 0.0698\n",
      "epoch 3547: loss = 0.1095\n",
      "epoch 3548: loss = 0.0917\n",
      "epoch 3549: loss = 0.1182\n",
      "epoch 3550: loss = 0.1421\n",
      "epoch 3551: loss = 0.1313\n",
      "epoch 3552: loss = 0.0772\n",
      "epoch 3553: loss = 0.1171\n",
      "epoch 3554: loss = 0.0726\n",
      "epoch 3555: loss = 0.0431\n",
      "epoch 3556: loss = 0.1030\n",
      "epoch 3557: loss = 0.0277\n",
      "epoch 3558: loss = 0.1388\n",
      "epoch 3559: loss = 0.1166\n",
      "epoch 3560: loss = 0.1488\n",
      "epoch 3561: loss = 0.1649\n",
      "epoch 3562: loss = 0.0845\n",
      "epoch 3563: loss = 0.0651\n",
      "epoch 3564: loss = 0.1359\n",
      "epoch 3565: loss = 0.1238\n",
      "epoch 3566: loss = 0.0992\n",
      "epoch 3567: loss = 0.0604\n",
      "epoch 3568: loss = 0.0662\n",
      "epoch 3569: loss = 0.0670\n",
      "epoch 3570: loss = 0.0969\n",
      "epoch 3571: loss = 0.1677\n",
      "epoch 3572: loss = 0.1340\n",
      "epoch 3573: loss = 0.1069\n",
      "epoch 3574: loss = 0.0332\n",
      "epoch 3575: loss = 0.0533\n",
      "epoch 3576: loss = 0.1194\n",
      "epoch 3577: loss = 0.1136\n",
      "epoch 3578: loss = 0.0371\n",
      "epoch 3579: loss = 0.0612\n",
      "epoch 3580: loss = 0.1228\n",
      "epoch 3581: loss = 0.0862\n",
      "epoch 3582: loss = 0.1299\n",
      "epoch 3583: loss = 0.0778\n",
      "epoch 3584: loss = 0.0243\n",
      "epoch 3585: loss = 0.1248\n",
      "epoch 3586: loss = 0.1230\n",
      "epoch 3587: loss = 0.1435\n",
      "epoch 3588: loss = 0.1175\n",
      "epoch 3589: loss = 0.0956\n",
      "epoch 3590: loss = 0.1212\n",
      "epoch 3591: loss = 0.1737\n",
      "epoch 3592: loss = 0.0687\n",
      "epoch 3593: loss = 0.0606\n",
      "epoch 3594: loss = 0.0839\n",
      "epoch 3595: loss = 0.0782\n",
      "epoch 3596: loss = 0.1156\n",
      "epoch 3597: loss = 0.1275\n",
      "epoch 3598: loss = 0.1345\n",
      "epoch 3599: loss = 0.0942\n",
      "epoch 3600: loss = 0.1026\n",
      "epoch 3601: loss = 0.1940\n",
      "epoch 3602: loss = 0.1314\n",
      "epoch 3603: loss = 0.1554\n",
      "epoch 3604: loss = 0.0861\n",
      "epoch 3605: loss = 0.0862\n",
      "epoch 3606: loss = 0.1538\n",
      "epoch 3607: loss = 0.1352\n",
      "epoch 3608: loss = 0.1170\n",
      "epoch 3609: loss = 0.0211\n",
      "epoch 3610: loss = 0.1519\n",
      "epoch 3611: loss = 0.1142\n",
      "epoch 3612: loss = 0.1249\n",
      "epoch 3613: loss = 0.1278\n",
      "epoch 3614: loss = 0.1627\n",
      "epoch 3615: loss = 0.1283\n",
      "epoch 3616: loss = 0.1180\n",
      "epoch 3617: loss = 0.0601\n",
      "epoch 3618: loss = 0.0951\n",
      "epoch 3619: loss = 0.0882\n",
      "epoch 3620: loss = 0.1703\n",
      "epoch 3621: loss = 0.1236\n",
      "epoch 3622: loss = 0.1370\n",
      "epoch 3623: loss = 0.0751\n",
      "epoch 3624: loss = 0.0640\n",
      "epoch 3625: loss = 0.0882\n",
      "epoch 3626: loss = 0.1465\n",
      "epoch 3627: loss = 0.1229\n",
      "epoch 3628: loss = 0.1410\n",
      "epoch 3629: loss = 0.1294\n",
      "epoch 3630: loss = 0.1357\n",
      "epoch 3631: loss = 0.1718\n",
      "epoch 3632: loss = 0.1544\n",
      "epoch 3633: loss = 0.1357\n",
      "epoch 3634: loss = 0.0952\n",
      "epoch 3635: loss = 0.0746\n",
      "epoch 3636: loss = 0.1056\n",
      "epoch 3637: loss = 0.0802\n",
      "epoch 3638: loss = 0.1611\n",
      "epoch 3639: loss = 0.0824\n",
      "epoch 3640: loss = 0.0828\n",
      "epoch 3641: loss = 0.1419\n",
      "epoch 3642: loss = 0.0863\n",
      "epoch 3643: loss = 0.0820\n",
      "epoch 3644: loss = 0.1072\n",
      "epoch 3645: loss = 0.1005\n",
      "epoch 3646: loss = 0.1147\n",
      "epoch 3647: loss = 0.1350\n",
      "epoch 3648: loss = 0.1120\n",
      "epoch 3649: loss = 0.0783\n",
      "epoch 3650: loss = 0.1265\n",
      "epoch 3651: loss = 0.1576\n",
      "epoch 3652: loss = 0.0644\n",
      "epoch 3653: loss = 0.0909\n",
      "epoch 3654: loss = 0.0838\n",
      "epoch 3655: loss = 0.0989\n",
      "epoch 3656: loss = 0.0917\n",
      "epoch 3657: loss = 0.1463\n",
      "epoch 3658: loss = 0.0741\n",
      "epoch 3659: loss = 0.1671\n",
      "epoch 3660: loss = 0.1426\n",
      "epoch 3661: loss = 0.0710\n",
      "epoch 3662: loss = 0.1605\n",
      "epoch 3663: loss = 0.1550\n",
      "epoch 3664: loss = 0.0849\n",
      "epoch 3665: loss = 0.1230\n",
      "epoch 3666: loss = 0.1617\n",
      "epoch 3667: loss = 0.0962\n",
      "epoch 3668: loss = 0.1495\n",
      "epoch 3669: loss = 0.1623\n",
      "epoch 3670: loss = 0.0389\n",
      "epoch 3671: loss = 0.1000\n",
      "epoch 3672: loss = 0.1092\n",
      "epoch 3673: loss = 0.0929\n",
      "epoch 3674: loss = 0.0578\n",
      "epoch 3675: loss = 0.0840\n",
      "epoch 3676: loss = 0.1986\n",
      "epoch 3677: loss = 0.1526\n",
      "epoch 3678: loss = 0.1112\n",
      "epoch 3679: loss = 0.1147\n",
      "epoch 3680: loss = 0.1171\n",
      "epoch 3681: loss = 0.1879\n",
      "epoch 3682: loss = 0.1543\n",
      "epoch 3683: loss = 0.1338\n",
      "epoch 3684: loss = 0.1741\n",
      "epoch 3685: loss = 0.2001\n",
      "epoch 3686: loss = 0.1475\n",
      "epoch 3687: loss = 0.0991\n",
      "epoch 3688: loss = 0.0598\n",
      "epoch 3689: loss = 0.0487\n",
      "epoch 3690: loss = 0.0622\n",
      "epoch 3691: loss = 0.1570\n",
      "epoch 3692: loss = 0.1429\n",
      "epoch 3693: loss = 0.1317\n",
      "epoch 3694: loss = 0.1008\n",
      "epoch 3695: loss = 0.0622\n",
      "epoch 3696: loss = 0.0660\n",
      "epoch 3697: loss = 0.1234\n",
      "epoch 3698: loss = 0.1078\n",
      "epoch 3699: loss = 0.0512\n",
      "epoch 3700: loss = 0.1318\n",
      "epoch 3701: loss = 0.1136\n",
      "epoch 3702: loss = 0.1657\n",
      "epoch 3703: loss = 0.0503\n",
      "epoch 3704: loss = 0.1381\n",
      "epoch 3705: loss = 0.1329\n",
      "epoch 3706: loss = 0.1392\n",
      "epoch 3707: loss = 0.0828\n",
      "epoch 3708: loss = 0.1128\n",
      "epoch 3709: loss = 0.1066\n",
      "epoch 3710: loss = 0.1583\n",
      "epoch 3711: loss = 0.1910\n",
      "epoch 3712: loss = 0.0809\n",
      "epoch 3713: loss = 0.1138\n",
      "epoch 3714: loss = 0.0890\n",
      "epoch 3715: loss = 0.1106\n",
      "epoch 3716: loss = 0.0727\n",
      "epoch 3717: loss = 0.0474\n",
      "epoch 3718: loss = 0.0537\n",
      "epoch 3719: loss = 0.1631\n",
      "epoch 3720: loss = 0.1558\n",
      "epoch 3721: loss = 0.1087\n",
      "epoch 3722: loss = 0.0921\n",
      "epoch 3723: loss = 0.0274\n",
      "epoch 3724: loss = 0.0432\n",
      "epoch 3725: loss = 0.1426\n",
      "epoch 3726: loss = 0.1468\n",
      "epoch 3727: loss = 0.0983\n",
      "epoch 3728: loss = 0.1624\n",
      "epoch 3729: loss = 0.0896\n",
      "epoch 3730: loss = 0.0582\n",
      "epoch 3731: loss = 0.0794\n",
      "epoch 3732: loss = 0.1222\n",
      "epoch 3733: loss = 0.0996\n",
      "epoch 3734: loss = 0.1004\n",
      "epoch 3735: loss = 0.0533\n",
      "epoch 3736: loss = 0.0981\n",
      "epoch 3737: loss = 0.1566\n",
      "epoch 3738: loss = 0.0647\n",
      "epoch 3739: loss = 0.0925\n",
      "epoch 3740: loss = 0.1187\n",
      "epoch 3741: loss = 0.1413\n",
      "epoch 3742: loss = 0.1334\n",
      "epoch 3743: loss = 0.1424\n",
      "epoch 3744: loss = 0.0449\n",
      "epoch 3745: loss = 0.0567\n",
      "epoch 3746: loss = 0.1462\n",
      "epoch 3747: loss = 0.1346\n",
      "epoch 3748: loss = 0.0384\n",
      "epoch 3749: loss = 0.1509\n",
      "epoch 3750: loss = 0.1110\n",
      "epoch 3751: loss = 0.0494\n",
      "epoch 3752: loss = 0.0856\n",
      "epoch 3753: loss = 0.0853\n",
      "epoch 3754: loss = 0.1649\n",
      "epoch 3755: loss = 0.1954\n",
      "epoch 3756: loss = 0.1611\n",
      "epoch 3757: loss = 0.1019\n",
      "epoch 3758: loss = 0.0621\n",
      "epoch 3759: loss = 0.1016\n",
      "epoch 3760: loss = 0.1160\n",
      "epoch 3761: loss = 0.0769\n",
      "epoch 3762: loss = 0.0875\n",
      "epoch 3763: loss = 0.1269\n",
      "epoch 3764: loss = 0.0914\n",
      "epoch 3765: loss = 0.0841\n",
      "epoch 3766: loss = 0.1016\n",
      "epoch 3767: loss = 0.0722\n",
      "epoch 3768: loss = 0.1578\n",
      "epoch 3769: loss = 0.1188\n",
      "epoch 3770: loss = 0.0982\n",
      "epoch 3771: loss = 0.0700\n",
      "epoch 3772: loss = 0.0230\n",
      "epoch 3773: loss = 0.1426\n",
      "epoch 3774: loss = 0.1623\n",
      "epoch 3775: loss = 0.1132\n",
      "epoch 3776: loss = 0.1610\n",
      "epoch 3777: loss = 0.1615\n",
      "epoch 3778: loss = 0.1296\n",
      "epoch 3779: loss = 0.1404\n",
      "epoch 3780: loss = 0.1581\n",
      "epoch 3781: loss = 0.1107\n",
      "epoch 3782: loss = 0.1271\n",
      "epoch 3783: loss = 0.1166\n",
      "epoch 3784: loss = 0.0821\n",
      "epoch 3785: loss = 0.0558\n",
      "epoch 3786: loss = 0.0769\n",
      "epoch 3787: loss = 0.0828\n",
      "epoch 3788: loss = 0.0690\n",
      "epoch 3789: loss = 0.0590\n",
      "epoch 3790: loss = 0.1505\n",
      "epoch 3791: loss = 0.0897\n",
      "epoch 3792: loss = 0.1536\n",
      "epoch 3793: loss = 0.1794\n",
      "epoch 3794: loss = 0.1287\n",
      "epoch 3795: loss = 0.1198\n",
      "epoch 3796: loss = 0.1359\n",
      "epoch 3797: loss = 0.1549\n",
      "epoch 3798: loss = 0.0985\n",
      "epoch 3799: loss = 0.1330\n",
      "epoch 3800: loss = 0.0133\n",
      "epoch 3801: loss = 0.1724\n",
      "epoch 3802: loss = 0.1169\n",
      "epoch 3803: loss = 0.0323\n",
      "epoch 3804: loss = 0.1758\n",
      "epoch 3805: loss = 0.0644\n",
      "epoch 3806: loss = 0.0677\n",
      "epoch 3807: loss = 0.0947\n",
      "epoch 3808: loss = 0.0813\n",
      "epoch 3809: loss = 0.1624\n",
      "epoch 3810: loss = 0.1533\n",
      "epoch 3811: loss = 0.1681\n",
      "epoch 3812: loss = 0.0586\n",
      "epoch 3813: loss = 0.1214\n",
      "epoch 3814: loss = 0.1641\n",
      "epoch 3815: loss = 0.0575\n",
      "epoch 3816: loss = 0.0656\n",
      "epoch 3817: loss = 0.1251\n",
      "epoch 3818: loss = 0.1652\n",
      "epoch 3819: loss = 0.1635\n",
      "epoch 3820: loss = 0.1530\n",
      "epoch 3821: loss = 0.1742\n",
      "epoch 3822: loss = 0.1678\n",
      "epoch 3823: loss = 0.0956\n",
      "epoch 3824: loss = 0.1726\n",
      "epoch 3825: loss = 0.1582\n",
      "epoch 3826: loss = 0.0562\n",
      "epoch 3827: loss = 0.0865\n",
      "epoch 3828: loss = 0.1313\n",
      "epoch 3829: loss = 0.1424\n",
      "epoch 3830: loss = 0.1199\n",
      "epoch 3831: loss = 0.0736\n",
      "epoch 3832: loss = 0.0665\n",
      "epoch 3833: loss = 0.1692\n",
      "epoch 3834: loss = 0.0885\n",
      "epoch 3835: loss = 0.1774\n",
      "epoch 3836: loss = 0.0526\n",
      "epoch 3837: loss = 0.1501\n",
      "epoch 3838: loss = 0.1376\n",
      "epoch 3839: loss = 0.1416\n",
      "epoch 3840: loss = 0.1323\n",
      "epoch 3841: loss = 0.1343\n",
      "epoch 3842: loss = 0.1298\n",
      "epoch 3843: loss = 0.1015\n",
      "epoch 3844: loss = 0.1349\n",
      "epoch 3845: loss = 0.1264\n",
      "epoch 3846: loss = 0.1340\n",
      "epoch 3847: loss = 0.0751\n",
      "epoch 3848: loss = 0.1698\n",
      "epoch 3849: loss = 0.1368\n",
      "epoch 3850: loss = 0.0988\n",
      "epoch 3851: loss = 0.1392\n",
      "epoch 3852: loss = 0.1120\n",
      "epoch 3853: loss = 0.1459\n",
      "epoch 3854: loss = 0.2077\n",
      "epoch 3855: loss = 0.1406\n",
      "epoch 3856: loss = 0.1132\n",
      "epoch 3857: loss = 0.1441\n",
      "epoch 3858: loss = 0.1424\n",
      "epoch 3859: loss = 0.1291\n",
      "epoch 3860: loss = 0.1395\n",
      "epoch 3861: loss = 0.1109\n",
      "epoch 3862: loss = 0.1588\n",
      "epoch 3863: loss = 0.1173\n",
      "epoch 3864: loss = 0.1435\n",
      "epoch 3865: loss = 0.1498\n",
      "epoch 3866: loss = 0.1280\n",
      "epoch 3867: loss = 0.1006\n",
      "epoch 3868: loss = 0.0832\n",
      "epoch 3869: loss = 0.1521\n",
      "epoch 3870: loss = 0.1029\n",
      "epoch 3871: loss = 0.1086\n",
      "epoch 3872: loss = 0.0838\n",
      "epoch 3873: loss = 0.0432\n",
      "epoch 3874: loss = 0.1378\n",
      "epoch 3875: loss = 0.1094\n",
      "epoch 3876: loss = 0.2014\n",
      "epoch 3877: loss = 0.0710\n",
      "epoch 3878: loss = 0.1931\n",
      "epoch 3879: loss = 0.0879\n",
      "epoch 3880: loss = 0.1038\n",
      "epoch 3881: loss = 0.1113\n",
      "epoch 3882: loss = 0.0839\n",
      "epoch 3883: loss = 0.1505\n",
      "epoch 3884: loss = 0.0534\n",
      "epoch 3885: loss = 0.1530\n",
      "epoch 3886: loss = 0.0902\n",
      "epoch 3887: loss = 0.1078\n",
      "epoch 3888: loss = 0.1328\n",
      "epoch 3889: loss = 0.0937\n",
      "epoch 3890: loss = 0.0518\n",
      "epoch 3891: loss = 0.1485\n",
      "epoch 3892: loss = 0.0661\n",
      "epoch 3893: loss = 0.0434\n",
      "epoch 3894: loss = 0.1114\n",
      "epoch 3895: loss = 0.0736\n",
      "epoch 3896: loss = 0.0374\n",
      "epoch 3897: loss = 0.1434\n",
      "epoch 3898: loss = 0.1241\n",
      "epoch 3899: loss = 0.1234\n",
      "epoch 3900: loss = 0.1231\n",
      "epoch 3901: loss = 0.1194\n",
      "epoch 3902: loss = 0.0686\n",
      "epoch 3903: loss = 0.0900\n",
      "epoch 3904: loss = 0.1922\n",
      "epoch 3905: loss = 0.1667\n",
      "epoch 3906: loss = 0.1273\n",
      "epoch 3907: loss = 0.0334\n",
      "epoch 3908: loss = 0.1573\n",
      "epoch 3909: loss = 0.1009\n",
      "epoch 3910: loss = 0.1246\n",
      "epoch 3911: loss = 0.0964\n",
      "epoch 3912: loss = 0.0709\n",
      "epoch 3913: loss = 0.1589\n",
      "epoch 3914: loss = 0.0505\n",
      "epoch 3915: loss = 0.1155\n",
      "epoch 3916: loss = 0.1886\n",
      "epoch 3917: loss = 0.1758\n",
      "epoch 3918: loss = 0.1126\n",
      "epoch 3919: loss = 0.1054\n",
      "epoch 3920: loss = 0.1489\n",
      "epoch 3921: loss = 0.0678\n",
      "epoch 3922: loss = 0.0608\n",
      "epoch 3923: loss = 0.1031\n",
      "epoch 3924: loss = 0.1513\n",
      "epoch 3925: loss = 0.1141\n",
      "epoch 3926: loss = 0.1626\n",
      "epoch 3927: loss = 0.1558\n",
      "epoch 3928: loss = 0.1195\n",
      "epoch 3929: loss = 0.0330\n",
      "epoch 3930: loss = 0.0578\n",
      "epoch 3931: loss = 0.1181\n",
      "epoch 3932: loss = 0.1590\n",
      "epoch 3933: loss = 0.1614\n",
      "epoch 3934: loss = 0.1587\n",
      "epoch 3935: loss = 0.1350\n",
      "epoch 3936: loss = 0.1768\n",
      "epoch 3937: loss = 0.1828\n",
      "epoch 3938: loss = 0.1894\n",
      "epoch 3939: loss = 0.1513\n",
      "epoch 3940: loss = 0.1300\n",
      "epoch 3941: loss = 0.0619\n",
      "epoch 3942: loss = 0.1388\n",
      "epoch 3943: loss = 0.0262\n",
      "epoch 3944: loss = 0.1932\n",
      "epoch 3945: loss = 0.1083\n",
      "epoch 3946: loss = 0.1156\n",
      "epoch 3947: loss = 0.0880\n",
      "epoch 3948: loss = 0.0956\n",
      "epoch 3949: loss = 0.1413\n",
      "epoch 3950: loss = 0.1715\n",
      "epoch 3951: loss = 0.0439\n",
      "epoch 3952: loss = 0.1496\n",
      "epoch 3953: loss = 0.1705\n",
      "epoch 3954: loss = 0.1540\n",
      "epoch 3955: loss = 0.0648\n",
      "epoch 3956: loss = 0.1043\n",
      "epoch 3957: loss = 0.1871\n",
      "epoch 3958: loss = 0.1367\n",
      "epoch 3959: loss = 0.1337\n",
      "epoch 3960: loss = 0.1462\n",
      "epoch 3961: loss = 0.1441\n",
      "epoch 3962: loss = 0.0401\n",
      "epoch 3963: loss = 0.0912\n",
      "epoch 3964: loss = 0.0856\n",
      "epoch 3965: loss = 0.1277\n",
      "epoch 3966: loss = 0.0751\n",
      "epoch 3967: loss = 0.0944\n",
      "epoch 3968: loss = 0.0417\n",
      "epoch 3969: loss = 0.0906\n",
      "epoch 3970: loss = 0.1469\n",
      "epoch 3971: loss = 0.1085\n",
      "epoch 3972: loss = 0.1643\n",
      "epoch 3973: loss = 0.1249\n",
      "epoch 3974: loss = 0.1622\n",
      "epoch 3975: loss = 0.1249\n",
      "epoch 3976: loss = 0.0084\n",
      "epoch 3977: loss = 0.1486\n",
      "epoch 3978: loss = 0.1272\n",
      "epoch 3979: loss = 0.1254\n",
      "epoch 3980: loss = 0.1078\n",
      "epoch 3981: loss = 0.0686\n",
      "epoch 3982: loss = 0.1775\n",
      "epoch 3983: loss = 0.1056\n",
      "epoch 3984: loss = 0.0749\n",
      "epoch 3985: loss = 0.1757\n",
      "epoch 3986: loss = 0.1549\n",
      "epoch 3987: loss = 0.0533\n",
      "epoch 3988: loss = 0.1753\n",
      "epoch 3989: loss = 0.0933\n",
      "epoch 3990: loss = 0.1108\n",
      "epoch 3991: loss = 0.0608\n",
      "epoch 3992: loss = 0.1099\n",
      "epoch 3993: loss = 0.1167\n",
      "epoch 3994: loss = 0.0610\n",
      "epoch 3995: loss = 0.0944\n",
      "epoch 3996: loss = 0.1268\n",
      "epoch 3997: loss = 0.0974\n",
      "epoch 3998: loss = 0.0543\n",
      "epoch 3999: loss = 0.1385\n",
      "epoch 4000: loss = 0.0559\n",
      "epoch 4001: loss = 0.0818\n",
      "epoch 4002: loss = 0.1639\n",
      "epoch 4003: loss = 0.1005\n",
      "epoch 4004: loss = 0.1186\n",
      "epoch 4005: loss = 0.0657\n",
      "epoch 4006: loss = 0.1649\n",
      "epoch 4007: loss = 0.1177\n",
      "epoch 4008: loss = 0.1394\n",
      "epoch 4009: loss = 0.1248\n",
      "epoch 4010: loss = 0.1305\n",
      "epoch 4011: loss = 0.1142\n",
      "epoch 4012: loss = 0.1995\n",
      "epoch 4013: loss = 0.1054\n",
      "epoch 4014: loss = 0.0934\n",
      "epoch 4015: loss = 0.1155\n",
      "epoch 4016: loss = 0.1231\n",
      "epoch 4017: loss = 0.0603\n",
      "epoch 4018: loss = 0.1472\n",
      "epoch 4019: loss = 0.1013\n",
      "epoch 4020: loss = 0.0851\n",
      "epoch 4021: loss = 0.0848\n",
      "epoch 4022: loss = 0.1726\n",
      "epoch 4023: loss = 0.0994\n",
      "epoch 4024: loss = 0.1249\n",
      "epoch 4025: loss = 0.1301\n",
      "epoch 4026: loss = 0.1145\n",
      "epoch 4027: loss = 0.1548\n",
      "epoch 4028: loss = 0.0950\n",
      "epoch 4029: loss = 0.1288\n",
      "epoch 4030: loss = 0.1111\n",
      "epoch 4031: loss = 0.1010\n",
      "epoch 4032: loss = 0.0717\n",
      "epoch 4033: loss = 0.1746\n",
      "epoch 4034: loss = 0.1611\n",
      "epoch 4035: loss = 0.0745\n",
      "epoch 4036: loss = 0.0994\n",
      "epoch 4037: loss = 0.1254\n",
      "epoch 4038: loss = 0.1428\n",
      "epoch 4039: loss = 0.1159\n",
      "epoch 4040: loss = 0.1234\n",
      "epoch 4041: loss = 0.1000\n",
      "epoch 4042: loss = 0.0441\n",
      "epoch 4043: loss = 0.0853\n",
      "epoch 4044: loss = 0.1509\n",
      "epoch 4045: loss = 0.1375\n",
      "epoch 4046: loss = 0.1120\n",
      "epoch 4047: loss = 0.1628\n",
      "epoch 4048: loss = 0.1078\n",
      "epoch 4049: loss = 0.0978\n",
      "epoch 4050: loss = 0.1176\n",
      "epoch 4051: loss = 0.1683\n",
      "epoch 4052: loss = 0.1800\n",
      "epoch 4053: loss = 0.1832\n",
      "epoch 4054: loss = 0.1411\n",
      "epoch 4055: loss = 0.0612\n",
      "epoch 4056: loss = 0.1266\n",
      "epoch 4057: loss = 0.0954\n",
      "epoch 4058: loss = 0.1435\n",
      "epoch 4059: loss = 0.1294\n",
      "epoch 4060: loss = 0.0808\n",
      "epoch 4061: loss = 0.1779\n",
      "epoch 4062: loss = 0.1600\n",
      "epoch 4063: loss = 0.0649\n",
      "epoch 4064: loss = 0.0433\n",
      "epoch 4065: loss = 0.0226\n",
      "epoch 4066: loss = 0.1394\n",
      "epoch 4067: loss = 0.1193\n",
      "epoch 4068: loss = 0.0794\n",
      "epoch 4069: loss = 0.1636\n",
      "epoch 4070: loss = 0.1059\n",
      "epoch 4071: loss = 0.1341\n",
      "epoch 4072: loss = 0.1056\n",
      "epoch 4073: loss = 0.0996\n",
      "epoch 4074: loss = 0.1653\n",
      "epoch 4075: loss = 0.0719\n",
      "epoch 4076: loss = 0.0944\n",
      "epoch 4077: loss = 0.0602\n",
      "epoch 4078: loss = 0.0908\n",
      "epoch 4079: loss = 0.1223\n",
      "epoch 4080: loss = 0.0669\n",
      "epoch 4081: loss = 0.1790\n",
      "epoch 4082: loss = 0.1371\n",
      "epoch 4083: loss = 0.0595\n",
      "epoch 4084: loss = 0.1078\n",
      "epoch 4085: loss = 0.0944\n",
      "epoch 4086: loss = 0.0822\n",
      "epoch 4087: loss = 0.1255\n",
      "epoch 4088: loss = 0.0641\n",
      "epoch 4089: loss = 0.0870\n",
      "epoch 4090: loss = 0.1147\n",
      "epoch 4091: loss = 0.1314\n",
      "epoch 4092: loss = 0.1821\n",
      "epoch 4093: loss = 0.0481\n",
      "epoch 4094: loss = 0.1160\n",
      "epoch 4095: loss = 0.1360\n",
      "epoch 4096: loss = 0.1391\n",
      "epoch 4097: loss = 0.1159\n",
      "epoch 4098: loss = 0.0986\n",
      "epoch 4099: loss = 0.1483\n",
      "epoch 4100: loss = 0.1957\n",
      "epoch 4101: loss = 0.1194\n",
      "epoch 4102: loss = 0.1250\n",
      "epoch 4103: loss = 0.1549\n",
      "epoch 4104: loss = 0.1232\n",
      "epoch 4105: loss = 0.0532\n",
      "epoch 4106: loss = 0.1340\n",
      "epoch 4107: loss = 0.0924\n",
      "epoch 4108: loss = 0.1078\n",
      "epoch 4109: loss = 0.0790\n",
      "epoch 4110: loss = 0.0686\n",
      "epoch 4111: loss = 0.0945\n",
      "epoch 4112: loss = 0.1427\n",
      "epoch 4113: loss = 0.0866\n",
      "epoch 4114: loss = 0.1079\n",
      "epoch 4115: loss = 0.1256\n",
      "epoch 4116: loss = 0.0919\n",
      "epoch 4117: loss = 0.0411\n",
      "epoch 4118: loss = 0.1853\n",
      "epoch 4119: loss = 0.0837\n",
      "epoch 4120: loss = 0.1540\n",
      "epoch 4121: loss = 0.1312\n",
      "epoch 4122: loss = 0.1251\n",
      "epoch 4123: loss = 0.0986\n",
      "epoch 4124: loss = 0.0818\n",
      "epoch 4125: loss = 0.0513\n",
      "epoch 4126: loss = 0.1534\n",
      "epoch 4127: loss = 0.1506\n",
      "epoch 4128: loss = 0.0807\n",
      "epoch 4129: loss = 0.1023\n",
      "epoch 4130: loss = 0.1797\n",
      "epoch 4131: loss = 0.1486\n",
      "epoch 4132: loss = 0.1805\n",
      "epoch 4133: loss = 0.1941\n",
      "epoch 4134: loss = 0.0621\n",
      "epoch 4135: loss = 0.0988\n",
      "epoch 4136: loss = 0.0131\n",
      "epoch 4137: loss = 0.0909\n",
      "epoch 4138: loss = 0.1576\n",
      "epoch 4139: loss = 0.1125\n",
      "epoch 4140: loss = 0.1049\n",
      "epoch 4141: loss = 0.0747\n",
      "epoch 4142: loss = 0.1307\n",
      "epoch 4143: loss = 0.0979\n",
      "epoch 4144: loss = 0.0723\n",
      "epoch 4145: loss = 0.1075\n",
      "epoch 4146: loss = 0.0938\n",
      "epoch 4147: loss = 0.0878\n",
      "epoch 4148: loss = 0.1435\n",
      "epoch 4149: loss = 0.1538\n",
      "epoch 4150: loss = 0.0377\n",
      "epoch 4151: loss = 0.1006\n",
      "epoch 4152: loss = 0.1817\n",
      "epoch 4153: loss = 0.1957\n",
      "epoch 4154: loss = 0.1200\n",
      "epoch 4155: loss = 0.1445\n",
      "epoch 4156: loss = 0.1408\n",
      "epoch 4157: loss = 0.0910\n",
      "epoch 4158: loss = 0.1472\n",
      "epoch 4159: loss = 0.1055\n",
      "epoch 4160: loss = 0.1033\n",
      "epoch 4161: loss = 0.1532\n",
      "epoch 4162: loss = 0.1321\n",
      "epoch 4163: loss = 0.0616\n",
      "epoch 4164: loss = 0.0704\n",
      "epoch 4165: loss = 0.0727\n",
      "epoch 4166: loss = 0.1632\n",
      "epoch 4167: loss = 0.0831\n",
      "epoch 4168: loss = 0.0970\n",
      "epoch 4169: loss = 0.0953\n",
      "epoch 4170: loss = 0.1222\n",
      "epoch 4171: loss = 0.1472\n",
      "epoch 4172: loss = 0.1263\n",
      "epoch 4173: loss = 0.0858\n",
      "epoch 4174: loss = 0.1963\n",
      "epoch 4175: loss = 0.1491\n",
      "epoch 4176: loss = 0.0355\n",
      "epoch 4177: loss = 0.1435\n",
      "epoch 4178: loss = 0.1603\n",
      "epoch 4179: loss = 0.1057\n",
      "epoch 4180: loss = 0.1122\n",
      "epoch 4181: loss = 0.1517\n",
      "epoch 4182: loss = 0.1589\n",
      "epoch 4183: loss = 0.1239\n",
      "epoch 4184: loss = 0.0567\n",
      "epoch 4185: loss = 0.1880\n",
      "epoch 4186: loss = 0.0974\n",
      "epoch 4187: loss = 0.1010\n",
      "epoch 4188: loss = 0.1491\n",
      "epoch 4189: loss = 0.1075\n",
      "epoch 4190: loss = 0.1015\n",
      "epoch 4191: loss = 0.1464\n",
      "epoch 4192: loss = 0.1883\n",
      "epoch 4193: loss = 0.1128\n",
      "epoch 4194: loss = 0.0997\n",
      "epoch 4195: loss = 0.1091\n",
      "epoch 4196: loss = 0.1488\n",
      "epoch 4197: loss = 0.1202\n",
      "epoch 4198: loss = 0.0267\n",
      "epoch 4199: loss = 0.1599\n",
      "epoch 4200: loss = 0.1879\n",
      "epoch 4201: loss = 0.1236\n",
      "epoch 4202: loss = 0.1605\n",
      "epoch 4203: loss = 0.0579\n",
      "epoch 4204: loss = 0.1161\n",
      "epoch 4205: loss = 0.1076\n",
      "epoch 4206: loss = 0.0578\n",
      "epoch 4207: loss = 0.1017\n",
      "epoch 4208: loss = 0.1141\n",
      "epoch 4209: loss = 0.0451\n",
      "epoch 4210: loss = 0.0803\n",
      "epoch 4211: loss = 0.0740\n",
      "epoch 4212: loss = 0.1598\n",
      "epoch 4213: loss = 0.1092\n",
      "epoch 4214: loss = 0.1343\n",
      "epoch 4215: loss = 0.1719\n",
      "epoch 4216: loss = 0.1183\n",
      "epoch 4217: loss = 0.1610\n",
      "epoch 4218: loss = 0.1833\n",
      "epoch 4219: loss = 0.1659\n",
      "epoch 4220: loss = 0.0670\n",
      "epoch 4221: loss = 0.0765\n",
      "epoch 4222: loss = 0.0686\n",
      "epoch 4223: loss = 0.1080\n",
      "epoch 4224: loss = 0.0282\n",
      "epoch 4225: loss = 0.0409\n",
      "epoch 4226: loss = 0.0876\n",
      "epoch 4227: loss = 0.0712\n",
      "epoch 4228: loss = 0.1339\n",
      "epoch 4229: loss = 0.1296\n",
      "epoch 4230: loss = 0.1298\n",
      "epoch 4231: loss = 0.1629\n",
      "epoch 4232: loss = 0.1490\n",
      "epoch 4233: loss = 0.1334\n",
      "epoch 4234: loss = 0.0806\n",
      "epoch 4235: loss = 0.1267\n",
      "epoch 4236: loss = 0.1049\n",
      "epoch 4237: loss = 0.0422\n",
      "epoch 4238: loss = 0.0624\n",
      "epoch 4239: loss = 0.0963\n",
      "epoch 4240: loss = 0.1885\n",
      "epoch 4241: loss = 0.1023\n",
      "epoch 4242: loss = 0.0455\n",
      "epoch 4243: loss = 0.1457\n",
      "epoch 4244: loss = 0.0964\n",
      "epoch 4245: loss = 0.1091\n",
      "epoch 4246: loss = 0.0893\n",
      "epoch 4247: loss = 0.0699\n",
      "epoch 4248: loss = 0.1483\n",
      "epoch 4249: loss = 0.1183\n",
      "epoch 4250: loss = 0.1569\n",
      "epoch 4251: loss = 0.1378\n",
      "epoch 4252: loss = 0.1103\n",
      "epoch 4253: loss = 0.0903\n",
      "epoch 4254: loss = 0.0924\n",
      "epoch 4255: loss = 0.0962\n",
      "epoch 4256: loss = 0.0612\n",
      "epoch 4257: loss = 0.1244\n",
      "epoch 4258: loss = 0.1273\n",
      "epoch 4259: loss = 0.1393\n",
      "epoch 4260: loss = 0.0627\n",
      "epoch 4261: loss = 0.1368\n",
      "epoch 4262: loss = 0.0963\n",
      "epoch 4263: loss = 0.1418\n",
      "epoch 4264: loss = 0.0043\n",
      "epoch 4265: loss = 0.1405\n",
      "epoch 4266: loss = 0.1456\n",
      "epoch 4267: loss = 0.1333\n",
      "epoch 4268: loss = 0.0984\n",
      "epoch 4269: loss = 0.1317\n",
      "epoch 4270: loss = 0.1087\n",
      "epoch 4271: loss = 0.1171\n",
      "epoch 4272: loss = 0.1409\n",
      "epoch 4273: loss = 0.1621\n",
      "epoch 4274: loss = 0.0466\n",
      "epoch 4275: loss = 0.1638\n",
      "epoch 4276: loss = 0.1187\n",
      "epoch 4277: loss = 0.0495\n",
      "epoch 4278: loss = 0.1039\n",
      "epoch 4279: loss = 0.0902\n",
      "epoch 4280: loss = 0.0884\n",
      "epoch 4281: loss = 0.1395\n",
      "epoch 4282: loss = 0.1425\n",
      "epoch 4283: loss = 0.1306\n",
      "epoch 4284: loss = 0.1294\n",
      "epoch 4285: loss = 0.0634\n",
      "epoch 4286: loss = 0.1000\n",
      "epoch 4287: loss = 0.1497\n",
      "epoch 4288: loss = 0.1206\n",
      "epoch 4289: loss = 0.0425\n",
      "epoch 4290: loss = 0.0238\n",
      "epoch 4291: loss = 0.1345\n",
      "epoch 4292: loss = 0.1383\n",
      "epoch 4293: loss = 0.0776\n",
      "epoch 4294: loss = 0.1173\n",
      "epoch 4295: loss = 0.1379\n",
      "epoch 4296: loss = 0.1262\n",
      "epoch 4297: loss = 0.1144\n",
      "epoch 4298: loss = 0.0775\n",
      "epoch 4299: loss = 0.0677\n",
      "epoch 4300: loss = 0.1150\n",
      "epoch 4301: loss = 0.1222\n",
      "epoch 4302: loss = 0.0370\n",
      "epoch 4303: loss = 0.1352\n",
      "epoch 4304: loss = 0.0922\n",
      "epoch 4305: loss = 0.1320\n",
      "epoch 4306: loss = 0.0297\n",
      "epoch 4307: loss = 0.1280\n",
      "epoch 4308: loss = 0.1577\n",
      "epoch 4309: loss = 0.1382\n",
      "epoch 4310: loss = 0.0649\n",
      "epoch 4311: loss = 0.1334\n",
      "epoch 4312: loss = 0.1671\n",
      "epoch 4313: loss = 0.1226\n",
      "epoch 4314: loss = 0.0880\n",
      "epoch 4315: loss = 0.1530\n",
      "epoch 4316: loss = 0.1566\n",
      "epoch 4317: loss = 0.0441\n",
      "epoch 4318: loss = 0.1640\n",
      "epoch 4319: loss = 0.1275\n",
      "epoch 4320: loss = 0.1520\n",
      "epoch 4321: loss = 0.1655\n",
      "epoch 4322: loss = 0.0803\n",
      "epoch 4323: loss = 0.1764\n",
      "epoch 4324: loss = 0.1408\n",
      "epoch 4325: loss = 0.1247\n",
      "epoch 4326: loss = 0.0920\n",
      "epoch 4327: loss = 0.0539\n",
      "epoch 4328: loss = 0.1479\n",
      "epoch 4329: loss = 0.0637\n",
      "epoch 4330: loss = 0.0663\n",
      "epoch 4331: loss = 0.1292\n",
      "epoch 4332: loss = 0.1443\n",
      "epoch 4333: loss = 0.1032\n",
      "epoch 4334: loss = 0.1538\n",
      "epoch 4335: loss = 0.1489\n",
      "epoch 4336: loss = 0.0571\n",
      "epoch 4337: loss = 0.0737\n",
      "epoch 4338: loss = 0.1057\n",
      "epoch 4339: loss = 0.1173\n",
      "epoch 4340: loss = 0.1297\n",
      "epoch 4341: loss = 0.0904\n",
      "epoch 4342: loss = 0.0709\n",
      "epoch 4343: loss = 0.0933\n",
      "epoch 4344: loss = 0.1304\n",
      "epoch 4345: loss = 0.1181\n",
      "epoch 4346: loss = 0.1570\n",
      "epoch 4347: loss = 0.1284\n",
      "epoch 4348: loss = 0.1113\n",
      "epoch 4349: loss = 0.1191\n",
      "epoch 4350: loss = 0.0422\n",
      "epoch 4351: loss = 0.1781\n",
      "epoch 4352: loss = 0.0896\n",
      "epoch 4353: loss = 0.1267\n",
      "epoch 4354: loss = 0.1602\n",
      "epoch 4355: loss = 0.1710\n",
      "epoch 4356: loss = 0.1053\n",
      "epoch 4357: loss = 0.1516\n",
      "epoch 4358: loss = 0.1138\n",
      "epoch 4359: loss = 0.1704\n",
      "epoch 4360: loss = 0.1239\n",
      "epoch 4361: loss = 0.1139\n",
      "epoch 4362: loss = 0.1089\n",
      "epoch 4363: loss = 0.1825\n",
      "epoch 4364: loss = 0.1295\n",
      "epoch 4365: loss = 0.1665\n",
      "epoch 4366: loss = 0.1219\n",
      "epoch 4367: loss = 0.1195\n",
      "epoch 4368: loss = 0.1736\n",
      "epoch 4369: loss = 0.1031\n",
      "epoch 4370: loss = 0.1903\n",
      "epoch 4371: loss = 0.1051\n",
      "epoch 4372: loss = 0.0660\n",
      "epoch 4373: loss = 0.0936\n",
      "epoch 4374: loss = 0.0950\n",
      "epoch 4375: loss = 0.1811\n",
      "epoch 4376: loss = 0.1843\n",
      "epoch 4377: loss = 0.1676\n",
      "epoch 4378: loss = 0.1405\n",
      "epoch 4379: loss = 0.1242\n",
      "epoch 4380: loss = 0.1427\n",
      "epoch 4381: loss = 0.1006\n",
      "epoch 4382: loss = 0.0988\n",
      "epoch 4383: loss = 0.1772\n",
      "epoch 4384: loss = 0.1922\n",
      "epoch 4385: loss = 0.0960\n",
      "epoch 4386: loss = 0.0593\n",
      "epoch 4387: loss = 0.0691\n",
      "epoch 4388: loss = 0.0527\n",
      "epoch 4389: loss = 0.1398\n",
      "epoch 4390: loss = 0.1487\n",
      "epoch 4391: loss = 0.1378\n",
      "epoch 4392: loss = 0.0982\n",
      "epoch 4393: loss = 0.1456\n",
      "epoch 4394: loss = 0.1825\n",
      "epoch 4395: loss = 0.1420\n",
      "epoch 4396: loss = 0.0728\n",
      "epoch 4397: loss = 0.0784\n",
      "epoch 4398: loss = 0.0606\n",
      "epoch 4399: loss = 0.1900\n",
      "epoch 4400: loss = 0.0873\n",
      "epoch 4401: loss = 0.1640\n",
      "epoch 4402: loss = 0.0921\n",
      "epoch 4403: loss = 0.0948\n",
      "epoch 4404: loss = 0.0607\n",
      "epoch 4405: loss = 0.1107\n",
      "epoch 4406: loss = 0.0770\n",
      "epoch 4407: loss = 0.1548\n",
      "epoch 4408: loss = 0.1596\n",
      "epoch 4409: loss = 0.1260\n",
      "epoch 4410: loss = 0.1477\n",
      "epoch 4411: loss = 0.1441\n",
      "epoch 4412: loss = 0.1124\n",
      "epoch 4413: loss = 0.1193\n",
      "epoch 4414: loss = 0.0954\n",
      "epoch 4415: loss = 0.1657\n",
      "epoch 4416: loss = 0.0506\n",
      "epoch 4417: loss = 0.1128\n",
      "epoch 4418: loss = 0.1647\n",
      "epoch 4419: loss = 0.1407\n",
      "epoch 4420: loss = 0.0065\n",
      "epoch 4421: loss = 0.0947\n",
      "epoch 4422: loss = 0.1228\n",
      "epoch 4423: loss = 0.0529\n",
      "epoch 4424: loss = 0.0734\n",
      "epoch 4425: loss = 0.1003\n",
      "epoch 4426: loss = 0.0812\n",
      "epoch 4427: loss = 0.0876\n",
      "epoch 4428: loss = 0.1033\n",
      "epoch 4429: loss = 0.0761\n",
      "epoch 4430: loss = 0.1782\n",
      "epoch 4431: loss = 0.1261\n",
      "epoch 4432: loss = 0.1053\n",
      "epoch 4433: loss = 0.0951\n",
      "epoch 4434: loss = 0.1349\n",
      "epoch 4435: loss = 0.0481\n",
      "epoch 4436: loss = 0.1338\n",
      "epoch 4437: loss = 0.1776\n",
      "epoch 4438: loss = 0.0396\n",
      "epoch 4439: loss = 0.1311\n",
      "epoch 4440: loss = 0.1395\n",
      "epoch 4441: loss = 0.1111\n",
      "epoch 4442: loss = 0.0773\n",
      "epoch 4443: loss = 0.0405\n",
      "epoch 4444: loss = 0.0775\n",
      "epoch 4445: loss = 0.1380\n",
      "epoch 4446: loss = 0.1109\n",
      "epoch 4447: loss = 0.1698\n",
      "epoch 4448: loss = 0.1347\n",
      "epoch 4449: loss = 0.1571\n",
      "epoch 4450: loss = 0.0527\n",
      "epoch 4451: loss = 0.0963\n",
      "epoch 4452: loss = 0.1090\n",
      "epoch 4453: loss = 0.0825\n",
      "epoch 4454: loss = 0.0827\n",
      "epoch 4455: loss = 0.1285\n",
      "epoch 4456: loss = 0.1250\n",
      "epoch 4457: loss = 0.1053\n",
      "epoch 4458: loss = 0.0840\n",
      "epoch 4459: loss = 0.1500\n",
      "epoch 4460: loss = 0.0853\n",
      "epoch 4461: loss = 0.1418\n",
      "epoch 4462: loss = 0.1184\n",
      "epoch 4463: loss = 0.0905\n",
      "epoch 4464: loss = 0.1463\n",
      "epoch 4465: loss = 0.0874\n",
      "epoch 4466: loss = 0.0972\n",
      "epoch 4467: loss = 0.0999\n",
      "epoch 4468: loss = 0.0908\n",
      "epoch 4469: loss = 0.0052\n",
      "epoch 4470: loss = 0.0754\n",
      "epoch 4471: loss = 0.0736\n",
      "epoch 4472: loss = 0.0725\n",
      "epoch 4473: loss = 0.0978\n",
      "epoch 4474: loss = 0.1381\n",
      "epoch 4475: loss = 0.1202\n",
      "epoch 4476: loss = 0.0500\n",
      "epoch 4477: loss = 0.0531\n",
      "epoch 4478: loss = 0.1276\n",
      "epoch 4479: loss = 0.0728\n",
      "epoch 4480: loss = 0.1347\n",
      "epoch 4481: loss = 0.0470\n",
      "epoch 4482: loss = 0.1346\n",
      "epoch 4483: loss = 0.0846\n",
      "epoch 4484: loss = 0.0601\n",
      "epoch 4485: loss = 0.1375\n",
      "epoch 4486: loss = 0.1144\n",
      "epoch 4487: loss = 0.1213\n",
      "epoch 4488: loss = 0.0567\n",
      "epoch 4489: loss = 0.1332\n",
      "epoch 4490: loss = 0.1009\n",
      "epoch 4491: loss = 0.0970\n",
      "epoch 4492: loss = 0.0839\n",
      "epoch 4493: loss = 0.1960\n",
      "epoch 4494: loss = 0.1524\n",
      "epoch 4495: loss = 0.0788\n",
      "epoch 4496: loss = 0.1189\n",
      "epoch 4497: loss = 0.0860\n",
      "epoch 4498: loss = 0.0800\n",
      "epoch 4499: loss = 0.0587\n",
      "epoch 4500: loss = 0.0869\n",
      "epoch 4501: loss = 0.1245\n",
      "epoch 4502: loss = 0.1622\n",
      "epoch 4503: loss = 0.1236\n",
      "epoch 4504: loss = 0.1328\n",
      "epoch 4505: loss = 0.0834\n",
      "epoch 4506: loss = 0.1314\n",
      "epoch 4507: loss = 0.1073\n",
      "epoch 4508: loss = 0.0882\n",
      "epoch 4509: loss = 0.1039\n",
      "epoch 4510: loss = 0.0358\n",
      "epoch 4511: loss = 0.1010\n",
      "epoch 4512: loss = 0.0753\n",
      "epoch 4513: loss = 0.1132\n",
      "epoch 4514: loss = 0.1182\n",
      "epoch 4515: loss = 0.1819\n",
      "epoch 4516: loss = 0.1366\n",
      "epoch 4517: loss = 0.1041\n",
      "epoch 4518: loss = 0.1067\n",
      "epoch 4519: loss = 0.1469\n",
      "epoch 4520: loss = 0.1940\n",
      "epoch 4521: loss = 0.1970\n",
      "epoch 4522: loss = 0.2096\n",
      "epoch 4523: loss = 0.1546\n",
      "epoch 4524: loss = 0.1036\n",
      "epoch 4525: loss = 0.1557\n",
      "epoch 4526: loss = 0.1550\n",
      "epoch 4527: loss = 0.1321\n",
      "epoch 4528: loss = 0.1595\n",
      "epoch 4529: loss = 0.1314\n",
      "epoch 4530: loss = 0.1023\n",
      "epoch 4531: loss = 0.0880\n",
      "epoch 4532: loss = 0.0798\n",
      "epoch 4533: loss = 0.1110\n",
      "epoch 4534: loss = 0.1820\n",
      "epoch 4535: loss = 0.0781\n",
      "epoch 4536: loss = 0.0513\n",
      "epoch 4537: loss = 0.1536\n",
      "epoch 4538: loss = 0.1369\n",
      "epoch 4539: loss = 0.0875\n",
      "epoch 4540: loss = 0.0661\n",
      "epoch 4541: loss = 0.1682\n",
      "epoch 4542: loss = 0.1292\n",
      "epoch 4543: loss = 0.1369\n",
      "epoch 4544: loss = 0.0956\n",
      "epoch 4545: loss = 0.1425\n",
      "epoch 4546: loss = 0.1075\n",
      "epoch 4547: loss = 0.1283\n",
      "epoch 4548: loss = 0.0716\n",
      "epoch 4549: loss = 0.0594\n",
      "epoch 4550: loss = 0.1294\n",
      "epoch 4551: loss = 0.1114\n",
      "epoch 4552: loss = 0.1503\n",
      "epoch 4553: loss = 0.0923\n",
      "epoch 4554: loss = 0.1046\n",
      "epoch 4555: loss = 0.0541\n",
      "epoch 4556: loss = 0.1365\n",
      "epoch 4557: loss = 0.1755\n",
      "epoch 4558: loss = 0.1176\n",
      "epoch 4559: loss = 0.1655\n",
      "epoch 4560: loss = 0.1947\n",
      "epoch 4561: loss = 0.1467\n",
      "epoch 4562: loss = 0.1545\n",
      "epoch 4563: loss = 0.1187\n",
      "epoch 4564: loss = 0.1907\n",
      "epoch 4565: loss = 0.1931\n",
      "epoch 4566: loss = 0.1717\n",
      "epoch 4567: loss = 0.1195\n",
      "epoch 4568: loss = 0.1381\n",
      "epoch 4569: loss = 0.1747\n",
      "epoch 4570: loss = 0.0516\n",
      "epoch 4571: loss = 0.0302\n",
      "epoch 4572: loss = 0.1342\n",
      "epoch 4573: loss = 0.1136\n",
      "epoch 4574: loss = 0.0770\n",
      "epoch 4575: loss = 0.0244\n",
      "epoch 4576: loss = 0.1238\n",
      "epoch 4577: loss = 0.0690\n",
      "epoch 4578: loss = 0.1314\n",
      "epoch 4579: loss = 0.1936\n",
      "epoch 4580: loss = 0.1853\n",
      "epoch 4581: loss = 0.0741\n",
      "epoch 4582: loss = 0.1301\n",
      "epoch 4583: loss = 0.0167\n",
      "epoch 4584: loss = 0.1376\n",
      "epoch 4585: loss = 0.1230\n",
      "epoch 4586: loss = 0.1302\n",
      "epoch 4587: loss = 0.0703\n",
      "epoch 4588: loss = 0.1574\n",
      "epoch 4589: loss = 0.1200\n",
      "epoch 4590: loss = 0.1520\n",
      "epoch 4591: loss = 0.0665\n",
      "epoch 4592: loss = 0.1002\n",
      "epoch 4593: loss = 0.1058\n",
      "epoch 4594: loss = 0.1651\n",
      "epoch 4595: loss = 0.1707\n",
      "epoch 4596: loss = 0.1374\n",
      "epoch 4597: loss = 0.0401\n",
      "epoch 4598: loss = 0.0958\n",
      "epoch 4599: loss = 0.1170\n",
      "epoch 4600: loss = 0.0395\n",
      "epoch 4601: loss = 0.1003\n",
      "epoch 4602: loss = 0.1005\n",
      "epoch 4603: loss = 0.0410\n",
      "epoch 4604: loss = 0.0676\n",
      "epoch 4605: loss = 0.1439\n",
      "epoch 4606: loss = 0.0957\n",
      "epoch 4607: loss = 0.0334\n",
      "epoch 4608: loss = 0.1239\n",
      "epoch 4609: loss = 0.1691\n",
      "epoch 4610: loss = 0.1246\n",
      "epoch 4611: loss = 0.0830\n",
      "epoch 4612: loss = 0.1322\n",
      "epoch 4613: loss = 0.1663\n",
      "epoch 4614: loss = 0.1495\n",
      "epoch 4615: loss = 0.1579\n",
      "epoch 4616: loss = 0.1220\n",
      "epoch 4617: loss = 0.1336\n",
      "epoch 4618: loss = 0.0939\n",
      "epoch 4619: loss = 0.1165\n",
      "epoch 4620: loss = 0.1359\n",
      "epoch 4621: loss = 0.0394\n",
      "epoch 4622: loss = 0.0423\n",
      "epoch 4623: loss = 0.1111\n",
      "epoch 4624: loss = 0.1651\n",
      "epoch 4625: loss = 0.0145\n",
      "epoch 4626: loss = 0.1311\n",
      "epoch 4627: loss = 0.1208\n",
      "epoch 4628: loss = 0.0671\n",
      "epoch 4629: loss = 0.0970\n",
      "epoch 4630: loss = 0.0290\n",
      "epoch 4631: loss = 0.1464\n",
      "epoch 4632: loss = 0.1254\n",
      "epoch 4633: loss = 0.0882\n",
      "epoch 4634: loss = 0.1165\n",
      "epoch 4635: loss = 0.0793\n",
      "epoch 4636: loss = 0.0655\n",
      "epoch 4637: loss = 0.1263\n",
      "epoch 4638: loss = 0.1342\n",
      "epoch 4639: loss = 0.0380\n",
      "epoch 4640: loss = 0.1060\n",
      "epoch 4641: loss = 0.1576\n",
      "epoch 4642: loss = 0.1674\n",
      "epoch 4643: loss = 0.1111\n",
      "epoch 4644: loss = 0.0593\n",
      "epoch 4645: loss = 0.0765\n",
      "epoch 4646: loss = 0.0675\n",
      "epoch 4647: loss = 0.0932\n",
      "epoch 4648: loss = 0.1096\n",
      "epoch 4649: loss = 0.0585\n",
      "epoch 4650: loss = 0.1170\n",
      "epoch 4651: loss = 0.1986\n",
      "epoch 4652: loss = 0.1149\n",
      "epoch 4653: loss = 0.1728\n",
      "epoch 4654: loss = 0.1698\n",
      "epoch 4655: loss = 0.1137\n",
      "epoch 4656: loss = 0.1317\n",
      "epoch 4657: loss = 0.0521\n",
      "epoch 4658: loss = 0.1255\n",
      "epoch 4659: loss = 0.1337\n",
      "epoch 4660: loss = 0.1087\n",
      "epoch 4661: loss = 0.1086\n",
      "epoch 4662: loss = 0.1605\n",
      "epoch 4663: loss = 0.1622\n",
      "epoch 4664: loss = 0.0947\n",
      "epoch 4665: loss = 0.0756\n",
      "epoch 4666: loss = 0.0704\n",
      "epoch 4667: loss = 0.1633\n",
      "epoch 4668: loss = 0.1688\n",
      "epoch 4669: loss = 0.1255\n",
      "epoch 4670: loss = 0.1242\n",
      "epoch 4671: loss = 0.1649\n",
      "epoch 4672: loss = 0.1579\n",
      "epoch 4673: loss = 0.1088\n",
      "epoch 4674: loss = 0.1707\n",
      "epoch 4675: loss = 0.1595\n",
      "epoch 4676: loss = 0.0166\n",
      "epoch 4677: loss = 0.0673\n",
      "epoch 4678: loss = 0.1400\n",
      "epoch 4679: loss = 0.0872\n",
      "epoch 4680: loss = 0.0538\n",
      "epoch 4681: loss = 0.1573\n",
      "epoch 4682: loss = 0.1264\n",
      "epoch 4683: loss = 0.0955\n",
      "epoch 4684: loss = 0.1113\n",
      "epoch 4685: loss = 0.0780\n",
      "epoch 4686: loss = 0.1600\n",
      "epoch 4687: loss = 0.1721\n",
      "epoch 4688: loss = 0.0778\n",
      "epoch 4689: loss = 0.1328\n",
      "epoch 4690: loss = 0.1090\n",
      "epoch 4691: loss = 0.1276\n",
      "epoch 4692: loss = 0.1558\n",
      "epoch 4693: loss = 0.1224\n",
      "epoch 4694: loss = 0.1180\n",
      "epoch 4695: loss = 0.1331\n",
      "epoch 4696: loss = 0.0941\n",
      "epoch 4697: loss = 0.0593\n",
      "epoch 4698: loss = 0.1041\n",
      "epoch 4699: loss = 0.1469\n",
      "epoch 4700: loss = 0.1905\n",
      "epoch 4701: loss = 0.0961\n",
      "epoch 4702: loss = 0.1411\n",
      "epoch 4703: loss = 0.1094\n",
      "epoch 4704: loss = 0.1310\n",
      "epoch 4705: loss = 0.1427\n",
      "epoch 4706: loss = 0.0975\n",
      "epoch 4707: loss = 0.1142\n",
      "epoch 4708: loss = 0.1483\n",
      "epoch 4709: loss = 0.1002\n",
      "epoch 4710: loss = 0.0687\n",
      "epoch 4711: loss = 0.1550\n",
      "epoch 4712: loss = 0.0878\n",
      "epoch 4713: loss = 0.1138\n",
      "epoch 4714: loss = 0.0717\n",
      "epoch 4715: loss = 0.0765\n",
      "epoch 4716: loss = 0.0326\n",
      "epoch 4717: loss = 0.1500\n",
      "epoch 4718: loss = 0.1313\n",
      "epoch 4719: loss = 0.1518\n",
      "epoch 4720: loss = 0.1476\n",
      "epoch 4721: loss = 0.1183\n",
      "epoch 4722: loss = 0.1609\n",
      "epoch 4723: loss = 0.0663\n",
      "epoch 4724: loss = 0.0866\n",
      "epoch 4725: loss = 0.1904\n",
      "epoch 4726: loss = 0.0832\n",
      "epoch 4727: loss = 0.0862\n",
      "epoch 4728: loss = 0.0999\n",
      "epoch 4729: loss = 0.1480\n",
      "epoch 4730: loss = 0.0667\n",
      "epoch 4731: loss = 0.0734\n",
      "epoch 4732: loss = 0.1379\n",
      "epoch 4733: loss = 0.1591\n",
      "epoch 4734: loss = 0.1420\n",
      "epoch 4735: loss = 0.1520\n",
      "epoch 4736: loss = 0.1164\n",
      "epoch 4737: loss = 0.1331\n",
      "epoch 4738: loss = 0.0666\n",
      "epoch 4739: loss = 0.1100\n",
      "epoch 4740: loss = 0.0758\n",
      "epoch 4741: loss = 0.1188\n",
      "epoch 4742: loss = 0.1500\n",
      "epoch 4743: loss = 0.0485\n",
      "epoch 4744: loss = 0.0481\n",
      "epoch 4745: loss = 0.0784\n",
      "epoch 4746: loss = 0.1628\n",
      "epoch 4747: loss = 0.1933\n",
      "epoch 4748: loss = 0.1730\n",
      "epoch 4749: loss = 0.1325\n",
      "epoch 4750: loss = 0.0587\n",
      "epoch 4751: loss = 0.0995\n",
      "epoch 4752: loss = 0.1370\n",
      "epoch 4753: loss = 0.1313\n",
      "epoch 4754: loss = 0.1150\n",
      "epoch 4755: loss = 0.1362\n",
      "epoch 4756: loss = 0.0830\n",
      "epoch 4757: loss = 0.1278\n",
      "epoch 4758: loss = 0.1036\n",
      "epoch 4759: loss = 0.1642\n",
      "epoch 4760: loss = 0.1269\n",
      "epoch 4761: loss = 0.0984\n",
      "epoch 4762: loss = 0.1549\n",
      "epoch 4763: loss = 0.1179\n",
      "epoch 4764: loss = 0.0478\n",
      "epoch 4765: loss = 0.0908\n",
      "epoch 4766: loss = 0.0866\n",
      "epoch 4767: loss = 0.1455\n",
      "epoch 4768: loss = 0.1158\n",
      "epoch 4769: loss = 0.1450\n",
      "epoch 4770: loss = 0.1789\n",
      "epoch 4771: loss = 0.1249\n",
      "epoch 4772: loss = 0.0643\n",
      "epoch 4773: loss = 0.0735\n",
      "epoch 4774: loss = 0.1014\n",
      "epoch 4775: loss = 0.0629\n",
      "epoch 4776: loss = 0.1372\n",
      "epoch 4777: loss = 0.1813\n",
      "epoch 4778: loss = 0.1264\n",
      "epoch 4779: loss = 0.1076\n",
      "epoch 4780: loss = 0.1371\n",
      "epoch 4781: loss = 0.1153\n",
      "epoch 4782: loss = 0.1362\n",
      "epoch 4783: loss = 0.0928\n",
      "epoch 4784: loss = 0.0668\n",
      "epoch 4785: loss = 0.0867\n",
      "epoch 4786: loss = 0.1500\n",
      "epoch 4787: loss = 0.1438\n",
      "epoch 4788: loss = 0.1748\n",
      "epoch 4789: loss = 0.1689\n",
      "epoch 4790: loss = 0.1510\n",
      "epoch 4791: loss = 0.0828\n",
      "epoch 4792: loss = 0.1296\n",
      "epoch 4793: loss = 0.0422\n",
      "epoch 4794: loss = 0.0774\n",
      "epoch 4795: loss = 0.0552\n",
      "epoch 4796: loss = 0.1040\n",
      "epoch 4797: loss = 0.1492\n",
      "epoch 4798: loss = 0.0819\n",
      "epoch 4799: loss = 0.1579\n",
      "epoch 4800: loss = 0.0930\n",
      "epoch 4801: loss = 0.1049\n",
      "epoch 4802: loss = 0.0583\n",
      "epoch 4803: loss = 0.1187\n",
      "epoch 4804: loss = 0.0734\n",
      "epoch 4805: loss = 0.1478\n",
      "epoch 4806: loss = 0.1603\n",
      "epoch 4807: loss = 0.1179\n",
      "epoch 4808: loss = 0.1020\n",
      "epoch 4809: loss = 0.0367\n",
      "epoch 4810: loss = 0.1303\n",
      "epoch 4811: loss = 0.0188\n",
      "epoch 4812: loss = 0.0843\n",
      "epoch 4813: loss = 0.1243\n",
      "epoch 4814: loss = 0.1428\n",
      "epoch 4815: loss = 0.1351\n",
      "epoch 4816: loss = 0.1404\n",
      "epoch 4817: loss = 0.1030\n",
      "epoch 4818: loss = 0.0443\n",
      "epoch 4819: loss = 0.0651\n",
      "epoch 4820: loss = 0.0789\n",
      "epoch 4821: loss = 0.0695\n",
      "epoch 4822: loss = 0.1360\n",
      "epoch 4823: loss = 0.0437\n",
      "epoch 4824: loss = 0.0472\n",
      "epoch 4825: loss = 0.0774\n",
      "epoch 4826: loss = 0.0743\n",
      "epoch 4827: loss = 0.0663\n",
      "epoch 4828: loss = 0.1014\n",
      "epoch 4829: loss = 0.0898\n",
      "epoch 4830: loss = 0.1649\n",
      "epoch 4831: loss = 0.0960\n",
      "epoch 4832: loss = 0.0962\n",
      "epoch 4833: loss = 0.1167\n",
      "epoch 4834: loss = 0.1468\n",
      "epoch 4835: loss = 0.1310\n",
      "epoch 4836: loss = 0.0934\n",
      "epoch 4837: loss = 0.1033\n",
      "epoch 4838: loss = 0.0367\n",
      "epoch 4839: loss = 0.1177\n",
      "epoch 4840: loss = 0.0933\n",
      "epoch 4841: loss = 0.1701\n",
      "epoch 4842: loss = 0.1252\n",
      "epoch 4843: loss = 0.0823\n",
      "epoch 4844: loss = 0.0449\n",
      "epoch 4845: loss = 0.0478\n",
      "epoch 4846: loss = 0.1655\n",
      "epoch 4847: loss = 0.0996\n",
      "epoch 4848: loss = 0.1231\n",
      "epoch 4849: loss = 0.0554\n",
      "epoch 4850: loss = 0.1164\n",
      "epoch 4851: loss = 0.1780\n",
      "epoch 4852: loss = 0.1175\n",
      "epoch 4853: loss = 0.1341\n",
      "epoch 4854: loss = 0.0355\n",
      "epoch 4855: loss = 0.0952\n",
      "epoch 4856: loss = 0.1204\n",
      "epoch 4857: loss = 0.0971\n",
      "epoch 4858: loss = 0.0891\n",
      "epoch 4859: loss = 0.1334\n",
      "epoch 4860: loss = 0.1056\n",
      "epoch 4861: loss = 0.1340\n",
      "epoch 4862: loss = 0.1433\n",
      "epoch 4863: loss = 0.0750\n",
      "epoch 4864: loss = 0.1023\n",
      "epoch 4865: loss = 0.0319\n",
      "epoch 4866: loss = 0.1180\n",
      "epoch 4867: loss = 0.0831\n",
      "epoch 4868: loss = 0.0913\n",
      "epoch 4869: loss = 0.1086\n",
      "epoch 4870: loss = 0.0454\n",
      "epoch 4871: loss = 0.1438\n",
      "epoch 4872: loss = 0.0248\n",
      "epoch 4873: loss = 0.1149\n",
      "epoch 4874: loss = 0.1099\n",
      "epoch 4875: loss = 0.1372\n",
      "epoch 4876: loss = 0.1814\n",
      "epoch 4877: loss = 0.1009\n",
      "epoch 4878: loss = 0.1142\n",
      "epoch 4879: loss = 0.1023\n",
      "epoch 4880: loss = 0.0514\n",
      "epoch 4881: loss = 0.0980\n",
      "epoch 4882: loss = 0.0656\n",
      "epoch 4883: loss = 0.0789\n",
      "epoch 4884: loss = 0.0573\n",
      "epoch 4885: loss = 0.1108\n",
      "epoch 4886: loss = 0.0961\n",
      "epoch 4887: loss = 0.0730\n",
      "epoch 4888: loss = 0.0667\n",
      "epoch 4889: loss = 0.0847\n",
      "epoch 4890: loss = 0.1516\n",
      "epoch 4891: loss = 0.0565\n",
      "epoch 4892: loss = 0.1472\n",
      "epoch 4893: loss = 0.1266\n",
      "epoch 4894: loss = 0.0456\n",
      "epoch 4895: loss = 0.1407\n",
      "epoch 4896: loss = 0.1042\n",
      "epoch 4897: loss = 0.0990\n",
      "epoch 4898: loss = 0.0627\n",
      "epoch 4899: loss = 0.1540\n",
      "epoch 4900: loss = 0.1633\n",
      "epoch 4901: loss = 0.1218\n",
      "epoch 4902: loss = 0.0995\n",
      "epoch 4903: loss = 0.0772\n",
      "epoch 4904: loss = 0.1782\n",
      "epoch 4905: loss = 0.1610\n",
      "epoch 4906: loss = 0.0813\n",
      "epoch 4907: loss = 0.1776\n",
      "epoch 4908: loss = 0.1499\n",
      "epoch 4909: loss = 0.1083\n",
      "epoch 4910: loss = 0.1800\n",
      "epoch 4911: loss = 0.1656\n",
      "epoch 4912: loss = 0.0808\n",
      "epoch 4913: loss = 0.0591\n",
      "epoch 4914: loss = 0.0681\n",
      "epoch 4915: loss = 0.1141\n",
      "epoch 4916: loss = 0.0124\n",
      "epoch 4917: loss = 0.1158\n",
      "epoch 4918: loss = 0.1260\n",
      "epoch 4919: loss = 0.1143\n",
      "epoch 4920: loss = 0.1219\n",
      "epoch 4921: loss = 0.0862\n",
      "epoch 4922: loss = 0.1658\n",
      "epoch 4923: loss = 0.1002\n",
      "epoch 4924: loss = 0.1027\n",
      "epoch 4925: loss = 0.0829\n",
      "epoch 4926: loss = 0.0137\n",
      "epoch 4927: loss = 0.1099\n",
      "epoch 4928: loss = 0.1502\n",
      "epoch 4929: loss = 0.0961\n",
      "epoch 4930: loss = 0.0880\n",
      "epoch 4931: loss = 0.1570\n",
      "epoch 4932: loss = 0.1134\n",
      "epoch 4933: loss = 0.0977\n",
      "epoch 4934: loss = 0.1056\n",
      "epoch 4935: loss = 0.1392\n",
      "epoch 4936: loss = 0.0781\n",
      "epoch 4937: loss = 0.0518\n",
      "epoch 4938: loss = 0.0639\n",
      "epoch 4939: loss = 0.0914\n",
      "epoch 4940: loss = 0.0940\n",
      "epoch 4941: loss = 0.0588\n",
      "epoch 4942: loss = 0.0572\n",
      "epoch 4943: loss = 0.0517\n",
      "epoch 4944: loss = 0.1304\n",
      "epoch 4945: loss = 0.1676\n",
      "epoch 4946: loss = 0.0559\n",
      "epoch 4947: loss = 0.1867\n",
      "epoch 4948: loss = 0.1668\n",
      "epoch 4949: loss = 0.1365\n",
      "epoch 4950: loss = 0.1398\n",
      "epoch 4951: loss = 0.1064\n",
      "epoch 4952: loss = 0.0841\n",
      "epoch 4953: loss = 0.1183\n",
      "epoch 4954: loss = 0.1021\n",
      "epoch 4955: loss = 0.1088\n",
      "epoch 4956: loss = 0.1310\n",
      "epoch 4957: loss = 0.1578\n",
      "epoch 4958: loss = 0.1465\n",
      "epoch 4959: loss = 0.1459\n",
      "epoch 4960: loss = 0.0647\n",
      "epoch 4961: loss = 0.1415\n",
      "epoch 4962: loss = 0.0877\n",
      "epoch 4963: loss = 0.0634\n",
      "epoch 4964: loss = 0.1445\n",
      "epoch 4965: loss = 0.1006\n",
      "epoch 4966: loss = 0.0768\n",
      "epoch 4967: loss = 0.0868\n",
      "epoch 4968: loss = 0.1072\n",
      "epoch 4969: loss = 0.0931\n",
      "epoch 4970: loss = 0.0793\n",
      "epoch 4971: loss = 0.0614\n",
      "epoch 4972: loss = 0.1460\n",
      "epoch 4973: loss = 0.1223\n",
      "epoch 4974: loss = 0.1126\n",
      "epoch 4975: loss = 0.1295\n",
      "epoch 4976: loss = 0.0982\n",
      "epoch 4977: loss = 0.1188\n",
      "epoch 4978: loss = 0.1122\n",
      "epoch 4979: loss = 0.1426\n",
      "epoch 4980: loss = 0.1262\n",
      "epoch 4981: loss = 0.1588\n",
      "epoch 4982: loss = 0.1597\n",
      "epoch 4983: loss = 0.1599\n",
      "epoch 4984: loss = 0.1859\n",
      "epoch 4985: loss = 0.1618\n",
      "epoch 4986: loss = 0.1044\n",
      "epoch 4987: loss = 0.0812\n",
      "epoch 4988: loss = 0.0856\n",
      "epoch 4989: loss = 0.1295\n",
      "epoch 4990: loss = 0.0634\n",
      "epoch 4991: loss = 0.1577\n",
      "epoch 4992: loss = 0.1116\n",
      "epoch 4993: loss = 0.0633\n",
      "epoch 4994: loss = 0.0840\n",
      "epoch 4995: loss = 0.1527\n",
      "epoch 4996: loss = 0.1792\n",
      "epoch 4997: loss = 0.0995\n",
      "epoch 4998: loss = 0.1247\n",
      "epoch 4999: loss = 0.0728\n",
      "epoch 5000: loss = 0.1344\n",
      "epoch 5001: loss = 0.1666\n",
      "epoch 5002: loss = 0.1138\n",
      "epoch 5003: loss = 0.0706\n",
      "epoch 5004: loss = 0.0966\n",
      "epoch 5005: loss = 0.0992\n",
      "epoch 5006: loss = 0.0770\n",
      "epoch 5007: loss = 0.1538\n",
      "epoch 5008: loss = 0.1648\n",
      "epoch 5009: loss = 0.1810\n",
      "epoch 5010: loss = 0.1084\n",
      "epoch 5011: loss = 0.1227\n",
      "epoch 5012: loss = 0.1413\n",
      "epoch 5013: loss = 0.0540\n",
      "epoch 5014: loss = 0.0802\n",
      "epoch 5015: loss = 0.1392\n",
      "epoch 5016: loss = 0.0724\n",
      "epoch 5017: loss = 0.1192\n",
      "epoch 5018: loss = 0.0818\n",
      "epoch 5019: loss = 0.1116\n",
      "epoch 5020: loss = 0.1374\n",
      "epoch 5021: loss = 0.1013\n",
      "epoch 5022: loss = 0.0853\n",
      "epoch 5023: loss = 0.1242\n",
      "epoch 5024: loss = 0.0645\n",
      "epoch 5025: loss = 0.1259\n",
      "epoch 5026: loss = 0.0703\n",
      "epoch 5027: loss = 0.1231\n",
      "epoch 5028: loss = 0.1016\n",
      "epoch 5029: loss = 0.0132\n",
      "epoch 5030: loss = 0.0293\n",
      "epoch 5031: loss = 0.1110\n",
      "epoch 5032: loss = 0.1317\n",
      "epoch 5033: loss = 0.0902\n",
      "epoch 5034: loss = 0.1087\n",
      "epoch 5035: loss = 0.1068\n",
      "epoch 5036: loss = 0.1493\n",
      "epoch 5037: loss = 0.1827\n",
      "epoch 5038: loss = 0.1173\n",
      "epoch 5039: loss = 0.1203\n",
      "epoch 5040: loss = 0.1517\n",
      "epoch 5041: loss = 0.1025\n",
      "epoch 5042: loss = 0.1166\n",
      "epoch 5043: loss = 0.1098\n",
      "epoch 5044: loss = 0.1266\n",
      "epoch 5045: loss = 0.0550\n",
      "epoch 5046: loss = 0.0782\n",
      "epoch 5047: loss = 0.1377\n",
      "epoch 5048: loss = 0.1145\n",
      "epoch 5049: loss = 0.0538\n",
      "epoch 5050: loss = 0.0410\n",
      "epoch 5051: loss = 0.1057\n",
      "epoch 5052: loss = 0.0615\n",
      "epoch 5053: loss = 0.0865\n",
      "epoch 5054: loss = 0.1053\n",
      "epoch 5055: loss = 0.1217\n",
      "epoch 5056: loss = 0.1368\n",
      "epoch 5057: loss = 0.1174\n",
      "epoch 5058: loss = 0.1932\n",
      "epoch 5059: loss = 0.1308\n",
      "epoch 5060: loss = 0.0666\n",
      "epoch 5061: loss = 0.0466\n",
      "epoch 5062: loss = 0.0460\n",
      "epoch 5063: loss = 0.1921\n",
      "epoch 5064: loss = 0.1054\n",
      "epoch 5065: loss = 0.1614\n",
      "epoch 5066: loss = 0.0135\n",
      "epoch 5067: loss = 0.0601\n",
      "epoch 5068: loss = 0.1343\n",
      "epoch 5069: loss = 0.1596\n",
      "epoch 5070: loss = 0.0946\n",
      "epoch 5071: loss = 0.1316\n",
      "epoch 5072: loss = 0.1216\n",
      "epoch 5073: loss = 0.1462\n",
      "epoch 5074: loss = 0.0875\n",
      "epoch 5075: loss = 0.1684\n",
      "epoch 5076: loss = 0.0897\n",
      "epoch 5077: loss = 0.1066\n",
      "epoch 5078: loss = 0.1321\n",
      "epoch 5079: loss = 0.0759\n",
      "epoch 5080: loss = 0.1232\n",
      "epoch 5081: loss = 0.1433\n",
      "epoch 5082: loss = 0.1492\n",
      "epoch 5083: loss = 0.1764\n",
      "epoch 5084: loss = 0.1392\n",
      "epoch 5085: loss = 0.0731\n",
      "epoch 5086: loss = 0.0575\n",
      "epoch 5087: loss = 0.1553\n",
      "epoch 5088: loss = 0.0428\n",
      "epoch 5089: loss = 0.1050\n",
      "epoch 5090: loss = 0.1299\n",
      "epoch 5091: loss = 0.0981\n",
      "epoch 5092: loss = 0.0698\n",
      "epoch 5093: loss = 0.0709\n",
      "epoch 5094: loss = 0.1039\n",
      "epoch 5095: loss = 0.0494\n",
      "epoch 5096: loss = 0.0762\n",
      "epoch 5097: loss = 0.1392\n",
      "epoch 5098: loss = 0.1497\n",
      "epoch 5099: loss = 0.0677\n",
      "epoch 5100: loss = 0.1501\n",
      "epoch 5101: loss = 0.0539\n",
      "epoch 5102: loss = 0.1082\n",
      "epoch 5103: loss = 0.1211\n",
      "epoch 5104: loss = 0.0492\n",
      "epoch 5105: loss = 0.0980\n",
      "epoch 5106: loss = 0.1156\n",
      "epoch 5107: loss = 0.0798\n",
      "epoch 5108: loss = 0.1603\n",
      "epoch 5109: loss = 0.0450\n",
      "epoch 5110: loss = 0.0604\n",
      "epoch 5111: loss = 0.1340\n",
      "epoch 5112: loss = 0.1062\n",
      "epoch 5113: loss = 0.0637\n",
      "epoch 5114: loss = 0.1197\n",
      "epoch 5115: loss = 0.1506\n",
      "epoch 5116: loss = 0.1872\n",
      "epoch 5117: loss = 0.0967\n",
      "epoch 5118: loss = 0.1817\n",
      "epoch 5119: loss = 0.0975\n",
      "epoch 5120: loss = 0.1041\n",
      "epoch 5121: loss = 0.1582\n",
      "epoch 5122: loss = 0.1880\n",
      "epoch 5123: loss = 0.1345\n",
      "epoch 5124: loss = 0.1695\n",
      "epoch 5125: loss = 0.0961\n",
      "epoch 5126: loss = 0.0875\n",
      "epoch 5127: loss = 0.0464\n",
      "epoch 5128: loss = 0.1537\n",
      "epoch 5129: loss = 0.1549\n",
      "epoch 5130: loss = 0.1218\n",
      "epoch 5131: loss = 0.1162\n",
      "epoch 5132: loss = 0.1751\n",
      "epoch 5133: loss = 0.1104\n",
      "epoch 5134: loss = 0.0664\n",
      "epoch 5135: loss = 0.0901\n",
      "epoch 5136: loss = 0.1189\n",
      "epoch 5137: loss = 0.1524\n",
      "epoch 5138: loss = 0.0801\n",
      "epoch 5139: loss = 0.1616\n",
      "epoch 5140: loss = 0.0613\n",
      "epoch 5141: loss = 0.1018\n",
      "epoch 5142: loss = 0.1111\n",
      "epoch 5143: loss = 0.1502\n",
      "epoch 5144: loss = 0.1517\n",
      "epoch 5145: loss = 0.1800\n",
      "epoch 5146: loss = 0.1631\n",
      "epoch 5147: loss = 0.1669\n",
      "epoch 5148: loss = 0.0974\n",
      "epoch 5149: loss = 0.0442\n",
      "epoch 5150: loss = 0.1286\n",
      "epoch 5151: loss = 0.1254\n",
      "epoch 5152: loss = 0.0925\n",
      "epoch 5153: loss = 0.0368\n",
      "epoch 5154: loss = 0.1445\n",
      "epoch 5155: loss = 0.1671\n",
      "epoch 5156: loss = 0.1403\n",
      "epoch 5157: loss = 0.1496\n",
      "epoch 5158: loss = 0.1262\n",
      "epoch 5159: loss = 0.1085\n",
      "epoch 5160: loss = 0.1375\n",
      "epoch 5161: loss = 0.1282\n",
      "epoch 5162: loss = 0.1804\n",
      "epoch 5163: loss = 0.0950\n",
      "epoch 5164: loss = 0.0272\n",
      "epoch 5165: loss = 0.0790\n",
      "epoch 5166: loss = 0.0944\n",
      "epoch 5167: loss = 0.1251\n",
      "epoch 5168: loss = 0.0949\n",
      "epoch 5169: loss = 0.1414\n",
      "epoch 5170: loss = 0.0937\n",
      "epoch 5171: loss = 0.1383\n",
      "epoch 5172: loss = 0.2017\n",
      "epoch 5173: loss = 0.1047\n",
      "epoch 5174: loss = 0.1219\n",
      "epoch 5175: loss = 0.0894\n",
      "epoch 5176: loss = 0.0624\n",
      "epoch 5177: loss = 0.0691\n",
      "epoch 5178: loss = 0.0793\n",
      "epoch 5179: loss = 0.0889\n",
      "epoch 5180: loss = 0.1753\n",
      "epoch 5181: loss = 0.0332\n",
      "epoch 5182: loss = 0.0858\n",
      "epoch 5183: loss = 0.0822\n",
      "epoch 5184: loss = 0.1195\n",
      "epoch 5185: loss = 0.1873\n",
      "epoch 5186: loss = 0.1507\n",
      "epoch 5187: loss = 0.0993\n",
      "epoch 5188: loss = 0.1587\n",
      "epoch 5189: loss = 0.0587\n",
      "epoch 5190: loss = 0.1157\n",
      "epoch 5191: loss = 0.1032\n",
      "epoch 5192: loss = 0.0998\n",
      "epoch 5193: loss = 0.1304\n",
      "epoch 5194: loss = 0.0995\n",
      "epoch 5195: loss = 0.1132\n",
      "epoch 5196: loss = 0.0328\n",
      "epoch 5197: loss = 0.1119\n",
      "epoch 5198: loss = 0.1605\n",
      "epoch 5199: loss = 0.1512\n",
      "epoch 5200: loss = 0.1265\n",
      "epoch 5201: loss = 0.1068\n",
      "epoch 5202: loss = 0.1480\n",
      "epoch 5203: loss = 0.0855\n",
      "epoch 5204: loss = 0.1559\n",
      "epoch 5205: loss = 0.0894\n",
      "epoch 5206: loss = 0.0905\n",
      "epoch 5207: loss = 0.0845\n",
      "epoch 5208: loss = 0.0948\n",
      "epoch 5209: loss = 0.1649\n",
      "epoch 5210: loss = 0.1827\n",
      "epoch 5211: loss = 0.0393\n",
      "epoch 5212: loss = 0.1200\n",
      "epoch 5213: loss = 0.1097\n",
      "epoch 5214: loss = 0.1578\n",
      "epoch 5215: loss = 0.1988\n",
      "epoch 5216: loss = 0.1547\n",
      "epoch 5217: loss = 0.1004\n",
      "epoch 5218: loss = 0.1225\n",
      "epoch 5219: loss = 0.0397\n",
      "epoch 5220: loss = 0.1117\n",
      "epoch 5221: loss = 0.1327\n",
      "epoch 5222: loss = 0.1170\n",
      "epoch 5223: loss = 0.1123\n",
      "epoch 5224: loss = 0.1079\n",
      "epoch 5225: loss = 0.0810\n",
      "epoch 5226: loss = 0.1322\n",
      "epoch 5227: loss = 0.1646\n",
      "epoch 5228: loss = 0.0948\n",
      "epoch 5229: loss = 0.1000\n",
      "epoch 5230: loss = 0.0739\n",
      "epoch 5231: loss = 0.1262\n",
      "epoch 5232: loss = 0.1667\n",
      "epoch 5233: loss = 0.1537\n",
      "epoch 5234: loss = 0.1007\n",
      "epoch 5235: loss = 0.1023\n",
      "epoch 5236: loss = 0.0918\n",
      "epoch 5237: loss = 0.1476\n",
      "epoch 5238: loss = 0.1044\n",
      "epoch 5239: loss = 0.1045\n",
      "epoch 5240: loss = 0.1502\n",
      "epoch 5241: loss = 0.0362\n",
      "epoch 5242: loss = 0.1211\n",
      "epoch 5243: loss = 0.1626\n",
      "epoch 5244: loss = 0.1536\n",
      "epoch 5245: loss = 0.0977\n",
      "epoch 5246: loss = 0.0506\n",
      "epoch 5247: loss = 0.0753\n",
      "epoch 5248: loss = 0.0589\n",
      "epoch 5249: loss = 0.0480\n",
      "epoch 5250: loss = 0.1321\n",
      "epoch 5251: loss = 0.0815\n",
      "epoch 5252: loss = 0.1412\n",
      "epoch 5253: loss = 0.0750\n",
      "epoch 5254: loss = 0.1541\n",
      "epoch 5255: loss = 0.0813\n",
      "epoch 5256: loss = 0.1106\n",
      "epoch 5257: loss = 0.1572\n",
      "epoch 5258: loss = 0.1360\n",
      "epoch 5259: loss = 0.0333\n",
      "epoch 5260: loss = 0.1423\n",
      "epoch 5261: loss = 0.0857\n",
      "epoch 5262: loss = 0.1503\n",
      "epoch 5263: loss = 0.1034\n",
      "epoch 5264: loss = 0.0839\n",
      "epoch 5265: loss = 0.0699\n",
      "epoch 5266: loss = 0.0681\n",
      "epoch 5267: loss = 0.0816\n",
      "epoch 5268: loss = 0.1319\n",
      "epoch 5269: loss = 0.0897\n",
      "epoch 5270: loss = 0.0557\n",
      "epoch 5271: loss = 0.1473\n",
      "epoch 5272: loss = 0.1130\n",
      "epoch 5273: loss = 0.0526\n",
      "epoch 5274: loss = 0.1549\n",
      "epoch 5275: loss = 0.1684\n",
      "epoch 5276: loss = 0.1736\n",
      "epoch 5277: loss = 0.1388\n",
      "epoch 5278: loss = 0.1209\n",
      "epoch 5279: loss = 0.1213\n",
      "epoch 5280: loss = 0.0910\n",
      "epoch 5281: loss = 0.0091\n",
      "epoch 5282: loss = 0.1255\n",
      "epoch 5283: loss = 0.0456\n",
      "epoch 5284: loss = 0.1405\n",
      "epoch 5285: loss = 0.0614\n",
      "epoch 5286: loss = 0.1359\n",
      "epoch 5287: loss = 0.1107\n",
      "epoch 5288: loss = 0.1877\n",
      "epoch 5289: loss = 0.1084\n",
      "epoch 5290: loss = 0.1095\n",
      "epoch 5291: loss = 0.1133\n",
      "epoch 5292: loss = 0.0868\n",
      "epoch 5293: loss = 0.0975\n",
      "epoch 5294: loss = 0.1670\n",
      "epoch 5295: loss = 0.0971\n",
      "epoch 5296: loss = 0.0651\n",
      "epoch 5297: loss = 0.1168\n",
      "epoch 5298: loss = 0.1005\n",
      "epoch 5299: loss = 0.1237\n",
      "epoch 5300: loss = 0.0766\n",
      "epoch 5301: loss = 0.0922\n",
      "epoch 5302: loss = 0.1448\n",
      "epoch 5303: loss = 0.1660\n",
      "epoch 5304: loss = 0.1569\n",
      "epoch 5305: loss = 0.0754\n",
      "epoch 5306: loss = 0.1273\n",
      "epoch 5307: loss = 0.0704\n",
      "epoch 5308: loss = 0.1357\n",
      "epoch 5309: loss = 0.0965\n",
      "epoch 5310: loss = 0.1001\n",
      "epoch 5311: loss = 0.1276\n",
      "epoch 5312: loss = 0.1465\n",
      "epoch 5313: loss = 0.1372\n",
      "epoch 5314: loss = 0.1644\n",
      "epoch 5315: loss = 0.1349\n",
      "epoch 5316: loss = 0.0878\n",
      "epoch 5317: loss = 0.0920\n",
      "epoch 5318: loss = 0.1303\n",
      "epoch 5319: loss = 0.1120\n",
      "epoch 5320: loss = 0.1438\n",
      "epoch 5321: loss = 0.1355\n",
      "epoch 5322: loss = 0.1685\n",
      "epoch 5323: loss = 0.0515\n",
      "epoch 5324: loss = 0.1627\n",
      "epoch 5325: loss = 0.0786\n",
      "epoch 5326: loss = 0.1393\n",
      "epoch 5327: loss = 0.0870\n",
      "epoch 5328: loss = 0.0890\n",
      "epoch 5329: loss = 0.1503\n",
      "epoch 5330: loss = 0.0905\n",
      "epoch 5331: loss = 0.1989\n",
      "epoch 5332: loss = 0.1759\n",
      "epoch 5333: loss = 0.0438\n",
      "epoch 5334: loss = 0.0117\n",
      "epoch 5335: loss = 0.0856\n",
      "epoch 5336: loss = 0.0401\n",
      "epoch 5337: loss = 0.1220\n",
      "epoch 5338: loss = 0.1027\n",
      "epoch 5339: loss = 0.0527\n",
      "epoch 5340: loss = 0.1206\n",
      "epoch 5341: loss = 0.1986\n",
      "epoch 5342: loss = 0.1707\n",
      "epoch 5343: loss = 0.1135\n",
      "epoch 5344: loss = 0.1612\n",
      "epoch 5345: loss = 0.1077\n",
      "epoch 5346: loss = 0.1542\n",
      "epoch 5347: loss = 0.0259\n",
      "epoch 5348: loss = 0.0818\n",
      "epoch 5349: loss = 0.1246\n",
      "epoch 5350: loss = 0.1146\n",
      "epoch 5351: loss = 0.1195\n",
      "epoch 5352: loss = 0.0329\n",
      "epoch 5353: loss = 0.1074\n",
      "epoch 5354: loss = 0.0320\n",
      "epoch 5355: loss = 0.1332\n",
      "epoch 5356: loss = 0.1414\n",
      "epoch 5357: loss = 0.1599\n",
      "epoch 5358: loss = 0.0844\n",
      "epoch 5359: loss = 0.1615\n",
      "epoch 5360: loss = 0.1895\n",
      "epoch 5361: loss = 0.1534\n",
      "epoch 5362: loss = 0.1662\n",
      "epoch 5363: loss = 0.0770\n",
      "epoch 5364: loss = 0.1467\n",
      "epoch 5365: loss = 0.1042\n",
      "epoch 5366: loss = 0.1293\n",
      "epoch 5367: loss = 0.1445\n",
      "epoch 5368: loss = 0.0624\n",
      "epoch 5369: loss = 0.1502\n",
      "epoch 5370: loss = 0.1335\n",
      "epoch 5371: loss = 0.1260\n",
      "epoch 5372: loss = 0.1290\n",
      "epoch 5373: loss = 0.1769\n",
      "epoch 5374: loss = 0.0601\n",
      "epoch 5375: loss = 0.1876\n",
      "epoch 5376: loss = 0.1705\n",
      "epoch 5377: loss = 0.0905\n",
      "epoch 5378: loss = 0.1124\n",
      "epoch 5379: loss = 0.1358\n",
      "epoch 5380: loss = 0.0652\n",
      "epoch 5381: loss = 0.1184\n",
      "epoch 5382: loss = 0.1589\n",
      "epoch 5383: loss = 0.1079\n",
      "epoch 5384: loss = 0.0419\n",
      "epoch 5385: loss = 0.1013\n",
      "epoch 5386: loss = 0.1270\n",
      "epoch 5387: loss = 0.0722\n",
      "epoch 5388: loss = 0.1300\n",
      "epoch 5389: loss = 0.0131\n",
      "epoch 5390: loss = 0.0859\n",
      "epoch 5391: loss = 0.1027\n",
      "epoch 5392: loss = 0.0175\n",
      "epoch 5393: loss = 0.1172\n",
      "epoch 5394: loss = 0.0453\n",
      "epoch 5395: loss = 0.1663\n",
      "epoch 5396: loss = 0.1630\n",
      "epoch 5397: loss = 0.0944\n",
      "epoch 5398: loss = 0.1281\n",
      "epoch 5399: loss = 0.1682\n",
      "epoch 5400: loss = 0.1686\n",
      "epoch 5401: loss = 0.0864\n",
      "epoch 5402: loss = 0.1033\n",
      "epoch 5403: loss = 0.1855\n",
      "epoch 5404: loss = 0.1360\n",
      "epoch 5405: loss = 0.0602\n",
      "epoch 5406: loss = 0.0480\n",
      "epoch 5407: loss = 0.1795\n",
      "epoch 5408: loss = 0.0874\n",
      "epoch 5409: loss = 0.1229\n",
      "epoch 5410: loss = 0.0228\n",
      "epoch 5411: loss = 0.1324\n",
      "epoch 5412: loss = 0.1345\n",
      "epoch 5413: loss = 0.1847\n",
      "epoch 5414: loss = 0.1405\n",
      "epoch 5415: loss = 0.1352\n",
      "epoch 5416: loss = 0.0870\n",
      "epoch 5417: loss = 0.1228\n",
      "epoch 5418: loss = 0.1640\n",
      "epoch 5419: loss = 0.1536\n",
      "epoch 5420: loss = 0.1863\n",
      "epoch 5421: loss = 0.1434\n",
      "epoch 5422: loss = 0.1723\n",
      "epoch 5423: loss = 0.1010\n",
      "epoch 5424: loss = 0.1765\n",
      "epoch 5425: loss = 0.1839\n",
      "epoch 5426: loss = 0.1468\n",
      "epoch 5427: loss = 0.0664\n",
      "epoch 5428: loss = 0.0573\n",
      "epoch 5429: loss = 0.1465\n",
      "epoch 5430: loss = 0.1171\n",
      "epoch 5431: loss = 0.0859\n",
      "epoch 5432: loss = 0.1434\n",
      "epoch 5433: loss = 0.0873\n",
      "epoch 5434: loss = 0.1339\n",
      "epoch 5435: loss = 0.1465\n",
      "epoch 5436: loss = 0.0533\n",
      "epoch 5437: loss = 0.1585\n",
      "epoch 5438: loss = 0.1290\n",
      "epoch 5439: loss = 0.1753\n",
      "epoch 5440: loss = 0.1022\n",
      "epoch 5441: loss = 0.0569\n",
      "epoch 5442: loss = 0.0583\n",
      "epoch 5443: loss = 0.1746\n",
      "epoch 5444: loss = 0.0974\n",
      "epoch 5445: loss = 0.1596\n",
      "epoch 5446: loss = 0.1691\n",
      "epoch 5447: loss = 0.0667\n",
      "epoch 5448: loss = 0.1580\n",
      "epoch 5449: loss = 0.1935\n",
      "epoch 5450: loss = 0.2026\n",
      "epoch 5451: loss = 0.1672\n",
      "epoch 5452: loss = 0.1087\n",
      "epoch 5453: loss = 0.1118\n",
      "epoch 5454: loss = 0.0985\n",
      "epoch 5455: loss = 0.1594\n",
      "epoch 5456: loss = 0.0744\n",
      "epoch 5457: loss = 0.1456\n",
      "epoch 5458: loss = 0.1109\n",
      "epoch 5459: loss = 0.0474\n",
      "epoch 5460: loss = 0.0802\n",
      "epoch 5461: loss = 0.1068\n",
      "epoch 5462: loss = 0.0443\n",
      "epoch 5463: loss = 0.1168\n",
      "epoch 5464: loss = 0.0989\n",
      "epoch 5465: loss = 0.1283\n",
      "epoch 5466: loss = 0.1190\n",
      "epoch 5467: loss = 0.1040\n",
      "epoch 5468: loss = 0.0353\n",
      "epoch 5469: loss = 0.1424\n",
      "epoch 5470: loss = 0.0491\n",
      "epoch 5471: loss = 0.0572\n",
      "epoch 5472: loss = 0.0322\n",
      "epoch 5473: loss = 0.0952\n",
      "epoch 5474: loss = 0.0994\n",
      "epoch 5475: loss = 0.1677\n",
      "epoch 5476: loss = 0.0737\n",
      "epoch 5477: loss = 0.1268\n",
      "epoch 5478: loss = 0.1366\n",
      "epoch 5479: loss = 0.1435\n",
      "epoch 5480: loss = 0.1826\n",
      "epoch 5481: loss = 0.1795\n",
      "epoch 5482: loss = 0.1658\n",
      "epoch 5483: loss = 0.0834\n",
      "epoch 5484: loss = 0.1529\n",
      "epoch 5485: loss = 0.1140\n",
      "epoch 5486: loss = 0.1437\n",
      "epoch 5487: loss = 0.0391\n",
      "epoch 5488: loss = 0.0101\n",
      "epoch 5489: loss = 0.0769\n",
      "epoch 5490: loss = 0.0162\n",
      "epoch 5491: loss = 0.1336\n",
      "epoch 5492: loss = 0.1238\n",
      "epoch 5493: loss = 0.1210\n",
      "epoch 5494: loss = 0.1150\n",
      "epoch 5495: loss = 0.1010\n",
      "epoch 5496: loss = 0.1558\n",
      "epoch 5497: loss = 0.1119\n",
      "epoch 5498: loss = 0.0550\n",
      "epoch 5499: loss = 0.1535\n",
      "epoch 5500: loss = 0.1586\n",
      "epoch 5501: loss = 0.1445\n",
      "epoch 5502: loss = 0.1394\n",
      "epoch 5503: loss = 0.1673\n",
      "epoch 5504: loss = 0.1159\n",
      "epoch 5505: loss = 0.1652\n",
      "epoch 5506: loss = 0.1066\n",
      "epoch 5507: loss = 0.1603\n",
      "epoch 5508: loss = 0.0209\n",
      "epoch 5509: loss = 0.1228\n",
      "epoch 5510: loss = 0.1052\n",
      "epoch 5511: loss = 0.1673\n",
      "epoch 5512: loss = 0.0844\n",
      "epoch 5513: loss = 0.0197\n",
      "epoch 5514: loss = 0.1219\n",
      "epoch 5515: loss = 0.0908\n",
      "epoch 5516: loss = 0.1750\n",
      "epoch 5517: loss = 0.1149\n",
      "epoch 5518: loss = 0.0847\n",
      "epoch 5519: loss = 0.1015\n",
      "epoch 5520: loss = 0.1006\n",
      "epoch 5521: loss = 0.0928\n",
      "epoch 5522: loss = 0.0389\n",
      "epoch 5523: loss = 0.0734\n",
      "epoch 5524: loss = 0.0938\n",
      "epoch 5525: loss = 0.0993\n",
      "epoch 5526: loss = 0.1312\n",
      "epoch 5527: loss = 0.1019\n",
      "epoch 5528: loss = 0.1375\n",
      "epoch 5529: loss = 0.1445\n",
      "epoch 5530: loss = 0.1491\n",
      "epoch 5531: loss = 0.0900\n",
      "epoch 5532: loss = 0.1221\n",
      "epoch 5533: loss = 0.1323\n",
      "epoch 5534: loss = 0.1172\n",
      "epoch 5535: loss = 0.0236\n",
      "epoch 5536: loss = 0.1043\n",
      "epoch 5537: loss = 0.1071\n",
      "epoch 5538: loss = 0.1595\n",
      "epoch 5539: loss = 0.1277\n",
      "epoch 5540: loss = 0.0808\n",
      "epoch 5541: loss = 0.0817\n",
      "epoch 5542: loss = 0.1094\n",
      "epoch 5543: loss = 0.0812\n",
      "epoch 5544: loss = 0.1107\n",
      "epoch 5545: loss = 0.1176\n",
      "epoch 5546: loss = 0.1210\n",
      "epoch 5547: loss = 0.1090\n",
      "epoch 5548: loss = 0.0514\n",
      "epoch 5549: loss = 0.0639\n",
      "epoch 5550: loss = 0.1509\n",
      "epoch 5551: loss = 0.1264\n",
      "epoch 5552: loss = 0.1511\n",
      "epoch 5553: loss = 0.0417\n",
      "epoch 5554: loss = 0.1397\n",
      "epoch 5555: loss = 0.0547\n",
      "epoch 5556: loss = 0.1022\n",
      "epoch 5557: loss = 0.0676\n",
      "epoch 5558: loss = 0.0556\n",
      "epoch 5559: loss = 0.1075\n",
      "epoch 5560: loss = 0.0491\n",
      "epoch 5561: loss = 0.0725\n",
      "epoch 5562: loss = 0.0431\n",
      "epoch 5563: loss = 0.1011\n",
      "epoch 5564: loss = 0.1547\n",
      "epoch 5565: loss = 0.1118\n",
      "epoch 5566: loss = 0.1219\n",
      "epoch 5567: loss = 0.0351\n",
      "epoch 5568: loss = 0.1161\n",
      "epoch 5569: loss = 0.1170\n",
      "epoch 5570: loss = 0.0493\n",
      "epoch 5571: loss = 0.1274\n",
      "epoch 5572: loss = 0.0884\n",
      "epoch 5573: loss = 0.0949\n",
      "epoch 5574: loss = 0.1481\n",
      "epoch 5575: loss = 0.0704\n",
      "epoch 5576: loss = 0.0698\n",
      "epoch 5577: loss = 0.0437\n",
      "epoch 5578: loss = 0.0716\n",
      "epoch 5579: loss = 0.0418\n",
      "epoch 5580: loss = 0.1440\n",
      "epoch 5581: loss = 0.1843\n",
      "epoch 5582: loss = 0.1437\n",
      "epoch 5583: loss = 0.0889\n",
      "epoch 5584: loss = 0.0777\n",
      "epoch 5585: loss = 0.1009\n",
      "epoch 5586: loss = 0.0827\n",
      "epoch 5587: loss = 0.1693\n",
      "epoch 5588: loss = 0.1325\n",
      "epoch 5589: loss = 0.1488\n",
      "epoch 5590: loss = 0.0823\n",
      "epoch 5591: loss = 0.0323\n",
      "epoch 5592: loss = 0.0978\n",
      "epoch 5593: loss = 0.1095\n",
      "epoch 5594: loss = 0.0168\n",
      "epoch 5595: loss = 0.1049\n",
      "epoch 5596: loss = 0.1038\n",
      "epoch 5597: loss = 0.1216\n",
      "epoch 5598: loss = 0.1342\n",
      "epoch 5599: loss = 0.0932\n",
      "epoch 5600: loss = 0.0909\n",
      "epoch 5601: loss = 0.1535\n",
      "epoch 5602: loss = 0.1628\n",
      "epoch 5603: loss = 0.0890\n",
      "epoch 5604: loss = 0.1328\n",
      "epoch 5605: loss = 0.1638\n",
      "epoch 5606: loss = 0.0804\n",
      "epoch 5607: loss = 0.0963\n",
      "epoch 5608: loss = 0.1421\n",
      "epoch 5609: loss = 0.1701\n",
      "epoch 5610: loss = 0.0694\n",
      "epoch 5611: loss = 0.1090\n",
      "epoch 5612: loss = 0.1350\n",
      "epoch 5613: loss = 0.1477\n",
      "epoch 5614: loss = 0.1861\n",
      "epoch 5615: loss = 0.1498\n",
      "epoch 5616: loss = 0.0432\n",
      "epoch 5617: loss = 0.1286\n",
      "epoch 5618: loss = 0.0615\n",
      "epoch 5619: loss = 0.1043\n",
      "epoch 5620: loss = 0.0787\n",
      "epoch 5621: loss = 0.1201\n",
      "epoch 5622: loss = 0.0353\n",
      "epoch 5623: loss = 0.1202\n",
      "epoch 5624: loss = 0.1501\n",
      "epoch 5625: loss = 0.1346\n",
      "epoch 5626: loss = 0.1109\n",
      "epoch 5627: loss = 0.1365\n",
      "epoch 5628: loss = 0.0546\n",
      "epoch 5629: loss = 0.0588\n",
      "epoch 5630: loss = 0.1569\n",
      "epoch 5631: loss = 0.0724\n",
      "epoch 5632: loss = 0.1484\n",
      "epoch 5633: loss = 0.1109\n",
      "epoch 5634: loss = 0.1526\n",
      "epoch 5635: loss = 0.0427\n",
      "epoch 5636: loss = 0.1322\n",
      "epoch 5637: loss = 0.1146\n",
      "epoch 5638: loss = 0.0869\n",
      "epoch 5639: loss = 0.0485\n",
      "epoch 5640: loss = 0.1722\n",
      "epoch 5641: loss = 0.1461\n",
      "epoch 5642: loss = 0.1015\n",
      "epoch 5643: loss = 0.1584\n",
      "epoch 5644: loss = 0.1101\n",
      "epoch 5645: loss = 0.0399\n",
      "epoch 5646: loss = 0.1042\n",
      "epoch 5647: loss = 0.0998\n",
      "epoch 5648: loss = 0.0681\n",
      "epoch 5649: loss = 0.1353\n",
      "epoch 5650: loss = 0.1368\n",
      "epoch 5651: loss = 0.1896\n",
      "epoch 5652: loss = 0.1557\n",
      "epoch 5653: loss = 0.0997\n",
      "epoch 5654: loss = 0.1536\n",
      "epoch 5655: loss = 0.0945\n",
      "epoch 5656: loss = 0.0998\n",
      "epoch 5657: loss = 0.0752\n",
      "epoch 5658: loss = 0.0977\n",
      "epoch 5659: loss = 0.0809\n",
      "epoch 5660: loss = 0.0999\n",
      "epoch 5661: loss = 0.1560\n",
      "epoch 5662: loss = 0.1270\n",
      "epoch 5663: loss = 0.0702\n",
      "epoch 5664: loss = 0.1085\n",
      "epoch 5665: loss = 0.1391\n",
      "epoch 5666: loss = 0.1248\n",
      "epoch 5667: loss = 0.1653\n",
      "epoch 5668: loss = 0.0834\n",
      "epoch 5669: loss = 0.0427\n",
      "epoch 5670: loss = 0.0247\n",
      "epoch 5671: loss = 0.1262\n",
      "epoch 5672: loss = 0.0788\n",
      "epoch 5673: loss = 0.0829\n",
      "epoch 5674: loss = 0.1002\n",
      "epoch 5675: loss = 0.1173\n",
      "epoch 5676: loss = 0.1171\n",
      "epoch 5677: loss = 0.1431\n",
      "epoch 5678: loss = 0.0927\n",
      "epoch 5679: loss = 0.1365\n",
      "epoch 5680: loss = 0.1152\n",
      "epoch 5681: loss = 0.0620\n",
      "epoch 5682: loss = 0.1588\n",
      "epoch 5683: loss = 0.1456\n",
      "epoch 5684: loss = 0.1338\n",
      "epoch 5685: loss = 0.1780\n",
      "epoch 5686: loss = 0.0983\n",
      "epoch 5687: loss = 0.1050\n",
      "epoch 5688: loss = 0.0736\n",
      "epoch 5689: loss = 0.0405\n",
      "epoch 5690: loss = 0.1055\n",
      "epoch 5691: loss = 0.1605\n",
      "epoch 5692: loss = 0.1747\n",
      "epoch 5693: loss = 0.0815\n",
      "epoch 5694: loss = 0.0536\n",
      "epoch 5695: loss = 0.1379\n",
      "epoch 5696: loss = 0.1777\n",
      "epoch 5697: loss = 0.0956\n",
      "epoch 5698: loss = 0.1280\n",
      "epoch 5699: loss = 0.1068\n",
      "epoch 5700: loss = 0.1290\n",
      "epoch 5701: loss = 0.0755\n",
      "epoch 5702: loss = 0.0714\n",
      "epoch 5703: loss = 0.0763\n",
      "epoch 5704: loss = 0.1133\n",
      "epoch 5705: loss = 0.0414\n",
      "epoch 5706: loss = 0.1427\n",
      "epoch 5707: loss = 0.1105\n",
      "epoch 5708: loss = 0.1056\n",
      "epoch 5709: loss = 0.1817\n",
      "epoch 5710: loss = 0.1685\n",
      "epoch 5711: loss = 0.1014\n",
      "epoch 5712: loss = 0.1004\n",
      "epoch 5713: loss = 0.0949\n",
      "epoch 5714: loss = 0.0410\n",
      "epoch 5715: loss = 0.0990\n",
      "epoch 5716: loss = 0.1499\n",
      "epoch 5717: loss = 0.1793\n",
      "epoch 5718: loss = 0.0557\n",
      "epoch 5719: loss = 0.1351\n",
      "epoch 5720: loss = 0.0943\n",
      "epoch 5721: loss = 0.1073\n",
      "epoch 5722: loss = 0.0514\n",
      "epoch 5723: loss = 0.1161\n",
      "epoch 5724: loss = 0.1373\n",
      "epoch 5725: loss = 0.1765\n",
      "epoch 5726: loss = 0.1289\n",
      "epoch 5727: loss = 0.1928\n",
      "epoch 5728: loss = 0.1324\n",
      "epoch 5729: loss = 0.1501\n",
      "epoch 5730: loss = 0.0856\n",
      "epoch 5731: loss = 0.1089\n",
      "epoch 5732: loss = 0.0797\n",
      "epoch 5733: loss = 0.1577\n",
      "epoch 5734: loss = 0.1314\n",
      "epoch 5735: loss = 0.1009\n",
      "epoch 5736: loss = 0.1541\n",
      "epoch 5737: loss = 0.0902\n",
      "epoch 5738: loss = 0.1563\n",
      "epoch 5739: loss = 0.1661\n",
      "epoch 5740: loss = 0.1097\n",
      "epoch 5741: loss = 0.1473\n",
      "epoch 5742: loss = 0.0639\n",
      "epoch 5743: loss = 0.0460\n",
      "epoch 5744: loss = 0.0969\n",
      "epoch 5745: loss = 0.0727\n",
      "epoch 5746: loss = 0.0792\n",
      "epoch 5747: loss = 0.0891\n",
      "epoch 5748: loss = 0.0844\n",
      "epoch 5749: loss = 0.1139\n",
      "epoch 5750: loss = 0.1536\n",
      "epoch 5751: loss = 0.1107\n",
      "epoch 5752: loss = 0.1403\n",
      "epoch 5753: loss = 0.1297\n",
      "epoch 5754: loss = 0.1210\n",
      "epoch 5755: loss = 0.1145\n",
      "epoch 5756: loss = 0.1505\n",
      "epoch 5757: loss = 0.1209\n",
      "epoch 5758: loss = 0.1143\n",
      "epoch 5759: loss = 0.1179\n",
      "epoch 5760: loss = 0.0686\n",
      "epoch 5761: loss = 0.0680\n",
      "epoch 5762: loss = 0.0656\n",
      "epoch 5763: loss = 0.0938\n",
      "epoch 5764: loss = 0.0741\n",
      "epoch 5765: loss = 0.1492\n",
      "epoch 5766: loss = 0.0835\n",
      "epoch 5767: loss = 0.1545\n",
      "epoch 5768: loss = 0.1858\n",
      "epoch 5769: loss = 0.1626\n",
      "epoch 5770: loss = 0.1451\n",
      "epoch 5771: loss = 0.1031\n",
      "epoch 5772: loss = 0.1226\n",
      "epoch 5773: loss = 0.0964\n",
      "epoch 5774: loss = 0.1058\n",
      "epoch 5775: loss = 0.0260\n",
      "epoch 5776: loss = 0.1005\n",
      "epoch 5777: loss = 0.0491\n",
      "epoch 5778: loss = 0.0898\n",
      "epoch 5779: loss = 0.0352\n",
      "epoch 5780: loss = 0.0902\n",
      "epoch 5781: loss = 0.1046\n",
      "epoch 5782: loss = 0.1651\n",
      "epoch 5783: loss = 0.0333\n",
      "epoch 5784: loss = 0.1713\n",
      "epoch 5785: loss = 0.0538\n",
      "epoch 5786: loss = 0.0544\n",
      "epoch 5787: loss = 0.1001\n",
      "epoch 5788: loss = 0.1336\n",
      "epoch 5789: loss = 0.1155\n",
      "epoch 5790: loss = 0.1285\n",
      "epoch 5791: loss = 0.0871\n",
      "epoch 5792: loss = 0.0941\n",
      "epoch 5793: loss = 0.1844\n",
      "epoch 5794: loss = 0.1706\n",
      "epoch 5795: loss = 0.0924\n",
      "epoch 5796: loss = 0.1138\n",
      "epoch 5797: loss = 0.1711\n",
      "epoch 5798: loss = 0.0470\n",
      "epoch 5799: loss = 0.0880\n",
      "epoch 5800: loss = 0.0738\n",
      "epoch 5801: loss = 0.1047\n",
      "epoch 5802: loss = 0.0439\n",
      "epoch 5803: loss = 0.0918\n",
      "epoch 5804: loss = 0.1043\n",
      "epoch 5805: loss = 0.1236\n",
      "epoch 5806: loss = 0.1224\n",
      "epoch 5807: loss = 0.1096\n",
      "epoch 5808: loss = 0.1418\n",
      "epoch 5809: loss = 0.0829\n",
      "epoch 5810: loss = 0.0589\n",
      "epoch 5811: loss = 0.1176\n",
      "epoch 5812: loss = 0.0981\n",
      "epoch 5813: loss = 0.1495\n",
      "epoch 5814: loss = 0.0887\n",
      "epoch 5815: loss = 0.1273\n",
      "epoch 5816: loss = 0.1037\n",
      "epoch 5817: loss = 0.0825\n",
      "epoch 5818: loss = 0.0463\n",
      "epoch 5819: loss = 0.0862\n",
      "epoch 5820: loss = 0.0778\n",
      "epoch 5821: loss = 0.0982\n",
      "epoch 5822: loss = 0.0721\n",
      "epoch 5823: loss = 0.0430\n",
      "epoch 5824: loss = 0.0874\n",
      "epoch 5825: loss = 0.1322\n",
      "epoch 5826: loss = 0.1594\n",
      "epoch 5827: loss = 0.0890\n",
      "epoch 5828: loss = 0.1387\n",
      "epoch 5829: loss = 0.0958\n",
      "epoch 5830: loss = 0.0928\n",
      "epoch 5831: loss = 0.1686\n",
      "epoch 5832: loss = 0.1413\n",
      "epoch 5833: loss = 0.1181\n",
      "epoch 5834: loss = 0.1763\n",
      "epoch 5835: loss = 0.1504\n",
      "epoch 5836: loss = 0.1116\n",
      "epoch 5837: loss = 0.1221\n",
      "epoch 5838: loss = 0.1108\n",
      "epoch 5839: loss = 0.0435\n",
      "epoch 5840: loss = 0.1301\n",
      "epoch 5841: loss = 0.1100\n",
      "epoch 5842: loss = 0.0492\n",
      "epoch 5843: loss = 0.1267\n",
      "epoch 5844: loss = 0.0974\n",
      "epoch 5845: loss = 0.0894\n",
      "epoch 5846: loss = 0.0687\n",
      "epoch 5847: loss = 0.1810\n",
      "epoch 5848: loss = 0.1933\n",
      "epoch 5849: loss = 0.1203\n",
      "epoch 5850: loss = 0.0287\n",
      "epoch 5851: loss = 0.1028\n",
      "epoch 5852: loss = 0.1480\n",
      "epoch 5853: loss = 0.0766\n",
      "epoch 5854: loss = 0.0583\n",
      "epoch 5855: loss = 0.1090\n",
      "epoch 5856: loss = 0.0816\n",
      "epoch 5857: loss = 0.1327\n",
      "epoch 5858: loss = 0.1640\n",
      "epoch 5859: loss = 0.1909\n",
      "epoch 5860: loss = 0.0259\n",
      "epoch 5861: loss = 0.1425\n",
      "epoch 5862: loss = 0.0931\n",
      "epoch 5863: loss = 0.0948\n",
      "epoch 5864: loss = 0.0984\n",
      "epoch 5865: loss = 0.1006\n",
      "epoch 5866: loss = 0.1254\n",
      "epoch 5867: loss = 0.1448\n",
      "epoch 5868: loss = 0.1274\n",
      "epoch 5869: loss = 0.1844\n",
      "epoch 5870: loss = 0.1552\n",
      "epoch 5871: loss = 0.0787\n",
      "epoch 5872: loss = 0.1167\n",
      "epoch 5873: loss = 0.0917\n",
      "epoch 5874: loss = 0.0878\n",
      "epoch 5875: loss = 0.1294\n",
      "epoch 5876: loss = 0.1330\n",
      "epoch 5877: loss = 0.1570\n",
      "epoch 5878: loss = 0.1432\n",
      "epoch 5879: loss = 0.1139\n",
      "epoch 5880: loss = 0.1550\n",
      "epoch 5881: loss = 0.1497\n",
      "epoch 5882: loss = 0.1198\n",
      "epoch 5883: loss = 0.0473\n",
      "epoch 5884: loss = 0.0786\n",
      "epoch 5885: loss = 0.1202\n",
      "epoch 5886: loss = 0.1058\n",
      "epoch 5887: loss = 0.0835\n",
      "epoch 5888: loss = 0.0992\n",
      "epoch 5889: loss = 0.1331\n",
      "epoch 5890: loss = 0.1714\n",
      "epoch 5891: loss = 0.1904\n",
      "epoch 5892: loss = 0.1156\n",
      "epoch 5893: loss = 0.1279\n",
      "epoch 5894: loss = 0.1381\n",
      "epoch 5895: loss = 0.0801\n",
      "epoch 5896: loss = 0.0683\n",
      "epoch 5897: loss = 0.0760\n",
      "epoch 5898: loss = 0.1448\n",
      "epoch 5899: loss = 0.0631\n",
      "epoch 5900: loss = 0.1016\n",
      "epoch 5901: loss = 0.1618\n",
      "epoch 5902: loss = 0.1077\n",
      "epoch 5903: loss = 0.1404\n",
      "epoch 5904: loss = 0.1274\n",
      "epoch 5905: loss = 0.0759\n",
      "epoch 5906: loss = 0.1145\n",
      "epoch 5907: loss = 0.1334\n",
      "epoch 5908: loss = 0.1295\n",
      "epoch 5909: loss = 0.1201\n",
      "epoch 5910: loss = 0.1180\n",
      "epoch 5911: loss = 0.0868\n",
      "epoch 5912: loss = 0.0972\n",
      "epoch 5913: loss = 0.0717\n",
      "epoch 5914: loss = 0.1563\n",
      "epoch 5915: loss = 0.0791\n",
      "epoch 5916: loss = 0.1354\n",
      "epoch 5917: loss = 0.0913\n",
      "epoch 5918: loss = 0.1271\n",
      "epoch 5919: loss = 0.1023\n",
      "epoch 5920: loss = 0.1424\n",
      "epoch 5921: loss = 0.1128\n",
      "epoch 5922: loss = 0.1249\n",
      "epoch 5923: loss = 0.0472\n",
      "epoch 5924: loss = 0.0867\n",
      "epoch 5925: loss = 0.0412\n",
      "epoch 5926: loss = 0.0968\n",
      "epoch 5927: loss = 0.1569\n",
      "epoch 5928: loss = 0.1283\n",
      "epoch 5929: loss = 0.1067\n",
      "epoch 5930: loss = 0.1082\n",
      "epoch 5931: loss = 0.1081\n",
      "epoch 5932: loss = 0.0834\n",
      "epoch 5933: loss = 0.1031\n",
      "epoch 5934: loss = 0.0877\n",
      "epoch 5935: loss = 0.1647\n",
      "epoch 5936: loss = 0.1063\n",
      "epoch 5937: loss = 0.1109\n",
      "epoch 5938: loss = 0.1205\n",
      "epoch 5939: loss = 0.0697\n",
      "epoch 5940: loss = 0.0924\n",
      "epoch 5941: loss = 0.0896\n",
      "epoch 5942: loss = 0.0800\n",
      "epoch 5943: loss = 0.1662\n",
      "epoch 5944: loss = 0.0696\n",
      "epoch 5945: loss = 0.1199\n",
      "epoch 5946: loss = 0.1591\n",
      "epoch 5947: loss = 0.1604\n",
      "epoch 5948: loss = 0.1036\n",
      "epoch 5949: loss = 0.1097\n",
      "epoch 5950: loss = 0.1534\n",
      "epoch 5951: loss = 0.1183\n",
      "epoch 5952: loss = 0.1510\n",
      "epoch 5953: loss = 0.1281\n",
      "epoch 5954: loss = 0.0723\n",
      "epoch 5955: loss = 0.1465\n",
      "epoch 5956: loss = 0.1347\n",
      "epoch 5957: loss = 0.1333\n",
      "epoch 5958: loss = 0.0745\n",
      "epoch 5959: loss = 0.0065\n",
      "epoch 5960: loss = 0.0116\n",
      "epoch 5961: loss = 0.1587\n",
      "epoch 5962: loss = 0.1655\n",
      "epoch 5963: loss = 0.0488\n",
      "epoch 5964: loss = 0.0409\n",
      "epoch 5965: loss = 0.0934\n",
      "epoch 5966: loss = 0.1500\n",
      "epoch 5967: loss = 0.1112\n",
      "epoch 5968: loss = 0.1742\n",
      "epoch 5969: loss = 0.1173\n",
      "epoch 5970: loss = 0.1588\n",
      "epoch 5971: loss = 0.1335\n",
      "epoch 5972: loss = 0.0810\n",
      "epoch 5973: loss = 0.1163\n",
      "epoch 5974: loss = 0.0883\n",
      "epoch 5975: loss = 0.1149\n",
      "epoch 5976: loss = 0.0480\n",
      "epoch 5977: loss = 0.1268\n",
      "epoch 5978: loss = 0.1509\n",
      "epoch 5979: loss = 0.1755\n",
      "epoch 5980: loss = 0.1124\n",
      "epoch 5981: loss = 0.1608\n",
      "epoch 5982: loss = 0.1501\n",
      "epoch 5983: loss = 0.1402\n",
      "epoch 5984: loss = 0.1245\n",
      "epoch 5985: loss = 0.0957\n",
      "epoch 5986: loss = 0.1684\n",
      "epoch 5987: loss = 0.0491\n",
      "epoch 5988: loss = 0.0942\n",
      "epoch 5989: loss = 0.1162\n",
      "epoch 5990: loss = 0.1176\n",
      "epoch 5991: loss = 0.1394\n",
      "epoch 5992: loss = 0.0772\n",
      "epoch 5993: loss = 0.0962\n",
      "epoch 5994: loss = 0.1374\n",
      "epoch 5995: loss = 0.0571\n",
      "epoch 5996: loss = 0.0924\n",
      "epoch 5997: loss = 0.1684\n",
      "epoch 5998: loss = 0.1821\n",
      "epoch 5999: loss = 0.1330\n",
      "epoch 6000: loss = 0.0597\n",
      "epoch 6001: loss = 0.1036\n",
      "epoch 6002: loss = 0.0953\n",
      "epoch 6003: loss = 0.1374\n",
      "epoch 6004: loss = 0.0944\n",
      "epoch 6005: loss = 0.1047\n",
      "epoch 6006: loss = 0.1166\n",
      "epoch 6007: loss = 0.0953\n",
      "epoch 6008: loss = 0.0738\n",
      "epoch 6009: loss = 0.1252\n",
      "epoch 6010: loss = 0.1825\n",
      "epoch 6011: loss = 0.1079\n",
      "epoch 6012: loss = 0.0619\n",
      "epoch 6013: loss = 0.1076\n",
      "epoch 6014: loss = 0.1338\n",
      "epoch 6015: loss = 0.1539\n",
      "epoch 6016: loss = 0.0802\n",
      "epoch 6017: loss = 0.1058\n",
      "epoch 6018: loss = 0.1777\n",
      "epoch 6019: loss = 0.0402\n",
      "epoch 6020: loss = 0.1905\n",
      "epoch 6021: loss = 0.1558\n",
      "epoch 6022: loss = 0.1384\n",
      "epoch 6023: loss = 0.1553\n",
      "epoch 6024: loss = 0.0989\n",
      "epoch 6025: loss = 0.1762\n",
      "epoch 6026: loss = 0.1249\n",
      "epoch 6027: loss = 0.0641\n",
      "epoch 6028: loss = 0.0571\n",
      "epoch 6029: loss = 0.1108\n",
      "epoch 6030: loss = 0.0687\n",
      "epoch 6031: loss = 0.1112\n",
      "epoch 6032: loss = 0.0779\n",
      "epoch 6033: loss = 0.0962\n",
      "epoch 6034: loss = 0.1250\n",
      "epoch 6035: loss = 0.1774\n",
      "epoch 6036: loss = 0.1599\n",
      "epoch 6037: loss = 0.1427\n",
      "epoch 6038: loss = 0.0186\n",
      "epoch 6039: loss = 0.1569\n",
      "epoch 6040: loss = 0.0770\n",
      "epoch 6041: loss = 0.1371\n",
      "epoch 6042: loss = 0.1343\n",
      "epoch 6043: loss = 0.1107\n",
      "epoch 6044: loss = 0.0648\n",
      "epoch 6045: loss = 0.0858\n",
      "epoch 6046: loss = 0.0195\n",
      "epoch 6047: loss = 0.1291\n",
      "epoch 6048: loss = 0.1013\n",
      "epoch 6049: loss = 0.0817\n",
      "epoch 6050: loss = 0.1230\n",
      "epoch 6051: loss = 0.0599\n",
      "epoch 6052: loss = 0.1546\n",
      "epoch 6053: loss = 0.0751\n",
      "epoch 6054: loss = 0.0719\n",
      "epoch 6055: loss = 0.0931\n",
      "epoch 6056: loss = 0.1198\n",
      "epoch 6057: loss = 0.0675\n",
      "epoch 6058: loss = 0.1606\n",
      "epoch 6059: loss = 0.1619\n",
      "epoch 6060: loss = 0.1268\n",
      "epoch 6061: loss = 0.0955\n",
      "epoch 6062: loss = 0.0920\n",
      "epoch 6063: loss = 0.1422\n",
      "epoch 6064: loss = 0.0721\n",
      "epoch 6065: loss = 0.0326\n",
      "epoch 6066: loss = 0.1442\n",
      "epoch 6067: loss = 0.1580\n",
      "epoch 6068: loss = 0.1449\n",
      "epoch 6069: loss = 0.1294\n",
      "epoch 6070: loss = 0.2071\n",
      "epoch 6071: loss = 0.1493\n",
      "epoch 6072: loss = 0.1528\n",
      "epoch 6073: loss = 0.1340\n",
      "epoch 6074: loss = 0.1518\n",
      "epoch 6075: loss = 0.1233\n",
      "epoch 6076: loss = 0.1426\n",
      "epoch 6077: loss = 0.1675\n",
      "epoch 6078: loss = 0.1758\n",
      "epoch 6079: loss = 0.1545\n",
      "epoch 6080: loss = 0.1286\n",
      "epoch 6081: loss = 0.0775\n",
      "epoch 6082: loss = 0.0575\n",
      "epoch 6083: loss = 0.0843\n",
      "epoch 6084: loss = 0.0961\n",
      "epoch 6085: loss = 0.0914\n",
      "epoch 6086: loss = 0.1826\n",
      "epoch 6087: loss = 0.0783\n",
      "epoch 6088: loss = 0.0934\n",
      "epoch 6089: loss = 0.0059\n",
      "epoch 6090: loss = 0.0863\n",
      "epoch 6091: loss = 0.0877\n",
      "epoch 6092: loss = 0.0983\n",
      "epoch 6093: loss = 0.1649\n",
      "epoch 6094: loss = 0.0690\n",
      "epoch 6095: loss = 0.1597\n",
      "epoch 6096: loss = 0.0772\n",
      "epoch 6097: loss = 0.1261\n",
      "epoch 6098: loss = 0.1728\n",
      "epoch 6099: loss = 0.0668\n",
      "epoch 6100: loss = 0.1228\n",
      "epoch 6101: loss = 0.0780\n",
      "epoch 6102: loss = 0.0869\n",
      "epoch 6103: loss = 0.1757\n",
      "epoch 6104: loss = 0.0835\n",
      "epoch 6105: loss = 0.0769\n",
      "epoch 6106: loss = 0.0147\n",
      "epoch 6107: loss = 0.0447\n",
      "epoch 6108: loss = 0.0945\n",
      "epoch 6109: loss = 0.1308\n",
      "epoch 6110: loss = 0.1029\n",
      "epoch 6111: loss = 0.1364\n",
      "epoch 6112: loss = 0.0865\n",
      "epoch 6113: loss = 0.1330\n",
      "epoch 6114: loss = 0.0938\n",
      "epoch 6115: loss = 0.1253\n",
      "epoch 6116: loss = 0.0866\n",
      "epoch 6117: loss = 0.0900\n",
      "epoch 6118: loss = 0.1317\n",
      "epoch 6119: loss = 0.0927\n",
      "epoch 6120: loss = 0.1400\n",
      "epoch 6121: loss = 0.1024\n",
      "epoch 6122: loss = 0.1307\n",
      "epoch 6123: loss = 0.1507\n",
      "epoch 6124: loss = 0.0903\n",
      "epoch 6125: loss = 0.1351\n",
      "epoch 6126: loss = 0.1218\n",
      "epoch 6127: loss = 0.1186\n",
      "epoch 6128: loss = 0.1405\n",
      "epoch 6129: loss = 0.0778\n",
      "epoch 6130: loss = 0.1041\n",
      "epoch 6131: loss = 0.1022\n",
      "epoch 6132: loss = 0.1835\n",
      "epoch 6133: loss = 0.1661\n",
      "epoch 6134: loss = 0.1498\n",
      "epoch 6135: loss = 0.1324\n",
      "epoch 6136: loss = 0.0942\n",
      "epoch 6137: loss = 0.0699\n",
      "epoch 6138: loss = 0.0941\n",
      "epoch 6139: loss = 0.1164\n",
      "epoch 6140: loss = 0.0522\n",
      "epoch 6141: loss = 0.1058\n",
      "epoch 6142: loss = 0.1236\n",
      "epoch 6143: loss = 0.1377\n",
      "epoch 6144: loss = 0.0985\n",
      "epoch 6145: loss = 0.1014\n",
      "epoch 6146: loss = 0.1091\n",
      "epoch 6147: loss = 0.1141\n",
      "epoch 6148: loss = 0.1343\n",
      "epoch 6149: loss = 0.1385\n",
      "epoch 6150: loss = 0.0860\n",
      "epoch 6151: loss = 0.0981\n",
      "epoch 6152: loss = 0.0963\n",
      "epoch 6153: loss = 0.1811\n",
      "epoch 6154: loss = 0.1307\n",
      "epoch 6155: loss = 0.0627\n",
      "epoch 6156: loss = 0.0559\n",
      "epoch 6157: loss = 0.1631\n",
      "epoch 6158: loss = 0.1501\n",
      "epoch 6159: loss = 0.1357\n",
      "epoch 6160: loss = 0.1538\n",
      "epoch 6161: loss = 0.1007\n",
      "epoch 6162: loss = 0.1404\n",
      "epoch 6163: loss = 0.1147\n",
      "epoch 6164: loss = 0.0789\n",
      "epoch 6165: loss = 0.1629\n",
      "epoch 6166: loss = 0.1796\n",
      "epoch 6167: loss = 0.1302\n",
      "epoch 6168: loss = 0.1339\n",
      "epoch 6169: loss = 0.1355\n",
      "epoch 6170: loss = 0.0720\n",
      "epoch 6171: loss = 0.1667\n",
      "epoch 6172: loss = 0.1898\n",
      "epoch 6173: loss = 0.0878\n",
      "epoch 6174: loss = 0.0917\n",
      "epoch 6175: loss = 0.0869\n",
      "epoch 6176: loss = 0.1849\n",
      "epoch 6177: loss = 0.1078\n",
      "epoch 6178: loss = 0.1221\n",
      "epoch 6179: loss = 0.1185\n",
      "epoch 6180: loss = 0.1058\n",
      "epoch 6181: loss = 0.1598\n",
      "epoch 6182: loss = 0.0848\n",
      "epoch 6183: loss = 0.0592\n",
      "epoch 6184: loss = 0.1006\n",
      "epoch 6185: loss = 0.1368\n",
      "epoch 6186: loss = 0.1770\n",
      "epoch 6187: loss = 0.0425\n",
      "epoch 6188: loss = 0.1345\n",
      "epoch 6189: loss = 0.0893\n",
      "epoch 6190: loss = 0.1109\n",
      "epoch 6191: loss = 0.0981\n",
      "epoch 6192: loss = 0.0810\n",
      "epoch 6193: loss = 0.0490\n",
      "epoch 6194: loss = 0.1180\n",
      "epoch 6195: loss = 0.1784\n",
      "epoch 6196: loss = 0.1563\n",
      "epoch 6197: loss = 0.0811\n",
      "epoch 6198: loss = 0.1136\n",
      "epoch 6199: loss = 0.0483\n",
      "epoch 6200: loss = 0.0812\n",
      "epoch 6201: loss = 0.1038\n",
      "epoch 6202: loss = 0.1660\n",
      "epoch 6203: loss = 0.1772\n",
      "epoch 6204: loss = 0.0969\n",
      "epoch 6205: loss = 0.1557\n",
      "epoch 6206: loss = 0.1233\n",
      "epoch 6207: loss = 0.1831\n",
      "epoch 6208: loss = 0.1334\n",
      "epoch 6209: loss = 0.1828\n",
      "epoch 6210: loss = 0.1758\n",
      "epoch 6211: loss = 0.0665\n",
      "epoch 6212: loss = 0.0961\n",
      "epoch 6213: loss = 0.1244\n",
      "epoch 6214: loss = 0.1218\n",
      "epoch 6215: loss = 0.1133\n",
      "epoch 6216: loss = 0.1399\n",
      "epoch 6217: loss = 0.1085\n",
      "epoch 6218: loss = 0.0935\n",
      "epoch 6219: loss = 0.1230\n",
      "epoch 6220: loss = 0.1343\n",
      "epoch 6221: loss = 0.0975\n",
      "epoch 6222: loss = 0.0803\n",
      "epoch 6223: loss = 0.1264\n",
      "epoch 6224: loss = 0.0207\n",
      "epoch 6225: loss = 0.0754\n",
      "epoch 6226: loss = 0.0957\n",
      "epoch 6227: loss = 0.1003\n",
      "epoch 6228: loss = 0.1230\n",
      "epoch 6229: loss = 0.0900\n",
      "epoch 6230: loss = 0.1261\n",
      "epoch 6231: loss = 0.0857\n",
      "epoch 6232: loss = 0.0850\n",
      "epoch 6233: loss = 0.0586\n",
      "epoch 6234: loss = 0.0718\n",
      "epoch 6235: loss = 0.1418\n",
      "epoch 6236: loss = 0.0814\n",
      "epoch 6237: loss = 0.1361\n",
      "epoch 6238: loss = 0.1327\n",
      "epoch 6239: loss = 0.1116\n",
      "epoch 6240: loss = 0.1686\n",
      "epoch 6241: loss = 0.0877\n",
      "epoch 6242: loss = 0.0392\n",
      "epoch 6243: loss = 0.1051\n",
      "epoch 6244: loss = 0.1477\n",
      "epoch 6245: loss = 0.1490\n",
      "epoch 6246: loss = 0.1024\n",
      "epoch 6247: loss = 0.1484\n",
      "epoch 6248: loss = 0.1452\n",
      "epoch 6249: loss = 0.0659\n",
      "epoch 6250: loss = 0.1380\n",
      "epoch 6251: loss = 0.1061\n",
      "epoch 6252: loss = 0.1981\n",
      "epoch 6253: loss = 0.0901\n",
      "epoch 6254: loss = 0.0316\n",
      "epoch 6255: loss = 0.1745\n",
      "epoch 6256: loss = 0.1340\n",
      "epoch 6257: loss = 0.1179\n",
      "epoch 6258: loss = 0.1103\n",
      "epoch 6259: loss = 0.1613\n",
      "epoch 6260: loss = 0.1000\n",
      "epoch 6261: loss = 0.1785\n",
      "epoch 6262: loss = 0.1961\n",
      "epoch 6263: loss = 0.1421\n",
      "epoch 6264: loss = 0.1333\n",
      "epoch 6265: loss = 0.1818\n",
      "epoch 6266: loss = 0.1086\n",
      "epoch 6267: loss = 0.0887\n",
      "epoch 6268: loss = 0.1384\n",
      "epoch 6269: loss = 0.1378\n",
      "epoch 6270: loss = 0.0958\n",
      "epoch 6271: loss = 0.0767\n",
      "epoch 6272: loss = 0.1440\n",
      "epoch 6273: loss = 0.1364\n",
      "epoch 6274: loss = 0.1048\n",
      "epoch 6275: loss = 0.1143\n",
      "epoch 6276: loss = 0.1035\n",
      "epoch 6277: loss = 0.0559\n",
      "epoch 6278: loss = 0.0537\n",
      "epoch 6279: loss = 0.0947\n",
      "epoch 6280: loss = 0.0987\n",
      "epoch 6281: loss = 0.0389\n",
      "epoch 6282: loss = 0.0441\n",
      "epoch 6283: loss = 0.1664\n",
      "epoch 6284: loss = 0.2047\n",
      "epoch 6285: loss = 0.1942\n",
      "epoch 6286: loss = 0.1105\n",
      "epoch 6287: loss = 0.0803\n",
      "epoch 6288: loss = 0.1258\n",
      "epoch 6289: loss = 0.1704\n",
      "epoch 6290: loss = 0.0918\n",
      "epoch 6291: loss = 0.0878\n",
      "epoch 6292: loss = 0.1173\n",
      "epoch 6293: loss = 0.1546\n",
      "epoch 6294: loss = 0.0705\n",
      "epoch 6295: loss = 0.1013\n",
      "epoch 6296: loss = 0.1273\n",
      "epoch 6297: loss = 0.1031\n",
      "epoch 6298: loss = 0.1002\n",
      "epoch 6299: loss = 0.1048\n",
      "epoch 6300: loss = 0.0381\n",
      "epoch 6301: loss = 0.1192\n",
      "epoch 6302: loss = 0.0953\n",
      "epoch 6303: loss = 0.1356\n",
      "epoch 6304: loss = 0.1069\n",
      "epoch 6305: loss = 0.0462\n",
      "epoch 6306: loss = 0.0985\n",
      "epoch 6307: loss = 0.1625\n",
      "epoch 6308: loss = 0.0268\n",
      "epoch 6309: loss = 0.0727\n",
      "epoch 6310: loss = 0.1555\n",
      "epoch 6311: loss = 0.1061\n",
      "epoch 6312: loss = 0.1010\n",
      "epoch 6313: loss = 0.0797\n",
      "epoch 6314: loss = 0.0961\n",
      "epoch 6315: loss = 0.1153\n",
      "epoch 6316: loss = 0.1048\n",
      "epoch 6317: loss = 0.1203\n",
      "epoch 6318: loss = 0.1517\n",
      "epoch 6319: loss = 0.0793\n",
      "epoch 6320: loss = 0.0567\n",
      "epoch 6321: loss = 0.1292\n",
      "epoch 6322: loss = 0.1030\n",
      "epoch 6323: loss = 0.1051\n",
      "epoch 6324: loss = 0.0690\n",
      "epoch 6325: loss = 0.1187\n",
      "epoch 6326: loss = 0.0143\n",
      "epoch 6327: loss = 0.1671\n",
      "epoch 6328: loss = 0.0802\n",
      "epoch 6329: loss = 0.0756\n",
      "epoch 6330: loss = 0.0871\n",
      "epoch 6331: loss = 0.0831\n",
      "epoch 6332: loss = 0.0854\n",
      "epoch 6333: loss = 0.1008\n",
      "epoch 6334: loss = 0.1075\n",
      "epoch 6335: loss = 0.1576\n",
      "epoch 6336: loss = 0.1591\n",
      "epoch 6337: loss = 0.0354\n",
      "epoch 6338: loss = 0.1006\n",
      "epoch 6339: loss = 0.0673\n",
      "epoch 6340: loss = 0.1160\n",
      "epoch 6341: loss = 0.1310\n",
      "epoch 6342: loss = 0.1290\n",
      "epoch 6343: loss = 0.0745\n",
      "epoch 6344: loss = 0.1016\n",
      "epoch 6345: loss = 0.0404\n",
      "epoch 6346: loss = 0.1424\n",
      "epoch 6347: loss = 0.1404\n",
      "epoch 6348: loss = 0.1070\n",
      "epoch 6349: loss = 0.1503\n",
      "epoch 6350: loss = 0.0552\n",
      "epoch 6351: loss = 0.1304\n",
      "epoch 6352: loss = 0.0757\n",
      "epoch 6353: loss = 0.0969\n",
      "epoch 6354: loss = 0.0962\n",
      "epoch 6355: loss = 0.0840\n",
      "epoch 6356: loss = 0.0261\n",
      "epoch 6357: loss = 0.0643\n",
      "epoch 6358: loss = 0.1406\n",
      "epoch 6359: loss = 0.2002\n",
      "epoch 6360: loss = 0.0515\n",
      "epoch 6361: loss = 0.1699\n",
      "epoch 6362: loss = 0.0957\n",
      "epoch 6363: loss = 0.1174\n",
      "epoch 6364: loss = 0.1100\n",
      "epoch 6365: loss = 0.1273\n",
      "epoch 6366: loss = 0.1658\n",
      "epoch 6367: loss = 0.0581\n",
      "epoch 6368: loss = 0.0849\n",
      "epoch 6369: loss = 0.1316\n",
      "epoch 6370: loss = 0.1648\n",
      "epoch 6371: loss = 0.0729\n",
      "epoch 6372: loss = 0.1413\n",
      "epoch 6373: loss = 0.1466\n",
      "epoch 6374: loss = 0.1801\n",
      "epoch 6375: loss = 0.1556\n",
      "epoch 6376: loss = 0.0865\n",
      "epoch 6377: loss = 0.1785\n",
      "epoch 6378: loss = 0.0946\n",
      "epoch 6379: loss = 0.1314\n",
      "epoch 6380: loss = 0.0856\n",
      "epoch 6381: loss = 0.1594\n",
      "epoch 6382: loss = 0.1480\n",
      "epoch 6383: loss = 0.0510\n",
      "epoch 6384: loss = 0.0655\n",
      "epoch 6385: loss = 0.1061\n",
      "epoch 6386: loss = 0.0458\n",
      "epoch 6387: loss = 0.1209\n",
      "epoch 6388: loss = 0.1197\n",
      "epoch 6389: loss = 0.1508\n",
      "epoch 6390: loss = 0.1809\n",
      "epoch 6391: loss = 0.1287\n",
      "epoch 6392: loss = 0.0909\n",
      "epoch 6393: loss = 0.1041\n",
      "epoch 6394: loss = 0.1126\n",
      "epoch 6395: loss = 0.1646\n",
      "epoch 6396: loss = 0.1483\n",
      "epoch 6397: loss = 0.1825\n",
      "epoch 6398: loss = 0.1653\n",
      "epoch 6399: loss = 0.1341\n",
      "epoch 6400: loss = 0.1081\n",
      "epoch 6401: loss = 0.1038\n",
      "epoch 6402: loss = 0.1365\n",
      "epoch 6403: loss = 0.1013\n",
      "epoch 6404: loss = 0.1061\n",
      "epoch 6405: loss = 0.1341\n",
      "epoch 6406: loss = 0.1059\n",
      "epoch 6407: loss = 0.1531\n",
      "epoch 6408: loss = 0.1215\n",
      "epoch 6409: loss = 0.1139\n",
      "epoch 6410: loss = 0.0960\n",
      "epoch 6411: loss = 0.0866\n",
      "epoch 6412: loss = 0.1331\n",
      "epoch 6413: loss = 0.1135\n",
      "epoch 6414: loss = 0.1111\n",
      "epoch 6415: loss = 0.0692\n",
      "epoch 6416: loss = 0.0969\n",
      "epoch 6417: loss = 0.0952\n",
      "epoch 6418: loss = 0.0380\n",
      "epoch 6419: loss = 0.1510\n",
      "epoch 6420: loss = 0.0967\n",
      "epoch 6421: loss = 0.1014\n",
      "epoch 6422: loss = 0.1625\n",
      "epoch 6423: loss = 0.0822\n",
      "epoch 6424: loss = 0.0827\n",
      "epoch 6425: loss = 0.0759\n",
      "epoch 6426: loss = 0.0891\n",
      "epoch 6427: loss = 0.0414\n",
      "epoch 6428: loss = 0.1710\n",
      "epoch 6429: loss = 0.1629\n",
      "epoch 6430: loss = 0.1275\n",
      "epoch 6431: loss = 0.1317\n",
      "epoch 6432: loss = 0.1975\n",
      "epoch 6433: loss = 0.1483\n",
      "epoch 6434: loss = 0.1301\n",
      "epoch 6435: loss = 0.0518\n",
      "epoch 6436: loss = 0.1122\n",
      "epoch 6437: loss = 0.0686\n",
      "epoch 6438: loss = 0.1664\n",
      "epoch 6439: loss = 0.1705\n",
      "epoch 6440: loss = 0.1226\n",
      "epoch 6441: loss = 0.1067\n",
      "epoch 6442: loss = 0.1390\n",
      "epoch 6443: loss = 0.1126\n",
      "epoch 6444: loss = 0.0774\n",
      "epoch 6445: loss = 0.1029\n",
      "epoch 6446: loss = 0.0863\n",
      "epoch 6447: loss = 0.1500\n",
      "epoch 6448: loss = 0.1141\n",
      "epoch 6449: loss = 0.2033\n",
      "epoch 6450: loss = 0.1000\n",
      "epoch 6451: loss = 0.1121\n",
      "epoch 6452: loss = 0.0609\n",
      "epoch 6453: loss = 0.1111\n",
      "epoch 6454: loss = 0.1119\n",
      "epoch 6455: loss = 0.0927\n",
      "epoch 6456: loss = 0.1273\n",
      "epoch 6457: loss = 0.1853\n",
      "epoch 6458: loss = 0.1457\n",
      "epoch 6459: loss = 0.1303\n",
      "epoch 6460: loss = 0.0963\n",
      "epoch 6461: loss = 0.0487\n",
      "epoch 6462: loss = 0.0261\n",
      "epoch 6463: loss = 0.1065\n",
      "epoch 6464: loss = 0.0817\n",
      "epoch 6465: loss = 0.0649\n",
      "epoch 6466: loss = 0.1256\n",
      "epoch 6467: loss = 0.1220\n",
      "epoch 6468: loss = 0.1549\n",
      "epoch 6469: loss = 0.0681\n",
      "epoch 6470: loss = 0.0961\n",
      "epoch 6471: loss = 0.0869\n",
      "epoch 6472: loss = 0.0312\n",
      "epoch 6473: loss = 0.1487\n",
      "epoch 6474: loss = 0.1530\n",
      "epoch 6475: loss = 0.1261\n",
      "epoch 6476: loss = 0.0967\n",
      "epoch 6477: loss = 0.0550\n",
      "epoch 6478: loss = 0.1317\n",
      "epoch 6479: loss = 0.0604\n",
      "epoch 6480: loss = 0.1095\n",
      "epoch 6481: loss = 0.1088\n",
      "epoch 6482: loss = 0.1689\n",
      "epoch 6483: loss = 0.1207\n",
      "epoch 6484: loss = 0.1295\n",
      "epoch 6485: loss = 0.1331\n",
      "epoch 6486: loss = 0.0412\n",
      "epoch 6487: loss = 0.0280\n",
      "epoch 6488: loss = 0.0946\n",
      "epoch 6489: loss = 0.1842\n",
      "epoch 6490: loss = 0.1447\n",
      "epoch 6491: loss = 0.1427\n",
      "epoch 6492: loss = 0.1598\n",
      "epoch 6493: loss = 0.0838\n",
      "epoch 6494: loss = 0.1041\n",
      "epoch 6495: loss = 0.1079\n",
      "epoch 6496: loss = 0.1655\n",
      "epoch 6497: loss = 0.0795\n",
      "epoch 6498: loss = 0.0820\n",
      "epoch 6499: loss = 0.1031\n",
      "epoch 6500: loss = 0.0992\n",
      "epoch 6501: loss = 0.1041\n",
      "epoch 6502: loss = 0.1194\n",
      "epoch 6503: loss = 0.0954\n",
      "epoch 6504: loss = 0.1082\n",
      "epoch 6505: loss = 0.1878\n",
      "epoch 6506: loss = 0.1335\n",
      "epoch 6507: loss = 0.1613\n",
      "epoch 6508: loss = 0.1205\n",
      "epoch 6509: loss = 0.1119\n",
      "epoch 6510: loss = 0.1503\n",
      "epoch 6511: loss = 0.0811\n",
      "epoch 6512: loss = 0.1367\n",
      "epoch 6513: loss = 0.1622\n",
      "epoch 6514: loss = 0.0983\n",
      "epoch 6515: loss = 0.1343\n",
      "epoch 6516: loss = 0.0780\n",
      "epoch 6517: loss = 0.1693\n",
      "epoch 6518: loss = 0.1171\n",
      "epoch 6519: loss = 0.0999\n",
      "epoch 6520: loss = 0.0861\n",
      "epoch 6521: loss = 0.0559\n",
      "epoch 6522: loss = 0.0852\n",
      "epoch 6523: loss = 0.1195\n",
      "epoch 6524: loss = 0.1433\n",
      "epoch 6525: loss = 0.1341\n",
      "epoch 6526: loss = 0.1778\n",
      "epoch 6527: loss = 0.0514\n",
      "epoch 6528: loss = 0.0561\n",
      "epoch 6529: loss = 0.1735\n",
      "epoch 6530: loss = 0.0518\n",
      "epoch 6531: loss = 0.1210\n",
      "epoch 6532: loss = 0.1598\n",
      "epoch 6533: loss = 0.1404\n",
      "epoch 6534: loss = 0.0794\n",
      "epoch 6535: loss = 0.0579\n",
      "epoch 6536: loss = 0.1217\n",
      "epoch 6537: loss = 0.0868\n",
      "epoch 6538: loss = 0.1657\n",
      "epoch 6539: loss = 0.1423\n",
      "epoch 6540: loss = 0.0802\n",
      "epoch 6541: loss = 0.1016\n",
      "epoch 6542: loss = 0.0747\n",
      "epoch 6543: loss = 0.1472\n",
      "epoch 6544: loss = 0.1079\n",
      "epoch 6545: loss = 0.1849\n",
      "epoch 6546: loss = 0.0446\n",
      "epoch 6547: loss = 0.1205\n",
      "epoch 6548: loss = 0.1223\n",
      "epoch 6549: loss = 0.1534\n",
      "epoch 6550: loss = 0.1712\n",
      "epoch 6551: loss = 0.1677\n",
      "epoch 6552: loss = 0.0884\n",
      "epoch 6553: loss = 0.1410\n",
      "epoch 6554: loss = 0.1635\n",
      "epoch 6555: loss = 0.1351\n",
      "epoch 6556: loss = 0.0563\n",
      "epoch 6557: loss = 0.0819\n",
      "epoch 6558: loss = 0.0535\n",
      "epoch 6559: loss = 0.0425\n",
      "epoch 6560: loss = 0.1189\n",
      "epoch 6561: loss = 0.1865\n",
      "epoch 6562: loss = 0.1667\n",
      "epoch 6563: loss = 0.1213\n",
      "epoch 6564: loss = 0.0530\n",
      "epoch 6565: loss = 0.0540\n",
      "epoch 6566: loss = 0.0577\n",
      "epoch 6567: loss = 0.1134\n",
      "epoch 6568: loss = 0.1135\n",
      "epoch 6569: loss = 0.0284\n",
      "epoch 6570: loss = 0.1018\n",
      "epoch 6571: loss = 0.1226\n",
      "epoch 6572: loss = 0.0931\n",
      "epoch 6573: loss = 0.1171\n",
      "epoch 6574: loss = 0.0770\n",
      "epoch 6575: loss = 0.1176\n",
      "epoch 6576: loss = 0.1138\n",
      "epoch 6577: loss = 0.1452\n",
      "epoch 6578: loss = 0.1544\n",
      "epoch 6579: loss = 0.1413\n",
      "epoch 6580: loss = 0.1518\n",
      "epoch 6581: loss = 0.0596\n",
      "epoch 6582: loss = 0.1025\n",
      "epoch 6583: loss = 0.0357\n",
      "epoch 6584: loss = 0.1244\n",
      "epoch 6585: loss = 0.0885\n",
      "epoch 6586: loss = 0.1536\n",
      "epoch 6587: loss = 0.0940\n",
      "epoch 6588: loss = 0.1871\n",
      "epoch 6589: loss = 0.1497\n",
      "epoch 6590: loss = 0.1549\n",
      "epoch 6591: loss = 0.1238\n",
      "epoch 6592: loss = 0.1962\n",
      "epoch 6593: loss = 0.1997\n",
      "epoch 6594: loss = 0.0309\n",
      "epoch 6595: loss = 0.1237\n",
      "epoch 6596: loss = 0.1025\n",
      "epoch 6597: loss = 0.1032\n",
      "epoch 6598: loss = 0.0857\n",
      "epoch 6599: loss = 0.1768\n",
      "epoch 6600: loss = 0.1261\n",
      "epoch 6601: loss = 0.1293\n",
      "epoch 6602: loss = 0.0541\n",
      "epoch 6603: loss = 0.1512\n",
      "epoch 6604: loss = 0.1199\n",
      "epoch 6605: loss = 0.0993\n",
      "epoch 6606: loss = 0.0925\n",
      "epoch 6607: loss = 0.1589\n",
      "epoch 6608: loss = 0.0682\n",
      "epoch 6609: loss = 0.1192\n",
      "epoch 6610: loss = 0.1239\n",
      "epoch 6611: loss = 0.1082\n",
      "epoch 6612: loss = 0.1540\n",
      "epoch 6613: loss = 0.1146\n",
      "epoch 6614: loss = 0.0894\n",
      "epoch 6615: loss = 0.1486\n",
      "epoch 6616: loss = 0.1178\n",
      "epoch 6617: loss = 0.0937\n",
      "epoch 6618: loss = 0.0522\n",
      "epoch 6619: loss = 0.0996\n",
      "epoch 6620: loss = 0.1686\n",
      "epoch 6621: loss = 0.1856\n",
      "epoch 6622: loss = 0.0470\n",
      "epoch 6623: loss = 0.0109\n",
      "epoch 6624: loss = 0.0677\n",
      "epoch 6625: loss = 0.0156\n",
      "epoch 6626: loss = 0.0877\n",
      "epoch 6627: loss = 0.1181\n",
      "epoch 6628: loss = 0.1666\n",
      "epoch 6629: loss = 0.1370\n",
      "epoch 6630: loss = 0.0680\n",
      "epoch 6631: loss = 0.0818\n",
      "epoch 6632: loss = 0.0563\n",
      "epoch 6633: loss = 0.1013\n",
      "epoch 6634: loss = 0.1343\n",
      "epoch 6635: loss = 0.1314\n",
      "epoch 6636: loss = 0.1596\n",
      "epoch 6637: loss = 0.0698\n",
      "epoch 6638: loss = 0.1111\n",
      "epoch 6639: loss = 0.0577\n",
      "epoch 6640: loss = 0.0794\n",
      "epoch 6641: loss = 0.1296\n",
      "epoch 6642: loss = 0.0997\n",
      "epoch 6643: loss = 0.1398\n",
      "epoch 6644: loss = 0.1404\n",
      "epoch 6645: loss = 0.0792\n",
      "epoch 6646: loss = 0.0472\n",
      "epoch 6647: loss = 0.1299\n",
      "epoch 6648: loss = 0.1115\n",
      "epoch 6649: loss = 0.1083\n",
      "epoch 6650: loss = 0.1003\n",
      "epoch 6651: loss = 0.0856\n",
      "epoch 6652: loss = 0.0428\n",
      "epoch 6653: loss = 0.0184\n",
      "epoch 6654: loss = 0.1084\n",
      "epoch 6655: loss = 0.0536\n",
      "epoch 6656: loss = 0.1318\n",
      "epoch 6657: loss = 0.0881\n",
      "epoch 6658: loss = 0.1048\n",
      "epoch 6659: loss = 0.1249\n",
      "epoch 6660: loss = 0.1261\n",
      "epoch 6661: loss = 0.0544\n",
      "epoch 6662: loss = 0.0439\n",
      "epoch 6663: loss = 0.0750\n",
      "epoch 6664: loss = 0.1718\n",
      "epoch 6665: loss = 0.1462\n",
      "epoch 6666: loss = 0.1632\n",
      "epoch 6667: loss = 0.1706\n",
      "epoch 6668: loss = 0.1004\n",
      "epoch 6669: loss = 0.1373\n",
      "epoch 6670: loss = 0.1305\n",
      "epoch 6671: loss = 0.0569\n",
      "epoch 6672: loss = 0.0831\n",
      "epoch 6673: loss = 0.1779\n",
      "epoch 6674: loss = 0.1550\n",
      "epoch 6675: loss = 0.1555\n",
      "epoch 6676: loss = 0.1187\n",
      "epoch 6677: loss = 0.1644\n",
      "epoch 6678: loss = 0.1170\n",
      "epoch 6679: loss = 0.0855\n",
      "epoch 6680: loss = 0.1319\n",
      "epoch 6681: loss = 0.1341\n",
      "epoch 6682: loss = 0.0877\n",
      "epoch 6683: loss = 0.1516\n",
      "epoch 6684: loss = 0.1144\n",
      "epoch 6685: loss = 0.1337\n",
      "epoch 6686: loss = 0.0850\n",
      "epoch 6687: loss = 0.0607\n",
      "epoch 6688: loss = 0.0760\n",
      "epoch 6689: loss = 0.1039\n",
      "epoch 6690: loss = 0.0785\n",
      "epoch 6691: loss = 0.1038\n",
      "epoch 6692: loss = 0.1472\n",
      "epoch 6693: loss = 0.0994\n",
      "epoch 6694: loss = 0.0870\n",
      "epoch 6695: loss = 0.0645\n",
      "epoch 6696: loss = 0.1737\n",
      "epoch 6697: loss = 0.1530\n",
      "epoch 6698: loss = 0.1537\n",
      "epoch 6699: loss = 0.1057\n",
      "epoch 6700: loss = 0.1819\n",
      "epoch 6701: loss = 0.1434\n",
      "epoch 6702: loss = 0.1401\n",
      "epoch 6703: loss = 0.0999\n",
      "epoch 6704: loss = 0.1239\n",
      "epoch 6705: loss = 0.1505\n",
      "epoch 6706: loss = 0.1763\n",
      "epoch 6707: loss = 0.1976\n",
      "epoch 6708: loss = 0.1444\n",
      "epoch 6709: loss = 0.0305\n",
      "epoch 6710: loss = 0.0839\n",
      "epoch 6711: loss = 0.1155\n",
      "epoch 6712: loss = 0.1246\n",
      "epoch 6713: loss = 0.1770\n",
      "epoch 6714: loss = 0.1327\n",
      "epoch 6715: loss = 0.1481\n",
      "epoch 6716: loss = 0.0374\n",
      "epoch 6717: loss = 0.0862\n",
      "epoch 6718: loss = 0.1178\n",
      "epoch 6719: loss = 0.1320\n",
      "epoch 6720: loss = 0.0940\n",
      "epoch 6721: loss = 0.1440\n",
      "epoch 6722: loss = 0.1439\n",
      "epoch 6723: loss = 0.1346\n",
      "epoch 6724: loss = 0.1223\n",
      "epoch 6725: loss = 0.0844\n",
      "epoch 6726: loss = 0.0295\n",
      "epoch 6727: loss = 0.1475\n",
      "epoch 6728: loss = 0.0797\n",
      "epoch 6729: loss = 0.0408\n",
      "epoch 6730: loss = 0.1393\n",
      "epoch 6731: loss = 0.1390\n",
      "epoch 6732: loss = 0.0842\n",
      "epoch 6733: loss = 0.0933\n",
      "epoch 6734: loss = 0.0834\n",
      "epoch 6735: loss = 0.1535\n",
      "epoch 6736: loss = 0.1522\n",
      "epoch 6737: loss = 0.1390\n",
      "epoch 6738: loss = 0.1814\n",
      "epoch 6739: loss = 0.0642\n",
      "epoch 6740: loss = 0.1344\n",
      "epoch 6741: loss = 0.1227\n",
      "epoch 6742: loss = 0.1595\n",
      "epoch 6743: loss = 0.1347\n",
      "epoch 6744: loss = 0.0886\n",
      "epoch 6745: loss = 0.1195\n",
      "epoch 6746: loss = 0.1263\n",
      "epoch 6747: loss = 0.1435\n",
      "epoch 6748: loss = 0.1491\n",
      "epoch 6749: loss = 0.1208\n",
      "epoch 6750: loss = 0.0634\n",
      "epoch 6751: loss = 0.0740\n",
      "epoch 6752: loss = 0.0405\n",
      "epoch 6753: loss = 0.1314\n",
      "epoch 6754: loss = 0.0645\n",
      "epoch 6755: loss = 0.1336\n",
      "epoch 6756: loss = 0.0849\n",
      "epoch 6757: loss = 0.0844\n",
      "epoch 6758: loss = 0.1109\n",
      "epoch 6759: loss = 0.0954\n",
      "epoch 6760: loss = 0.0613\n",
      "epoch 6761: loss = 0.1138\n",
      "epoch 6762: loss = 0.1213\n",
      "epoch 6763: loss = 0.1946\n",
      "epoch 6764: loss = 0.1751\n",
      "epoch 6765: loss = 0.0723\n",
      "epoch 6766: loss = 0.1387\n",
      "epoch 6767: loss = 0.0989\n",
      "epoch 6768: loss = 0.0930\n",
      "epoch 6769: loss = 0.1548\n",
      "epoch 6770: loss = 0.1185\n",
      "epoch 6771: loss = 0.1089\n",
      "epoch 6772: loss = 0.0273\n",
      "epoch 6773: loss = 0.0698\n",
      "epoch 6774: loss = 0.0736\n",
      "epoch 6775: loss = 0.0971\n",
      "epoch 6776: loss = 0.1235\n",
      "epoch 6777: loss = 0.1077\n",
      "epoch 6778: loss = 0.0834\n",
      "epoch 6779: loss = 0.0170\n",
      "epoch 6780: loss = 0.1007\n",
      "epoch 6781: loss = 0.1084\n",
      "epoch 6782: loss = 0.1226\n",
      "epoch 6783: loss = 0.1411\n",
      "epoch 6784: loss = 0.1505\n",
      "epoch 6785: loss = 0.0732\n",
      "epoch 6786: loss = 0.0788\n",
      "epoch 6787: loss = 0.0571\n",
      "epoch 6788: loss = 0.1228\n",
      "epoch 6789: loss = 0.1424\n",
      "epoch 6790: loss = 0.1610\n",
      "epoch 6791: loss = 0.1818\n",
      "epoch 6792: loss = 0.0854\n",
      "epoch 6793: loss = 0.0432\n",
      "epoch 6794: loss = 0.1153\n",
      "epoch 6795: loss = 0.1273\n",
      "epoch 6796: loss = 0.0975\n",
      "epoch 6797: loss = 0.1493\n",
      "epoch 6798: loss = 0.1772\n",
      "epoch 6799: loss = 0.0766\n",
      "epoch 6800: loss = 0.1322\n",
      "epoch 6801: loss = 0.1200\n",
      "epoch 6802: loss = 0.1615\n",
      "epoch 6803: loss = 0.0571\n",
      "epoch 6804: loss = 0.0029\n",
      "epoch 6805: loss = 0.0514\n",
      "epoch 6806: loss = 0.1542\n",
      "epoch 6807: loss = 0.1018\n",
      "epoch 6808: loss = 0.0812\n",
      "epoch 6809: loss = 0.0880\n",
      "epoch 6810: loss = 0.0465\n",
      "epoch 6811: loss = 0.0178\n",
      "epoch 6812: loss = 0.1729\n",
      "epoch 6813: loss = 0.0979\n",
      "epoch 6814: loss = 0.1629\n",
      "epoch 6815: loss = 0.0865\n",
      "epoch 6816: loss = 0.1289\n",
      "epoch 6817: loss = 0.1121\n",
      "epoch 6818: loss = 0.1571\n",
      "epoch 6819: loss = 0.1025\n",
      "epoch 6820: loss = 0.0948\n",
      "epoch 6821: loss = 0.1853\n",
      "epoch 6822: loss = 0.1256\n",
      "epoch 6823: loss = 0.1375\n",
      "epoch 6824: loss = 0.1759\n",
      "epoch 6825: loss = 0.1579\n",
      "epoch 6826: loss = 0.0740\n",
      "epoch 6827: loss = 0.0061\n",
      "epoch 6828: loss = 0.1557\n",
      "epoch 6829: loss = 0.0531\n",
      "epoch 6830: loss = 0.1392\n",
      "epoch 6831: loss = 0.1370\n",
      "epoch 6832: loss = 0.1008\n",
      "epoch 6833: loss = 0.1385\n",
      "epoch 6834: loss = 0.1003\n",
      "epoch 6835: loss = 0.1464\n",
      "epoch 6836: loss = 0.1778\n",
      "epoch 6837: loss = 0.0864\n",
      "epoch 6838: loss = 0.1395\n",
      "epoch 6839: loss = 0.1663\n",
      "epoch 6840: loss = 0.1400\n",
      "epoch 6841: loss = 0.1640\n",
      "epoch 6842: loss = 0.0360\n",
      "epoch 6843: loss = 0.0574\n",
      "epoch 6844: loss = 0.1240\n",
      "epoch 6845: loss = 0.1472\n",
      "epoch 6846: loss = 0.1765\n",
      "epoch 6847: loss = 0.1097\n",
      "epoch 6848: loss = 0.1460\n",
      "epoch 6849: loss = 0.1521\n",
      "epoch 6850: loss = 0.1727\n",
      "epoch 6851: loss = 0.1035\n",
      "epoch 6852: loss = 0.0546\n",
      "epoch 6853: loss = 0.1171\n",
      "epoch 6854: loss = 0.1278\n",
      "epoch 6855: loss = 0.1048\n",
      "epoch 6856: loss = 0.0329\n",
      "epoch 6857: loss = 0.0592\n",
      "epoch 6858: loss = 0.0895\n",
      "epoch 6859: loss = 0.0463\n",
      "epoch 6860: loss = 0.0342\n",
      "epoch 6861: loss = 0.0689\n",
      "epoch 6862: loss = 0.1072\n",
      "epoch 6863: loss = 0.1180\n",
      "epoch 6864: loss = 0.1027\n",
      "epoch 6865: loss = 0.0659\n",
      "epoch 6866: loss = 0.1387\n",
      "epoch 6867: loss = 0.1174\n",
      "epoch 6868: loss = 0.1524\n",
      "epoch 6869: loss = 0.1382\n",
      "epoch 6870: loss = 0.1228\n",
      "epoch 6871: loss = 0.0472\n",
      "epoch 6872: loss = 0.1392\n",
      "epoch 6873: loss = 0.0763\n",
      "epoch 6874: loss = 0.1184\n",
      "epoch 6875: loss = 0.1713\n",
      "epoch 6876: loss = 0.0665\n",
      "epoch 6877: loss = 0.1212\n",
      "epoch 6878: loss = 0.1131\n",
      "epoch 6879: loss = 0.0317\n",
      "epoch 6880: loss = 0.1423\n",
      "epoch 6881: loss = 0.0666\n",
      "epoch 6882: loss = 0.1158\n",
      "epoch 6883: loss = 0.1032\n",
      "epoch 6884: loss = 0.0783\n",
      "epoch 6885: loss = 0.1478\n",
      "epoch 6886: loss = 0.0587\n",
      "epoch 6887: loss = 0.1378\n",
      "epoch 6888: loss = 0.0891\n",
      "epoch 6889: loss = 0.1378\n",
      "epoch 6890: loss = 0.0919\n",
      "epoch 6891: loss = 0.1601\n",
      "epoch 6892: loss = 0.0865\n",
      "epoch 6893: loss = 0.0344\n",
      "epoch 6894: loss = 0.1228\n",
      "epoch 6895: loss = 0.1733\n",
      "epoch 6896: loss = 0.1002\n",
      "epoch 6897: loss = 0.1108\n",
      "epoch 6898: loss = 0.1720\n",
      "epoch 6899: loss = 0.1879\n",
      "epoch 6900: loss = 0.1052\n",
      "epoch 6901: loss = 0.1296\n",
      "epoch 6902: loss = 0.1056\n",
      "epoch 6903: loss = 0.1555\n",
      "epoch 6904: loss = 0.1052\n",
      "epoch 6905: loss = 0.0358\n",
      "epoch 6906: loss = 0.1130\n",
      "epoch 6907: loss = 0.0450\n",
      "epoch 6908: loss = 0.0713\n",
      "epoch 6909: loss = 0.1234\n",
      "epoch 6910: loss = 0.1210\n",
      "epoch 6911: loss = 0.1342\n",
      "epoch 6912: loss = 0.1572\n",
      "epoch 6913: loss = 0.1157\n",
      "epoch 6914: loss = 0.1824\n",
      "epoch 6915: loss = 0.1916\n",
      "epoch 6916: loss = 0.1681\n",
      "epoch 6917: loss = 0.1930\n",
      "epoch 6918: loss = 0.0808\n",
      "epoch 6919: loss = 0.1196\n",
      "epoch 6920: loss = 0.0531\n",
      "epoch 6921: loss = 0.0835\n",
      "epoch 6922: loss = 0.0522\n",
      "epoch 6923: loss = 0.0627\n",
      "epoch 6924: loss = 0.0987\n",
      "epoch 6925: loss = 0.1331\n",
      "epoch 6926: loss = 0.0993\n",
      "epoch 6927: loss = 0.1781\n",
      "epoch 6928: loss = 0.1877\n",
      "epoch 6929: loss = 0.1374\n",
      "epoch 6930: loss = 0.1169\n",
      "epoch 6931: loss = 0.0826\n",
      "epoch 6932: loss = 0.0490\n",
      "epoch 6933: loss = 0.0410\n",
      "epoch 6934: loss = 0.0856\n",
      "epoch 6935: loss = 0.1531\n",
      "epoch 6936: loss = 0.1022\n",
      "epoch 6937: loss = 0.1111\n",
      "epoch 6938: loss = 0.1820\n",
      "epoch 6939: loss = 0.1309\n",
      "epoch 6940: loss = 0.1645\n",
      "epoch 6941: loss = 0.1221\n",
      "epoch 6942: loss = 0.1141\n",
      "epoch 6943: loss = 0.1242\n",
      "epoch 6944: loss = 0.0938\n",
      "epoch 6945: loss = 0.1475\n",
      "epoch 6946: loss = 0.1366\n",
      "epoch 6947: loss = 0.1528\n",
      "epoch 6948: loss = 0.1691\n",
      "epoch 6949: loss = 0.0533\n",
      "epoch 6950: loss = 0.1364\n",
      "epoch 6951: loss = 0.1291\n",
      "epoch 6952: loss = 0.0909\n",
      "epoch 6953: loss = 0.0449\n",
      "epoch 6954: loss = 0.0860\n",
      "epoch 6955: loss = 0.0877\n",
      "epoch 6956: loss = 0.0469\n",
      "epoch 6957: loss = 0.0988\n",
      "epoch 6958: loss = 0.1327\n",
      "epoch 6959: loss = 0.0772\n",
      "epoch 6960: loss = 0.1695\n",
      "epoch 6961: loss = 0.0688\n",
      "epoch 6962: loss = 0.0694\n",
      "epoch 6963: loss = 0.1615\n",
      "epoch 6964: loss = 0.1467\n",
      "epoch 6965: loss = 0.1557\n",
      "epoch 6966: loss = 0.1680\n",
      "epoch 6967: loss = 0.1057\n",
      "epoch 6968: loss = 0.0756\n",
      "epoch 6969: loss = 0.1240\n",
      "epoch 6970: loss = 0.1326\n",
      "epoch 6971: loss = 0.1842\n",
      "epoch 6972: loss = 0.1650\n",
      "epoch 6973: loss = 0.1595\n",
      "epoch 6974: loss = 0.0847\n",
      "epoch 6975: loss = 0.1350\n",
      "epoch 6976: loss = 0.1606\n",
      "epoch 6977: loss = 0.1145\n",
      "epoch 6978: loss = 0.1379\n",
      "epoch 6979: loss = 0.1318\n",
      "epoch 6980: loss = 0.1052\n",
      "epoch 6981: loss = 0.1353\n",
      "epoch 6982: loss = 0.1842\n",
      "epoch 6983: loss = 0.1819\n",
      "epoch 6984: loss = 0.1437\n",
      "epoch 6985: loss = 0.1654\n",
      "epoch 6986: loss = 0.1315\n",
      "epoch 6987: loss = 0.0646\n",
      "epoch 6988: loss = 0.0988\n",
      "epoch 6989: loss = 0.0861\n",
      "epoch 6990: loss = 0.0902\n",
      "epoch 6991: loss = 0.0920\n",
      "epoch 6992: loss = 0.0891\n",
      "epoch 6993: loss = 0.1073\n",
      "epoch 6994: loss = 0.0716\n",
      "epoch 6995: loss = 0.1195\n",
      "epoch 6996: loss = 0.0942\n",
      "epoch 6997: loss = 0.1788\n",
      "epoch 6998: loss = 0.1374\n",
      "epoch 6999: loss = 0.1589\n",
      "epoch 7000: loss = 0.1186\n",
      "epoch 7001: loss = 0.0732\n",
      "epoch 7002: loss = 0.0093\n",
      "epoch 7003: loss = 0.0629\n",
      "epoch 7004: loss = 0.0511\n",
      "epoch 7005: loss = 0.1050\n",
      "epoch 7006: loss = 0.1272\n",
      "epoch 7007: loss = 0.0470\n",
      "epoch 7008: loss = 0.1116\n",
      "epoch 7009: loss = 0.1370\n",
      "epoch 7010: loss = 0.0717\n",
      "epoch 7011: loss = 0.1249\n",
      "epoch 7012: loss = 0.1073\n",
      "epoch 7013: loss = 0.1321\n",
      "epoch 7014: loss = 0.1088\n",
      "epoch 7015: loss = 0.1404\n",
      "epoch 7016: loss = 0.0710\n",
      "epoch 7017: loss = 0.0812\n",
      "epoch 7018: loss = 0.1213\n",
      "epoch 7019: loss = 0.1469\n",
      "epoch 7020: loss = 0.1199\n",
      "epoch 7021: loss = 0.1884\n",
      "epoch 7022: loss = 0.0779\n",
      "epoch 7023: loss = 0.1394\n",
      "epoch 7024: loss = 0.1122\n",
      "epoch 7025: loss = 0.0850\n",
      "epoch 7026: loss = 0.1334\n",
      "epoch 7027: loss = 0.0906\n",
      "epoch 7028: loss = 0.1562\n",
      "epoch 7029: loss = 0.0964\n",
      "epoch 7030: loss = 0.1454\n",
      "epoch 7031: loss = 0.1434\n",
      "epoch 7032: loss = 0.0711\n",
      "epoch 7033: loss = 0.1161\n",
      "epoch 7034: loss = 0.0670\n",
      "epoch 7035: loss = 0.1624\n",
      "epoch 7036: loss = 0.1210\n",
      "epoch 7037: loss = 0.1822\n",
      "epoch 7038: loss = 0.1592\n",
      "epoch 7039: loss = 0.1430\n",
      "epoch 7040: loss = 0.1177\n",
      "epoch 7041: loss = 0.1180\n",
      "epoch 7042: loss = 0.2023\n",
      "epoch 7043: loss = 0.1696\n",
      "epoch 7044: loss = 0.1194\n",
      "epoch 7045: loss = 0.1629\n",
      "epoch 7046: loss = 0.1277\n",
      "epoch 7047: loss = 0.0937\n",
      "epoch 7048: loss = 0.1116\n",
      "epoch 7049: loss = 0.1361\n",
      "epoch 7050: loss = 0.0939\n",
      "epoch 7051: loss = 0.1417\n",
      "epoch 7052: loss = 0.1053\n",
      "epoch 7053: loss = 0.1058\n",
      "epoch 7054: loss = 0.1073\n",
      "epoch 7055: loss = 0.0996\n",
      "epoch 7056: loss = 0.1198\n",
      "epoch 7057: loss = 0.1608\n",
      "epoch 7058: loss = 0.1261\n",
      "epoch 7059: loss = 0.1378\n",
      "epoch 7060: loss = 0.1369\n",
      "epoch 7061: loss = 0.1093\n",
      "epoch 7062: loss = 0.1718\n",
      "epoch 7063: loss = 0.1118\n",
      "epoch 7064: loss = 0.0800\n",
      "epoch 7065: loss = 0.0480\n",
      "epoch 7066: loss = 0.1388\n",
      "epoch 7067: loss = 0.1073\n",
      "epoch 7068: loss = 0.0984\n",
      "epoch 7069: loss = 0.1399\n",
      "epoch 7070: loss = 0.1348\n",
      "epoch 7071: loss = 0.1893\n",
      "epoch 7072: loss = 0.1807\n",
      "epoch 7073: loss = 0.1462\n",
      "epoch 7074: loss = 0.1433\n",
      "epoch 7075: loss = 0.0667\n",
      "epoch 7076: loss = 0.0862\n",
      "epoch 7077: loss = 0.1299\n",
      "epoch 7078: loss = 0.1141\n",
      "epoch 7079: loss = 0.0705\n",
      "epoch 7080: loss = 0.1682\n",
      "epoch 7081: loss = 0.1107\n",
      "epoch 7082: loss = 0.1345\n",
      "epoch 7083: loss = 0.1025\n",
      "epoch 7084: loss = 0.1754\n",
      "epoch 7085: loss = 0.1158\n",
      "epoch 7086: loss = 0.1751\n",
      "epoch 7087: loss = 0.0831\n",
      "epoch 7088: loss = 0.1282\n",
      "epoch 7089: loss = 0.1005\n",
      "epoch 7090: loss = 0.1374\n",
      "epoch 7091: loss = 0.1517\n",
      "epoch 7092: loss = 0.1533\n",
      "epoch 7093: loss = 0.0481\n",
      "epoch 7094: loss = 0.1250\n",
      "epoch 7095: loss = 0.0694\n",
      "epoch 7096: loss = 0.1064\n",
      "epoch 7097: loss = 0.1195\n",
      "epoch 7098: loss = 0.0973\n",
      "epoch 7099: loss = 0.1571\n",
      "epoch 7100: loss = 0.0720\n",
      "epoch 7101: loss = 0.1496\n",
      "epoch 7102: loss = 0.1625\n",
      "epoch 7103: loss = 0.0507\n",
      "epoch 7104: loss = 0.1361\n",
      "epoch 7105: loss = 0.1190\n",
      "epoch 7106: loss = 0.0913\n",
      "epoch 7107: loss = 0.1437\n",
      "epoch 7108: loss = 0.1514\n",
      "epoch 7109: loss = 0.1202\n",
      "epoch 7110: loss = 0.0804\n",
      "epoch 7111: loss = 0.1023\n",
      "epoch 7112: loss = 0.0972\n",
      "epoch 7113: loss = 0.1675\n",
      "epoch 7114: loss = 0.1509\n",
      "epoch 7115: loss = 0.0737\n",
      "epoch 7116: loss = 0.1164\n",
      "epoch 7117: loss = 0.0424\n",
      "epoch 7118: loss = 0.0961\n",
      "epoch 7119: loss = 0.1640\n",
      "epoch 7120: loss = 0.1793\n",
      "epoch 7121: loss = 0.0746\n",
      "epoch 7122: loss = 0.1272\n",
      "epoch 7123: loss = 0.1023\n",
      "epoch 7124: loss = 0.0541\n",
      "epoch 7125: loss = 0.1065\n",
      "epoch 7126: loss = 0.1974\n",
      "epoch 7127: loss = 0.1854\n",
      "epoch 7128: loss = 0.0542\n",
      "epoch 7129: loss = 0.1503\n",
      "epoch 7130: loss = 0.1084\n",
      "epoch 7131: loss = 0.1593\n",
      "epoch 7132: loss = 0.1193\n",
      "epoch 7133: loss = 0.0524\n",
      "epoch 7134: loss = 0.1313\n",
      "epoch 7135: loss = 0.0512\n",
      "epoch 7136: loss = 0.1792\n",
      "epoch 7137: loss = 0.1095\n",
      "epoch 7138: loss = 0.1340\n",
      "epoch 7139: loss = 0.1476\n",
      "epoch 7140: loss = 0.1126\n",
      "epoch 7141: loss = 0.1011\n",
      "epoch 7142: loss = 0.1241\n",
      "epoch 7143: loss = 0.0890\n",
      "epoch 7144: loss = 0.0367\n",
      "epoch 7145: loss = 0.0444\n",
      "epoch 7146: loss = 0.1050\n",
      "epoch 7147: loss = 0.0837\n",
      "epoch 7148: loss = 0.0749\n",
      "epoch 7149: loss = 0.1109\n",
      "epoch 7150: loss = 0.0374\n",
      "epoch 7151: loss = 0.0989\n",
      "epoch 7152: loss = 0.1183\n",
      "epoch 7153: loss = 0.0876\n",
      "epoch 7154: loss = 0.0849\n",
      "epoch 7155: loss = 0.1677\n",
      "epoch 7156: loss = 0.1184\n",
      "epoch 7157: loss = 0.1204\n",
      "epoch 7158: loss = 0.1056\n",
      "epoch 7159: loss = 0.0750\n",
      "epoch 7160: loss = 0.1561\n",
      "epoch 7161: loss = 0.1573\n",
      "epoch 7162: loss = 0.0684\n",
      "epoch 7163: loss = 0.0931\n",
      "epoch 7164: loss = 0.0566\n",
      "epoch 7165: loss = 0.1282\n",
      "epoch 7166: loss = 0.1211\n",
      "epoch 7167: loss = 0.0682\n",
      "epoch 7168: loss = 0.1640\n",
      "epoch 7169: loss = 0.1087\n",
      "epoch 7170: loss = 0.1113\n",
      "epoch 7171: loss = 0.0716\n",
      "epoch 7172: loss = 0.0675\n",
      "epoch 7173: loss = 0.1445\n",
      "epoch 7174: loss = 0.0993\n",
      "epoch 7175: loss = 0.0658\n",
      "epoch 7176: loss = 0.0840\n",
      "epoch 7177: loss = 0.0687\n",
      "epoch 7178: loss = 0.0916\n",
      "epoch 7179: loss = 0.1266\n",
      "epoch 7180: loss = 0.1493\n",
      "epoch 7181: loss = 0.1323\n",
      "epoch 7182: loss = 0.1136\n",
      "epoch 7183: loss = 0.1369\n",
      "epoch 7184: loss = 0.1122\n",
      "epoch 7185: loss = 0.1001\n",
      "epoch 7186: loss = 0.0207\n",
      "epoch 7187: loss = 0.0838\n",
      "epoch 7188: loss = 0.1163\n",
      "epoch 7189: loss = 0.1217\n",
      "epoch 7190: loss = 0.1230\n",
      "epoch 7191: loss = 0.0548\n",
      "epoch 7192: loss = 0.1455\n",
      "epoch 7193: loss = 0.1170\n",
      "epoch 7194: loss = 0.0807\n",
      "epoch 7195: loss = 0.0977\n",
      "epoch 7196: loss = 0.0672\n",
      "epoch 7197: loss = 0.0717\n",
      "epoch 7198: loss = 0.1853\n",
      "epoch 7199: loss = 0.1191\n",
      "epoch 7200: loss = 0.1043\n",
      "epoch 7201: loss = 0.1385\n",
      "epoch 7202: loss = 0.0824\n",
      "epoch 7203: loss = 0.1338\n",
      "epoch 7204: loss = 0.1620\n",
      "epoch 7205: loss = 0.1580\n",
      "epoch 7206: loss = 0.1058\n",
      "epoch 7207: loss = 0.1405\n",
      "epoch 7208: loss = 0.1442\n",
      "epoch 7209: loss = 0.1220\n",
      "epoch 7210: loss = 0.0967\n",
      "epoch 7211: loss = 0.0483\n",
      "epoch 7212: loss = 0.0861\n",
      "epoch 7213: loss = 0.1176\n",
      "epoch 7214: loss = 0.0940\n",
      "epoch 7215: loss = 0.1346\n",
      "epoch 7216: loss = 0.0881\n",
      "epoch 7217: loss = 0.1299\n",
      "epoch 7218: loss = 0.0246\n",
      "epoch 7219: loss = 0.1244\n",
      "epoch 7220: loss = 0.0776\n",
      "epoch 7221: loss = 0.1215\n",
      "epoch 7222: loss = 0.0942\n",
      "epoch 7223: loss = 0.0355\n",
      "epoch 7224: loss = 0.0655\n",
      "epoch 7225: loss = 0.0542\n",
      "epoch 7226: loss = 0.0522\n",
      "epoch 7227: loss = 0.1300\n",
      "epoch 7228: loss = 0.1484\n",
      "epoch 7229: loss = 0.1432\n",
      "epoch 7230: loss = 0.0944\n",
      "epoch 7231: loss = 0.0874\n",
      "epoch 7232: loss = 0.1253\n",
      "epoch 7233: loss = 0.0506\n",
      "epoch 7234: loss = 0.1158\n",
      "epoch 7235: loss = 0.1288\n",
      "epoch 7236: loss = 0.1039\n",
      "epoch 7237: loss = 0.0608\n",
      "epoch 7238: loss = 0.1324\n",
      "epoch 7239: loss = 0.1973\n",
      "epoch 7240: loss = 0.1038\n",
      "epoch 7241: loss = 0.0440\n",
      "epoch 7242: loss = 0.0945\n",
      "epoch 7243: loss = 0.1785\n",
      "epoch 7244: loss = 0.0549\n",
      "epoch 7245: loss = 0.0911\n",
      "epoch 7246: loss = 0.1535\n",
      "epoch 7247: loss = 0.0851\n",
      "epoch 7248: loss = 0.1724\n",
      "epoch 7249: loss = 0.1423\n",
      "epoch 7250: loss = 0.0588\n",
      "epoch 7251: loss = 0.1818\n",
      "epoch 7252: loss = 0.1806\n",
      "epoch 7253: loss = 0.0510\n",
      "epoch 7254: loss = 0.1106\n",
      "epoch 7255: loss = 0.1281\n",
      "epoch 7256: loss = 0.1265\n",
      "epoch 7257: loss = 0.1024\n",
      "epoch 7258: loss = 0.1405\n",
      "epoch 7259: loss = 0.1557\n",
      "epoch 7260: loss = 0.0822\n",
      "epoch 7261: loss = 0.0694\n",
      "epoch 7262: loss = 0.0974\n",
      "epoch 7263: loss = 0.1287\n",
      "epoch 7264: loss = 0.1159\n",
      "epoch 7265: loss = 0.0981\n",
      "epoch 7266: loss = 0.1450\n",
      "epoch 7267: loss = 0.0849\n",
      "epoch 7268: loss = 0.1676\n",
      "epoch 7269: loss = 0.1636\n",
      "epoch 7270: loss = 0.1422\n",
      "epoch 7271: loss = 0.0981\n",
      "epoch 7272: loss = 0.0421\n",
      "epoch 7273: loss = 0.1726\n",
      "epoch 7274: loss = 0.0920\n",
      "epoch 7275: loss = 0.0930\n",
      "epoch 7276: loss = 0.1241\n",
      "epoch 7277: loss = 0.0853\n",
      "epoch 7278: loss = 0.1291\n",
      "epoch 7279: loss = 0.1499\n",
      "epoch 7280: loss = 0.1874\n",
      "epoch 7281: loss = 0.0927\n",
      "epoch 7282: loss = 0.0754\n",
      "epoch 7283: loss = 0.0642\n",
      "epoch 7284: loss = 0.1283\n",
      "epoch 7285: loss = 0.1024\n",
      "epoch 7286: loss = 0.1643\n",
      "epoch 7287: loss = 0.1677\n",
      "epoch 7288: loss = 0.1489\n",
      "epoch 7289: loss = 0.0978\n",
      "epoch 7290: loss = 0.0077\n",
      "epoch 7291: loss = 0.1306\n",
      "epoch 7292: loss = 0.1870\n",
      "epoch 7293: loss = 0.0877\n",
      "epoch 7294: loss = 0.0544\n",
      "epoch 7295: loss = 0.0931\n",
      "epoch 7296: loss = 0.1294\n",
      "epoch 7297: loss = 0.1500\n",
      "epoch 7298: loss = 0.1011\n",
      "epoch 7299: loss = 0.1688\n",
      "epoch 7300: loss = 0.1779\n",
      "epoch 7301: loss = 0.1450\n",
      "epoch 7302: loss = 0.0360\n",
      "epoch 7303: loss = 0.1373\n",
      "epoch 7304: loss = 0.1235\n",
      "epoch 7305: loss = 0.1706\n",
      "epoch 7306: loss = 0.1135\n",
      "epoch 7307: loss = 0.1450\n",
      "epoch 7308: loss = 0.1515\n",
      "epoch 7309: loss = 0.1340\n",
      "epoch 7310: loss = 0.1338\n",
      "epoch 7311: loss = 0.1145\n",
      "epoch 7312: loss = 0.0920\n",
      "epoch 7313: loss = 0.1005\n",
      "epoch 7314: loss = 0.1311\n",
      "epoch 7315: loss = 0.1113\n",
      "epoch 7316: loss = 0.0441\n",
      "epoch 7317: loss = 0.0826\n",
      "epoch 7318: loss = 0.1446\n",
      "epoch 7319: loss = 0.1349\n",
      "epoch 7320: loss = 0.1764\n",
      "epoch 7321: loss = 0.1531\n",
      "epoch 7322: loss = 0.1169\n",
      "epoch 7323: loss = 0.1734\n",
      "epoch 7324: loss = 0.0981\n",
      "epoch 7325: loss = 0.1869\n",
      "epoch 7326: loss = 0.0962\n",
      "epoch 7327: loss = 0.1110\n",
      "epoch 7328: loss = 0.0750\n",
      "epoch 7329: loss = 0.1407\n",
      "epoch 7330: loss = 0.1982\n",
      "epoch 7331: loss = 0.1665\n",
      "epoch 7332: loss = 0.0630\n",
      "epoch 7333: loss = 0.0662\n",
      "epoch 7334: loss = 0.1046\n",
      "epoch 7335: loss = 0.0791\n",
      "epoch 7336: loss = 0.1504\n",
      "epoch 7337: loss = 0.1089\n",
      "epoch 7338: loss = 0.1215\n",
      "epoch 7339: loss = 0.1559\n",
      "epoch 7340: loss = 0.0723\n",
      "epoch 7341: loss = 0.0501\n",
      "epoch 7342: loss = 0.0546\n",
      "epoch 7343: loss = 0.0924\n",
      "epoch 7344: loss = 0.1483\n",
      "epoch 7345: loss = 0.1027\n",
      "epoch 7346: loss = 0.1192\n",
      "epoch 7347: loss = 0.0425\n",
      "epoch 7348: loss = 0.0773\n",
      "epoch 7349: loss = 0.1542\n",
      "epoch 7350: loss = 0.1421\n",
      "epoch 7351: loss = 0.0875\n",
      "epoch 7352: loss = 0.0130\n",
      "epoch 7353: loss = 0.1682\n",
      "epoch 7354: loss = 0.0642\n",
      "epoch 7355: loss = 0.0822\n",
      "epoch 7356: loss = 0.1417\n",
      "epoch 7357: loss = 0.0998\n",
      "epoch 7358: loss = 0.0957\n",
      "epoch 7359: loss = 0.1208\n",
      "epoch 7360: loss = 0.1682\n",
      "epoch 7361: loss = 0.1713\n",
      "epoch 7362: loss = 0.1317\n",
      "epoch 7363: loss = 0.0692\n",
      "epoch 7364: loss = 0.1317\n",
      "epoch 7365: loss = 0.1467\n",
      "epoch 7366: loss = 0.1151\n",
      "epoch 7367: loss = 0.0723\n",
      "epoch 7368: loss = 0.1681\n",
      "epoch 7369: loss = 0.1034\n",
      "epoch 7370: loss = 0.1103\n",
      "epoch 7371: loss = 0.1144\n",
      "epoch 7372: loss = 0.1351\n",
      "epoch 7373: loss = 0.0948\n",
      "epoch 7374: loss = 0.1713\n",
      "epoch 7375: loss = 0.0780\n",
      "epoch 7376: loss = 0.1679\n",
      "epoch 7377: loss = 0.1124\n",
      "epoch 7378: loss = 0.1149\n",
      "epoch 7379: loss = 0.0765\n",
      "epoch 7380: loss = 0.1076\n",
      "epoch 7381: loss = 0.1395\n",
      "epoch 7382: loss = 0.1782\n",
      "epoch 7383: loss = 0.1075\n",
      "epoch 7384: loss = 0.1676\n",
      "epoch 7385: loss = 0.0894\n",
      "epoch 7386: loss = 0.0765\n",
      "epoch 7387: loss = 0.1392\n",
      "epoch 7388: loss = 0.1826\n",
      "epoch 7389: loss = 0.1440\n",
      "epoch 7390: loss = 0.1487\n",
      "epoch 7391: loss = 0.1672\n",
      "epoch 7392: loss = 0.1081\n",
      "epoch 7393: loss = 0.0535\n",
      "epoch 7394: loss = 0.0757\n",
      "epoch 7395: loss = 0.1151\n",
      "epoch 7396: loss = 0.1245\n",
      "epoch 7397: loss = 0.1621\n",
      "epoch 7398: loss = 0.1561\n",
      "epoch 7399: loss = 0.1498\n",
      "epoch 7400: loss = 0.0438\n",
      "epoch 7401: loss = 0.1516\n",
      "epoch 7402: loss = 0.0909\n",
      "epoch 7403: loss = 0.0953\n",
      "epoch 7404: loss = 0.0717\n",
      "epoch 7405: loss = 0.1144\n",
      "epoch 7406: loss = 0.1532\n",
      "epoch 7407: loss = 0.1015\n",
      "epoch 7408: loss = 0.1667\n",
      "epoch 7409: loss = 0.0979\n",
      "epoch 7410: loss = 0.0729\n",
      "epoch 7411: loss = 0.0869\n",
      "epoch 7412: loss = 0.0305\n",
      "epoch 7413: loss = 0.1718\n",
      "epoch 7414: loss = 0.1367\n",
      "epoch 7415: loss = 0.1458\n",
      "epoch 7416: loss = 0.1923\n",
      "epoch 7417: loss = 0.1109\n",
      "epoch 7418: loss = 0.1359\n",
      "epoch 7419: loss = 0.0889\n",
      "epoch 7420: loss = 0.1093\n",
      "epoch 7421: loss = 0.1038\n",
      "epoch 7422: loss = 0.1578\n",
      "epoch 7423: loss = 0.1157\n",
      "epoch 7424: loss = 0.1217\n",
      "epoch 7425: loss = 0.1041\n",
      "epoch 7426: loss = 0.1482\n",
      "epoch 7427: loss = 0.1496\n",
      "epoch 7428: loss = 0.1761\n",
      "epoch 7429: loss = 0.1093\n",
      "epoch 7430: loss = 0.1209\n",
      "epoch 7431: loss = 0.1276\n",
      "epoch 7432: loss = 0.1246\n",
      "epoch 7433: loss = 0.1315\n",
      "epoch 7434: loss = 0.1286\n",
      "epoch 7435: loss = 0.1816\n",
      "epoch 7436: loss = 0.1263\n",
      "epoch 7437: loss = 0.0587\n",
      "epoch 7438: loss = 0.1913\n",
      "epoch 7439: loss = 0.1659\n",
      "epoch 7440: loss = 0.0653\n",
      "epoch 7441: loss = 0.0876\n",
      "epoch 7442: loss = 0.0925\n",
      "epoch 7443: loss = 0.0762\n",
      "epoch 7444: loss = 0.0943\n",
      "epoch 7445: loss = 0.1079\n",
      "epoch 7446: loss = 0.0922\n",
      "epoch 7447: loss = 0.1765\n",
      "epoch 7448: loss = 0.1324\n",
      "epoch 7449: loss = 0.1213\n",
      "epoch 7450: loss = 0.1039\n",
      "epoch 7451: loss = 0.1185\n",
      "epoch 7452: loss = 0.0520\n",
      "epoch 7453: loss = 0.1250\n",
      "epoch 7454: loss = 0.1470\n",
      "epoch 7455: loss = 0.1006\n",
      "epoch 7456: loss = 0.1578\n",
      "epoch 7457: loss = 0.0557\n",
      "epoch 7458: loss = 0.1476\n",
      "epoch 7459: loss = 0.1684\n",
      "epoch 7460: loss = 0.1097\n",
      "epoch 7461: loss = 0.0158\n",
      "epoch 7462: loss = 0.0704\n",
      "epoch 7463: loss = 0.0187\n",
      "epoch 7464: loss = 0.1074\n",
      "epoch 7465: loss = 0.0292\n",
      "epoch 7466: loss = 0.0988\n",
      "epoch 7467: loss = 0.0409\n",
      "epoch 7468: loss = 0.0972\n",
      "epoch 7469: loss = 0.1111\n",
      "epoch 7470: loss = 0.0851\n",
      "epoch 7471: loss = 0.0213\n",
      "epoch 7472: loss = 0.1717\n",
      "epoch 7473: loss = 0.1180\n",
      "epoch 7474: loss = 0.1055\n",
      "epoch 7475: loss = 0.1191\n",
      "epoch 7476: loss = 0.1610\n",
      "epoch 7477: loss = 0.1016\n",
      "epoch 7478: loss = 0.1608\n",
      "epoch 7479: loss = 0.1416\n",
      "epoch 7480: loss = 0.1264\n",
      "epoch 7481: loss = 0.0674\n",
      "epoch 7482: loss = 0.1599\n",
      "epoch 7483: loss = 0.0397\n",
      "epoch 7484: loss = 0.1320\n",
      "epoch 7485: loss = 0.0563\n",
      "epoch 7486: loss = 0.1310\n",
      "epoch 7487: loss = 0.1340\n",
      "epoch 7488: loss = 0.0775\n",
      "epoch 7489: loss = 0.1355\n",
      "epoch 7490: loss = 0.1060\n",
      "epoch 7491: loss = 0.0570\n",
      "epoch 7492: loss = 0.1791\n",
      "epoch 7493: loss = 0.1676\n",
      "epoch 7494: loss = 0.1118\n",
      "epoch 7495: loss = 0.1407\n",
      "epoch 7496: loss = 0.1041\n",
      "epoch 7497: loss = 0.1251\n",
      "epoch 7498: loss = 0.0285\n",
      "epoch 7499: loss = 0.0895\n",
      "epoch 7500: loss = 0.1386\n",
      "epoch 7501: loss = 0.1723\n",
      "epoch 7502: loss = 0.0802\n",
      "epoch 7503: loss = 0.0319\n",
      "epoch 7504: loss = 0.0817\n",
      "epoch 7505: loss = 0.1693\n",
      "epoch 7506: loss = 0.1413\n",
      "epoch 7507: loss = 0.1674\n",
      "epoch 7508: loss = 0.0960\n",
      "epoch 7509: loss = 0.1011\n",
      "epoch 7510: loss = 0.1717\n",
      "epoch 7511: loss = 0.1155\n",
      "epoch 7512: loss = 0.0333\n",
      "epoch 7513: loss = 0.1318\n",
      "epoch 7514: loss = 0.0752\n",
      "epoch 7515: loss = 0.1175\n",
      "epoch 7516: loss = 0.0706\n",
      "epoch 7517: loss = 0.0800\n",
      "epoch 7518: loss = 0.1397\n",
      "epoch 7519: loss = 0.1717\n",
      "epoch 7520: loss = 0.1768\n",
      "epoch 7521: loss = 0.1129\n",
      "epoch 7522: loss = 0.1275\n",
      "epoch 7523: loss = 0.2082\n",
      "epoch 7524: loss = 0.1974\n",
      "epoch 7525: loss = 0.1013\n",
      "epoch 7526: loss = 0.1190\n",
      "epoch 7527: loss = 0.1412\n",
      "epoch 7528: loss = 0.0849\n",
      "epoch 7529: loss = 0.0751\n",
      "epoch 7530: loss = 0.1482\n",
      "epoch 7531: loss = 0.0776\n",
      "epoch 7532: loss = 0.0335\n",
      "epoch 7533: loss = 0.1050\n",
      "epoch 7534: loss = 0.1283\n",
      "epoch 7535: loss = 0.1594\n",
      "epoch 7536: loss = 0.0556\n",
      "epoch 7537: loss = 0.0503\n",
      "epoch 7538: loss = 0.1151\n",
      "epoch 7539: loss = 0.1053\n",
      "epoch 7540: loss = 0.0464\n",
      "epoch 7541: loss = 0.1206\n",
      "epoch 7542: loss = 0.1104\n",
      "epoch 7543: loss = 0.0987\n",
      "epoch 7544: loss = 0.1343\n",
      "epoch 7545: loss = 0.1123\n",
      "epoch 7546: loss = 0.1098\n",
      "epoch 7547: loss = 0.1477\n",
      "epoch 7548: loss = 0.1264\n",
      "epoch 7549: loss = 0.0650\n",
      "epoch 7550: loss = 0.0943\n",
      "epoch 7551: loss = 0.1015\n",
      "epoch 7552: loss = 0.1365\n",
      "epoch 7553: loss = 0.1908\n",
      "epoch 7554: loss = 0.1880\n",
      "epoch 7555: loss = 0.0845\n",
      "epoch 7556: loss = 0.1558\n",
      "epoch 7557: loss = 0.1346\n",
      "epoch 7558: loss = 0.1992\n",
      "epoch 7559: loss = 0.2064\n",
      "epoch 7560: loss = 0.1031\n",
      "epoch 7561: loss = 0.1331\n",
      "epoch 7562: loss = 0.1455\n",
      "epoch 7563: loss = 0.0981\n",
      "epoch 7564: loss = 0.1827\n",
      "epoch 7565: loss = 0.1232\n",
      "epoch 7566: loss = 0.0926\n",
      "epoch 7567: loss = 0.1337\n",
      "epoch 7568: loss = 0.1140\n",
      "epoch 7569: loss = 0.0697\n",
      "epoch 7570: loss = 0.0252\n",
      "epoch 7571: loss = 0.0996\n",
      "epoch 7572: loss = 0.0647\n",
      "epoch 7573: loss = 0.1402\n",
      "epoch 7574: loss = 0.0926\n",
      "epoch 7575: loss = 0.1046\n",
      "epoch 7576: loss = 0.1669\n",
      "epoch 7577: loss = 0.0415\n",
      "epoch 7578: loss = 0.1188\n",
      "epoch 7579: loss = 0.1148\n",
      "epoch 7580: loss = 0.1208\n",
      "epoch 7581: loss = 0.1083\n",
      "epoch 7582: loss = 0.1833\n",
      "epoch 7583: loss = 0.1291\n",
      "epoch 7584: loss = 0.0491\n",
      "epoch 7585: loss = 0.1245\n",
      "epoch 7586: loss = 0.1097\n",
      "epoch 7587: loss = 0.0995\n",
      "epoch 7588: loss = 0.1653\n",
      "epoch 7589: loss = 0.1166\n",
      "epoch 7590: loss = 0.0557\n",
      "epoch 7591: loss = 0.0927\n",
      "epoch 7592: loss = 0.1257\n",
      "epoch 7593: loss = 0.1428\n",
      "epoch 7594: loss = 0.0690\n",
      "epoch 7595: loss = 0.0806\n",
      "epoch 7596: loss = 0.0909\n",
      "epoch 7597: loss = 0.0580\n",
      "epoch 7598: loss = 0.0579\n",
      "epoch 7599: loss = 0.1488\n",
      "epoch 7600: loss = 0.1646\n",
      "epoch 7601: loss = 0.1132\n",
      "epoch 7602: loss = 0.1273\n",
      "epoch 7603: loss = 0.1218\n",
      "epoch 7604: loss = 0.1021\n",
      "epoch 7605: loss = 0.1398\n",
      "epoch 7606: loss = 0.1828\n",
      "epoch 7607: loss = 0.1895\n",
      "epoch 7608: loss = 0.1417\n",
      "epoch 7609: loss = 0.1358\n",
      "epoch 7610: loss = 0.1439\n",
      "epoch 7611: loss = 0.1476\n",
      "epoch 7612: loss = 0.1589\n",
      "epoch 7613: loss = 0.0773\n",
      "epoch 7614: loss = 0.0749\n",
      "epoch 7615: loss = 0.1245\n",
      "epoch 7616: loss = 0.1226\n",
      "epoch 7617: loss = 0.1153\n",
      "epoch 7618: loss = 0.1175\n",
      "epoch 7619: loss = 0.1253\n",
      "epoch 7620: loss = 0.1090\n",
      "epoch 7621: loss = 0.1315\n",
      "epoch 7622: loss = 0.1811\n",
      "epoch 7623: loss = 0.1425\n",
      "epoch 7624: loss = 0.1333\n",
      "epoch 7625: loss = 0.1464\n",
      "epoch 7626: loss = 0.1390\n",
      "epoch 7627: loss = 0.1699\n",
      "epoch 7628: loss = 0.1378\n",
      "epoch 7629: loss = 0.1843\n",
      "epoch 7630: loss = 0.1254\n",
      "epoch 7631: loss = 0.0609\n",
      "epoch 7632: loss = 0.1209\n",
      "epoch 7633: loss = 0.1213\n",
      "epoch 7634: loss = 0.0828\n",
      "epoch 7635: loss = 0.1068\n",
      "epoch 7636: loss = 0.0576\n",
      "epoch 7637: loss = 0.0887\n",
      "epoch 7638: loss = 0.1532\n",
      "epoch 7639: loss = 0.0946\n",
      "epoch 7640: loss = 0.1102\n",
      "epoch 7641: loss = 0.1470\n",
      "epoch 7642: loss = 0.1202\n",
      "epoch 7643: loss = 0.1307\n",
      "epoch 7644: loss = 0.1850\n",
      "epoch 7645: loss = 0.0739\n",
      "epoch 7646: loss = 0.0710\n",
      "epoch 7647: loss = 0.0874\n",
      "epoch 7648: loss = 0.1205\n",
      "epoch 7649: loss = 0.1194\n",
      "epoch 7650: loss = 0.0691\n",
      "epoch 7651: loss = 0.1040\n",
      "epoch 7652: loss = 0.0866\n",
      "epoch 7653: loss = 0.1859\n",
      "epoch 7654: loss = 0.1059\n",
      "epoch 7655: loss = 0.1170\n",
      "epoch 7656: loss = 0.1000\n",
      "epoch 7657: loss = 0.1592\n",
      "epoch 7658: loss = 0.1059\n",
      "epoch 7659: loss = 0.0903\n",
      "epoch 7660: loss = 0.1327\n",
      "epoch 7661: loss = 0.0996\n",
      "epoch 7662: loss = 0.0670\n",
      "epoch 7663: loss = 0.1189\n",
      "epoch 7664: loss = 0.1104\n",
      "epoch 7665: loss = 0.1218\n",
      "epoch 7666: loss = 0.0750\n",
      "epoch 7667: loss = 0.0628\n",
      "epoch 7668: loss = 0.0911\n",
      "epoch 7669: loss = 0.0762\n",
      "epoch 7670: loss = 0.1365\n",
      "epoch 7671: loss = 0.1053\n",
      "epoch 7672: loss = 0.0997\n",
      "epoch 7673: loss = 0.0396\n",
      "epoch 7674: loss = 0.1586\n",
      "epoch 7675: loss = 0.1752\n",
      "epoch 7676: loss = 0.1865\n",
      "epoch 7677: loss = 0.1779\n",
      "epoch 7678: loss = 0.0881\n",
      "epoch 7679: loss = 0.0882\n",
      "epoch 7680: loss = 0.0441\n",
      "epoch 7681: loss = 0.0552\n",
      "epoch 7682: loss = 0.1747\n",
      "epoch 7683: loss = 0.0940\n",
      "epoch 7684: loss = 0.1948\n",
      "epoch 7685: loss = 0.0958\n",
      "epoch 7686: loss = 0.0450\n",
      "epoch 7687: loss = 0.1172\n",
      "epoch 7688: loss = 0.1298\n",
      "epoch 7689: loss = 0.1246\n",
      "epoch 7690: loss = 0.1400\n",
      "epoch 7691: loss = 0.1620\n",
      "epoch 7692: loss = 0.1188\n",
      "epoch 7693: loss = 0.1574\n",
      "epoch 7694: loss = 0.1686\n",
      "epoch 7695: loss = 0.0775\n",
      "epoch 7696: loss = 0.0756\n",
      "epoch 7697: loss = 0.0897\n",
      "epoch 7698: loss = 0.1031\n",
      "epoch 7699: loss = 0.1155\n",
      "epoch 7700: loss = 0.1296\n",
      "epoch 7701: loss = 0.1231\n",
      "epoch 7702: loss = 0.1342\n",
      "epoch 7703: loss = 0.1460\n",
      "epoch 7704: loss = 0.1360\n",
      "epoch 7705: loss = 0.0938\n",
      "epoch 7706: loss = 0.1962\n",
      "epoch 7707: loss = 0.1764\n",
      "epoch 7708: loss = 0.0765\n",
      "epoch 7709: loss = 0.0456\n",
      "epoch 7710: loss = 0.1112\n",
      "epoch 7711: loss = 0.1249\n",
      "epoch 7712: loss = 0.1042\n",
      "epoch 7713: loss = 0.1161\n",
      "epoch 7714: loss = 0.0303\n",
      "epoch 7715: loss = 0.1170\n",
      "epoch 7716: loss = 0.1482\n",
      "epoch 7717: loss = 0.1552\n",
      "epoch 7718: loss = 0.0331\n",
      "epoch 7719: loss = 0.0703\n",
      "epoch 7720: loss = 0.1734\n",
      "epoch 7721: loss = 0.1166\n",
      "epoch 7722: loss = 0.1349\n",
      "epoch 7723: loss = 0.0491\n",
      "epoch 7724: loss = 0.1425\n",
      "epoch 7725: loss = 0.0622\n",
      "epoch 7726: loss = 0.1414\n",
      "epoch 7727: loss = 0.0959\n",
      "epoch 7728: loss = 0.1574\n",
      "epoch 7729: loss = 0.1157\n",
      "epoch 7730: loss = 0.1420\n",
      "epoch 7731: loss = 0.0894\n",
      "epoch 7732: loss = 0.0530\n",
      "epoch 7733: loss = 0.1052\n",
      "epoch 7734: loss = 0.1476\n",
      "epoch 7735: loss = 0.1708\n",
      "epoch 7736: loss = 0.1501\n",
      "epoch 7737: loss = 0.0969\n",
      "epoch 7738: loss = 0.0736\n",
      "epoch 7739: loss = 0.0427\n",
      "epoch 7740: loss = 0.1047\n",
      "epoch 7741: loss = 0.1313\n",
      "epoch 7742: loss = 0.1431\n",
      "epoch 7743: loss = 0.0193\n",
      "epoch 7744: loss = 0.1163\n",
      "epoch 7745: loss = 0.1465\n",
      "epoch 7746: loss = 0.1502\n",
      "epoch 7747: loss = 0.1765\n",
      "epoch 7748: loss = 0.1887\n",
      "epoch 7749: loss = 0.1671\n",
      "epoch 7750: loss = 0.1521\n",
      "epoch 7751: loss = 0.0903\n",
      "epoch 7752: loss = 0.0427\n",
      "epoch 7753: loss = 0.1066\n",
      "epoch 7754: loss = 0.1102\n",
      "epoch 7755: loss = 0.1178\n",
      "epoch 7756: loss = 0.1013\n",
      "epoch 7757: loss = 0.0888\n",
      "epoch 7758: loss = 0.0791\n",
      "epoch 7759: loss = 0.0204\n",
      "epoch 7760: loss = 0.0945\n",
      "epoch 7761: loss = 0.1176\n",
      "epoch 7762: loss = 0.1072\n",
      "epoch 7763: loss = 0.1200\n",
      "epoch 7764: loss = 0.1351\n",
      "epoch 7765: loss = 0.1708\n",
      "epoch 7766: loss = 0.0578\n",
      "epoch 7767: loss = 0.1283\n",
      "epoch 7768: loss = 0.0239\n",
      "epoch 7769: loss = 0.0359\n",
      "epoch 7770: loss = 0.1424\n",
      "epoch 7771: loss = 0.0878\n",
      "epoch 7772: loss = 0.1608\n",
      "epoch 7773: loss = 0.1709\n",
      "epoch 7774: loss = 0.1075\n",
      "epoch 7775: loss = 0.0925\n",
      "epoch 7776: loss = 0.0525\n",
      "epoch 7777: loss = 0.1875\n",
      "epoch 7778: loss = 0.1811\n",
      "epoch 7779: loss = 0.0966\n",
      "epoch 7780: loss = 0.1370\n",
      "epoch 7781: loss = 0.0974\n",
      "epoch 7782: loss = 0.1049\n",
      "epoch 7783: loss = 0.1278\n",
      "epoch 7784: loss = 0.1275\n",
      "epoch 7785: loss = 0.1187\n",
      "epoch 7786: loss = 0.0986\n",
      "epoch 7787: loss = 0.1044\n",
      "epoch 7788: loss = 0.1864\n",
      "epoch 7789: loss = 0.0425\n",
      "epoch 7790: loss = 0.1930\n",
      "epoch 7791: loss = 0.1426\n",
      "epoch 7792: loss = 0.1238\n",
      "epoch 7793: loss = 0.1489\n",
      "epoch 7794: loss = 0.0413\n",
      "epoch 7795: loss = 0.1391\n",
      "epoch 7796: loss = 0.1088\n",
      "epoch 7797: loss = 0.1188\n",
      "epoch 7798: loss = 0.0050\n",
      "epoch 7799: loss = 0.1251\n",
      "epoch 7800: loss = 0.1626\n",
      "epoch 7801: loss = 0.1767\n",
      "epoch 7802: loss = 0.0892\n",
      "epoch 7803: loss = 0.0306\n",
      "epoch 7804: loss = 0.0615\n",
      "epoch 7805: loss = 0.0831\n",
      "epoch 7806: loss = 0.0991\n",
      "epoch 7807: loss = 0.1579\n",
      "epoch 7808: loss = 0.1497\n",
      "epoch 7809: loss = 0.0330\n",
      "epoch 7810: loss = 0.0545\n",
      "epoch 7811: loss = 0.1550\n",
      "epoch 7812: loss = 0.0661\n",
      "epoch 7813: loss = 0.0879\n",
      "epoch 7814: loss = 0.1785\n",
      "epoch 7815: loss = 0.1712\n",
      "epoch 7816: loss = 0.1417\n",
      "epoch 7817: loss = 0.1585\n",
      "epoch 7818: loss = 0.1145\n",
      "epoch 7819: loss = 0.1227\n",
      "epoch 7820: loss = 0.1006\n",
      "epoch 7821: loss = 0.1105\n",
      "epoch 7822: loss = 0.1581\n",
      "epoch 7823: loss = 0.1579\n",
      "epoch 7824: loss = 0.1042\n",
      "epoch 7825: loss = 0.1509\n",
      "epoch 7826: loss = 0.1050\n",
      "epoch 7827: loss = 0.0679\n",
      "epoch 7828: loss = 0.1137\n",
      "epoch 7829: loss = 0.1312\n",
      "epoch 7830: loss = 0.0948\n",
      "epoch 7831: loss = 0.1414\n",
      "epoch 7832: loss = 0.1312\n",
      "epoch 7833: loss = 0.1281\n",
      "epoch 7834: loss = 0.0375\n",
      "epoch 7835: loss = 0.1186\n",
      "epoch 7836: loss = 0.1293\n",
      "epoch 7837: loss = 0.1156\n",
      "epoch 7838: loss = 0.0698\n",
      "epoch 7839: loss = 0.0889\n",
      "epoch 7840: loss = 0.0281\n",
      "epoch 7841: loss = 0.1224\n",
      "epoch 7842: loss = 0.1266\n",
      "epoch 7843: loss = 0.0932\n",
      "epoch 7844: loss = 0.0630\n",
      "epoch 7845: loss = 0.1471\n",
      "epoch 7846: loss = 0.1392\n",
      "epoch 7847: loss = 0.1146\n",
      "epoch 7848: loss = 0.0287\n",
      "epoch 7849: loss = 0.1435\n",
      "epoch 7850: loss = 0.1435\n",
      "epoch 7851: loss = 0.1517\n",
      "epoch 7852: loss = 0.0462\n",
      "epoch 7853: loss = 0.1513\n",
      "epoch 7854: loss = 0.1587\n",
      "epoch 7855: loss = 0.1563\n",
      "epoch 7856: loss = 0.0758\n",
      "epoch 7857: loss = 0.1403\n",
      "epoch 7858: loss = 0.1514\n",
      "epoch 7859: loss = 0.0331\n",
      "epoch 7860: loss = 0.1275\n",
      "epoch 7861: loss = 0.0861\n",
      "epoch 7862: loss = 0.1248\n",
      "epoch 7863: loss = 0.1312\n",
      "epoch 7864: loss = 0.1885\n",
      "epoch 7865: loss = 0.1229\n",
      "epoch 7866: loss = 0.1679\n",
      "epoch 7867: loss = 0.1115\n",
      "epoch 7868: loss = 0.0871\n",
      "epoch 7869: loss = 0.0880\n",
      "epoch 7870: loss = 0.0573\n",
      "epoch 7871: loss = 0.0861\n",
      "epoch 7872: loss = 0.1550\n",
      "epoch 7873: loss = 0.1233\n",
      "epoch 7874: loss = 0.0636\n",
      "epoch 7875: loss = 0.1222\n",
      "epoch 7876: loss = 0.1259\n",
      "epoch 7877: loss = 0.1314\n",
      "epoch 7878: loss = 0.0890\n",
      "epoch 7879: loss = 0.1317\n",
      "epoch 7880: loss = 0.0684\n",
      "epoch 7881: loss = 0.0873\n",
      "epoch 7882: loss = 0.0717\n",
      "epoch 7883: loss = 0.1609\n",
      "epoch 7884: loss = 0.1627\n",
      "epoch 7885: loss = 0.1176\n",
      "epoch 7886: loss = 0.1099\n",
      "epoch 7887: loss = 0.0941\n",
      "epoch 7888: loss = 0.0858\n",
      "epoch 7889: loss = 0.1199\n",
      "epoch 7890: loss = 0.0995\n",
      "epoch 7891: loss = 0.1323\n",
      "epoch 7892: loss = 0.0760\n",
      "epoch 7893: loss = 0.0674\n",
      "epoch 7894: loss = 0.1358\n",
      "epoch 7895: loss = 0.1729\n",
      "epoch 7896: loss = 0.0976\n",
      "epoch 7897: loss = 0.0854\n",
      "epoch 7898: loss = 0.1518\n",
      "epoch 7899: loss = 0.1690\n",
      "epoch 7900: loss = 0.0884\n",
      "epoch 7901: loss = 0.1267\n",
      "epoch 7902: loss = 0.1121\n",
      "epoch 7903: loss = 0.1671\n",
      "epoch 7904: loss = 0.1518\n",
      "epoch 7905: loss = 0.0935\n",
      "epoch 7906: loss = 0.0311\n",
      "epoch 7907: loss = 0.0972\n",
      "epoch 7908: loss = 0.0708\n",
      "epoch 7909: loss = 0.0838\n",
      "epoch 7910: loss = 0.0832\n",
      "epoch 7911: loss = 0.1041\n",
      "epoch 7912: loss = 0.1004\n",
      "epoch 7913: loss = 0.1484\n",
      "epoch 7914: loss = 0.1426\n",
      "epoch 7915: loss = 0.0971\n",
      "epoch 7916: loss = 0.1737\n",
      "epoch 7917: loss = 0.1255\n",
      "epoch 7918: loss = 0.1082\n",
      "epoch 7919: loss = 0.0805\n",
      "epoch 7920: loss = 0.1445\n",
      "epoch 7921: loss = 0.0748\n",
      "epoch 7922: loss = 0.1830\n",
      "epoch 7923: loss = 0.1819\n",
      "epoch 7924: loss = 0.1212\n",
      "epoch 7925: loss = 0.0874\n",
      "epoch 7926: loss = 0.1526\n",
      "epoch 7927: loss = 0.0739\n",
      "epoch 7928: loss = 0.0583\n",
      "epoch 7929: loss = 0.0305\n",
      "epoch 7930: loss = 0.1010\n",
      "epoch 7931: loss = 0.1820\n",
      "epoch 7932: loss = 0.0999\n",
      "epoch 7933: loss = 0.1493\n",
      "epoch 7934: loss = 0.1104\n",
      "epoch 7935: loss = 0.1594\n",
      "epoch 7936: loss = 0.1066\n",
      "epoch 7937: loss = 0.1180\n",
      "epoch 7938: loss = 0.1674\n",
      "epoch 7939: loss = 0.1215\n",
      "epoch 7940: loss = 0.0479\n",
      "epoch 7941: loss = 0.1434\n",
      "epoch 7942: loss = 0.1238\n",
      "epoch 7943: loss = 0.1650\n",
      "epoch 7944: loss = 0.0565\n",
      "epoch 7945: loss = 0.0543\n",
      "epoch 7946: loss = 0.0774\n",
      "epoch 7947: loss = 0.0483\n",
      "epoch 7948: loss = 0.1676\n",
      "epoch 7949: loss = 0.1088\n",
      "epoch 7950: loss = 0.1014\n",
      "epoch 7951: loss = 0.1716\n",
      "epoch 7952: loss = 0.0472\n",
      "epoch 7953: loss = 0.1035\n",
      "epoch 7954: loss = 0.0686\n",
      "epoch 7955: loss = 0.0935\n",
      "epoch 7956: loss = 0.0594\n",
      "epoch 7957: loss = 0.1362\n",
      "epoch 7958: loss = 0.1713\n",
      "epoch 7959: loss = 0.1334\n",
      "epoch 7960: loss = 0.0595\n",
      "epoch 7961: loss = 0.1243\n",
      "epoch 7962: loss = 0.1316\n",
      "epoch 7963: loss = 0.1450\n",
      "epoch 7964: loss = 0.1529\n",
      "epoch 7965: loss = 0.0685\n",
      "epoch 7966: loss = 0.1406\n",
      "epoch 7967: loss = 0.0897\n",
      "epoch 7968: loss = 0.0716\n",
      "epoch 7969: loss = 0.0719\n",
      "epoch 7970: loss = 0.0930\n",
      "epoch 7971: loss = 0.0792\n",
      "epoch 7972: loss = 0.0597\n",
      "epoch 7973: loss = 0.1437\n",
      "epoch 7974: loss = 0.1637\n",
      "epoch 7975: loss = 0.1062\n",
      "epoch 7976: loss = 0.1498\n",
      "epoch 7977: loss = 0.1143\n",
      "epoch 7978: loss = 0.0684\n",
      "epoch 7979: loss = 0.1103\n",
      "epoch 7980: loss = 0.1132\n",
      "epoch 7981: loss = 0.0316\n",
      "epoch 7982: loss = 0.0929\n",
      "epoch 7983: loss = 0.1531\n",
      "epoch 7984: loss = 0.1829\n",
      "epoch 7985: loss = 0.1247\n",
      "epoch 7986: loss = 0.0969\n",
      "epoch 7987: loss = 0.1136\n",
      "epoch 7988: loss = 0.0836\n",
      "epoch 7989: loss = 0.1392\n",
      "epoch 7990: loss = 0.1361\n",
      "epoch 7991: loss = 0.1132\n",
      "epoch 7992: loss = 0.0736\n",
      "epoch 7993: loss = 0.0147\n",
      "epoch 7994: loss = 0.0608\n",
      "epoch 7995: loss = 0.0819\n",
      "epoch 7996: loss = 0.1677\n",
      "epoch 7997: loss = 0.0969\n",
      "epoch 7998: loss = 0.0717\n",
      "epoch 7999: loss = 0.1595\n",
      "epoch 8000: loss = 0.1458\n",
      "epoch 8001: loss = 0.1823\n",
      "epoch 8002: loss = 0.1989\n",
      "epoch 8003: loss = 0.1318\n",
      "epoch 8004: loss = 0.1593\n",
      "epoch 8005: loss = 0.0309\n",
      "epoch 8006: loss = 0.1528\n",
      "epoch 8007: loss = 0.1419\n",
      "epoch 8008: loss = 0.1203\n",
      "epoch 8009: loss = 0.1513\n",
      "epoch 8010: loss = 0.0974\n",
      "epoch 8011: loss = 0.1351\n",
      "epoch 8012: loss = 0.1080\n",
      "epoch 8013: loss = 0.1424\n",
      "epoch 8014: loss = 0.1899\n",
      "epoch 8015: loss = 0.1181\n",
      "epoch 8016: loss = 0.1381\n",
      "epoch 8017: loss = 0.1289\n",
      "epoch 8018: loss = 0.1896\n",
      "epoch 8019: loss = 0.1128\n",
      "epoch 8020: loss = 0.1509\n",
      "epoch 8021: loss = 0.0862\n",
      "epoch 8022: loss = 0.1028\n",
      "epoch 8023: loss = 0.0934\n",
      "epoch 8024: loss = 0.1149\n",
      "epoch 8025: loss = 0.1445\n",
      "epoch 8026: loss = 0.1425\n",
      "epoch 8027: loss = 0.1611\n",
      "epoch 8028: loss = 0.1162\n",
      "epoch 8029: loss = 0.1552\n",
      "epoch 8030: loss = 0.0752\n",
      "epoch 8031: loss = 0.1746\n",
      "epoch 8032: loss = 0.1450\n",
      "epoch 8033: loss = 0.1515\n",
      "epoch 8034: loss = 0.1330\n",
      "epoch 8035: loss = 0.0794\n",
      "epoch 8036: loss = 0.1293\n",
      "epoch 8037: loss = 0.1715\n",
      "epoch 8038: loss = 0.0837\n",
      "epoch 8039: loss = 0.0952\n",
      "epoch 8040: loss = 0.0951\n",
      "epoch 8041: loss = 0.1658\n",
      "epoch 8042: loss = 0.1600\n",
      "epoch 8043: loss = 0.1348\n",
      "epoch 8044: loss = 0.1306\n",
      "epoch 8045: loss = 0.1194\n",
      "epoch 8046: loss = 0.1036\n",
      "epoch 8047: loss = 0.0481\n",
      "epoch 8048: loss = 0.1080\n",
      "epoch 8049: loss = 0.1538\n",
      "epoch 8050: loss = 0.1488\n",
      "epoch 8051: loss = 0.1461\n",
      "epoch 8052: loss = 0.1665\n",
      "epoch 8053: loss = 0.1722\n",
      "epoch 8054: loss = 0.1614\n",
      "epoch 8055: loss = 0.1540\n",
      "epoch 8056: loss = 0.2043\n",
      "epoch 8057: loss = 0.0635\n",
      "epoch 8058: loss = 0.0927\n",
      "epoch 8059: loss = 0.0242\n",
      "epoch 8060: loss = 0.0637\n",
      "epoch 8061: loss = 0.0902\n",
      "epoch 8062: loss = 0.0917\n",
      "epoch 8063: loss = 0.0603\n",
      "epoch 8064: loss = 0.1161\n",
      "epoch 8065: loss = 0.1124\n",
      "epoch 8066: loss = 0.0360\n",
      "epoch 8067: loss = 0.1364\n",
      "epoch 8068: loss = 0.1029\n",
      "epoch 8069: loss = 0.1693\n",
      "epoch 8070: loss = 0.1220\n",
      "epoch 8071: loss = 0.0764\n",
      "epoch 8072: loss = 0.1727\n",
      "epoch 8073: loss = 0.1180\n",
      "epoch 8074: loss = 0.1025\n",
      "epoch 8075: loss = 0.1275\n",
      "epoch 8076: loss = 0.1253\n",
      "epoch 8077: loss = 0.1325\n",
      "epoch 8078: loss = 0.0940\n",
      "epoch 8079: loss = 0.1197\n",
      "epoch 8080: loss = 0.1515\n",
      "epoch 8081: loss = 0.1024\n",
      "epoch 8082: loss = 0.1511\n",
      "epoch 8083: loss = 0.1399\n",
      "epoch 8084: loss = 0.0830\n",
      "epoch 8085: loss = 0.1218\n",
      "epoch 8086: loss = 0.1139\n",
      "epoch 8087: loss = 0.1441\n",
      "epoch 8088: loss = 0.1299\n",
      "epoch 8089: loss = 0.1778\n",
      "epoch 8090: loss = 0.1107\n",
      "epoch 8091: loss = 0.1565\n",
      "epoch 8092: loss = 0.1712\n",
      "epoch 8093: loss = 0.1864\n",
      "epoch 8094: loss = 0.1566\n",
      "epoch 8095: loss = 0.1118\n",
      "epoch 8096: loss = 0.1046\n",
      "epoch 8097: loss = 0.0904\n",
      "epoch 8098: loss = 0.1231\n",
      "epoch 8099: loss = 0.0787\n",
      "epoch 8100: loss = 0.1012\n",
      "epoch 8101: loss = 0.1100\n",
      "epoch 8102: loss = 0.1030\n",
      "epoch 8103: loss = 0.1291\n",
      "epoch 8104: loss = 0.0534\n",
      "epoch 8105: loss = 0.0893\n",
      "epoch 8106: loss = 0.0743\n",
      "epoch 8107: loss = 0.1968\n",
      "epoch 8108: loss = 0.1737\n",
      "epoch 8109: loss = 0.1624\n",
      "epoch 8110: loss = 0.1713\n",
      "epoch 8111: loss = 0.1580\n",
      "epoch 8112: loss = 0.1126\n",
      "epoch 8113: loss = 0.1264\n",
      "epoch 8114: loss = 0.1754\n",
      "epoch 8115: loss = 0.1318\n",
      "epoch 8116: loss = 0.1094\n",
      "epoch 8117: loss = 0.1053\n",
      "epoch 8118: loss = 0.0718\n",
      "epoch 8119: loss = 0.1114\n",
      "epoch 8120: loss = 0.0952\n",
      "epoch 8121: loss = 0.1236\n",
      "epoch 8122: loss = 0.1067\n",
      "epoch 8123: loss = 0.0949\n",
      "epoch 8124: loss = 0.1642\n",
      "epoch 8125: loss = 0.0958\n",
      "epoch 8126: loss = 0.1588\n",
      "epoch 8127: loss = 0.1749\n",
      "epoch 8128: loss = 0.1282\n",
      "epoch 8129: loss = 0.1350\n",
      "epoch 8130: loss = 0.0321\n",
      "epoch 8131: loss = 0.1108\n",
      "epoch 8132: loss = 0.0877\n",
      "epoch 8133: loss = 0.1531\n",
      "epoch 8134: loss = 0.0733\n",
      "epoch 8135: loss = 0.1285\n",
      "epoch 8136: loss = 0.1756\n",
      "epoch 8137: loss = 0.1330\n",
      "epoch 8138: loss = 0.0448\n",
      "epoch 8139: loss = 0.1535\n",
      "epoch 8140: loss = 0.1288\n",
      "epoch 8141: loss = 0.0845\n",
      "epoch 8142: loss = 0.0974\n",
      "epoch 8143: loss = 0.1078\n",
      "epoch 8144: loss = 0.1467\n",
      "epoch 8145: loss = 0.1734\n",
      "epoch 8146: loss = 0.1465\n",
      "epoch 8147: loss = 0.1594\n",
      "epoch 8148: loss = 0.0708\n",
      "epoch 8149: loss = 0.0500\n",
      "epoch 8150: loss = 0.1391\n",
      "epoch 8151: loss = 0.0198\n",
      "epoch 8152: loss = 0.1355\n",
      "epoch 8153: loss = 0.0605\n",
      "epoch 8154: loss = 0.1632\n",
      "epoch 8155: loss = 0.1236\n",
      "epoch 8156: loss = 0.1190\n",
      "epoch 8157: loss = 0.1779\n",
      "epoch 8158: loss = 0.1469\n",
      "epoch 8159: loss = 0.1484\n",
      "epoch 8160: loss = 0.1544\n",
      "epoch 8161: loss = 0.1631\n",
      "epoch 8162: loss = 0.1180\n",
      "epoch 8163: loss = 0.1009\n",
      "epoch 8164: loss = 0.1591\n",
      "epoch 8165: loss = 0.1134\n",
      "epoch 8166: loss = 0.1984\n",
      "epoch 8167: loss = 0.1756\n",
      "epoch 8168: loss = 0.0911\n",
      "epoch 8169: loss = 0.1118\n",
      "epoch 8170: loss = 0.1258\n",
      "epoch 8171: loss = 0.1428\n",
      "epoch 8172: loss = 0.1316\n",
      "epoch 8173: loss = 0.0359\n",
      "epoch 8174: loss = 0.1700\n",
      "epoch 8175: loss = 0.0158\n",
      "epoch 8176: loss = 0.1822\n",
      "epoch 8177: loss = 0.0972\n",
      "epoch 8178: loss = 0.0933\n",
      "epoch 8179: loss = 0.0921\n",
      "epoch 8180: loss = 0.0681\n",
      "epoch 8181: loss = 0.1521\n",
      "epoch 8182: loss = 0.0447\n",
      "epoch 8183: loss = 0.0806\n",
      "epoch 8184: loss = 0.1278\n",
      "epoch 8185: loss = 0.1138\n",
      "epoch 8186: loss = 0.1279\n",
      "epoch 8187: loss = 0.0677\n",
      "epoch 8188: loss = 0.0626\n",
      "epoch 8189: loss = 0.1128\n",
      "epoch 8190: loss = 0.0987\n",
      "epoch 8191: loss = 0.0529\n",
      "epoch 8192: loss = 0.1343\n",
      "epoch 8193: loss = 0.1300\n",
      "epoch 8194: loss = 0.0746\n",
      "epoch 8195: loss = 0.1184\n",
      "epoch 8196: loss = 0.0729\n",
      "epoch 8197: loss = 0.1654\n",
      "epoch 8198: loss = 0.1571\n",
      "epoch 8199: loss = 0.1289\n",
      "epoch 8200: loss = 0.1468\n",
      "epoch 8201: loss = 0.0721\n",
      "epoch 8202: loss = 0.0111\n",
      "epoch 8203: loss = 0.0998\n",
      "epoch 8204: loss = 0.1244\n",
      "epoch 8205: loss = 0.1043\n",
      "epoch 8206: loss = 0.1150\n",
      "epoch 8207: loss = 0.1649\n",
      "epoch 8208: loss = 0.0432\n",
      "epoch 8209: loss = 0.0152\n",
      "epoch 8210: loss = 0.0904\n",
      "epoch 8211: loss = 0.0698\n",
      "epoch 8212: loss = 0.1472\n",
      "epoch 8213: loss = 0.1312\n",
      "epoch 8214: loss = 0.1373\n",
      "epoch 8215: loss = 0.0811\n",
      "epoch 8216: loss = 0.1275\n",
      "epoch 8217: loss = 0.1410\n",
      "epoch 8218: loss = 0.1661\n",
      "epoch 8219: loss = 0.1490\n",
      "epoch 8220: loss = 0.1381\n",
      "epoch 8221: loss = 0.1053\n",
      "epoch 8222: loss = 0.1603\n",
      "epoch 8223: loss = 0.0446\n",
      "epoch 8224: loss = 0.0966\n",
      "epoch 8225: loss = 0.0817\n",
      "epoch 8226: loss = 0.0359\n",
      "epoch 8227: loss = 0.0958\n",
      "epoch 8228: loss = 0.0662\n",
      "epoch 8229: loss = 0.1067\n",
      "epoch 8230: loss = 0.1034\n",
      "epoch 8231: loss = 0.1366\n",
      "epoch 8232: loss = 0.1614\n",
      "epoch 8233: loss = 0.0889\n",
      "epoch 8234: loss = 0.0804\n",
      "epoch 8235: loss = 0.1069\n",
      "epoch 8236: loss = 0.0761\n",
      "epoch 8237: loss = 0.1342\n",
      "epoch 8238: loss = 0.1931\n",
      "epoch 8239: loss = 0.1853\n",
      "epoch 8240: loss = 0.1556\n",
      "epoch 8241: loss = 0.1329\n",
      "epoch 8242: loss = 0.1744\n",
      "epoch 8243: loss = 0.1781\n",
      "epoch 8244: loss = 0.1491\n",
      "epoch 8245: loss = 0.1273\n",
      "epoch 8246: loss = 0.1255\n",
      "epoch 8247: loss = 0.1610\n",
      "epoch 8248: loss = 0.0969\n",
      "epoch 8249: loss = 0.1254\n",
      "epoch 8250: loss = 0.0910\n",
      "epoch 8251: loss = 0.1110\n",
      "epoch 8252: loss = 0.1000\n",
      "epoch 8253: loss = 0.1289\n",
      "epoch 8254: loss = 0.0968\n",
      "epoch 8255: loss = 0.1833\n",
      "epoch 8256: loss = 0.0916\n",
      "epoch 8257: loss = 0.1160\n",
      "epoch 8258: loss = 0.1602\n",
      "epoch 8259: loss = 0.0632\n",
      "epoch 8260: loss = 0.1054\n",
      "epoch 8261: loss = 0.1571\n",
      "epoch 8262: loss = 0.1755\n",
      "epoch 8263: loss = 0.0888\n",
      "epoch 8264: loss = 0.1781\n",
      "epoch 8265: loss = 0.1038\n",
      "epoch 8266: loss = 0.1040\n",
      "epoch 8267: loss = 0.0682\n",
      "epoch 8268: loss = 0.1413\n",
      "epoch 8269: loss = 0.1418\n",
      "epoch 8270: loss = 0.0650\n",
      "epoch 8271: loss = 0.1828\n",
      "epoch 8272: loss = 0.1965\n",
      "epoch 8273: loss = 0.1914\n",
      "epoch 8274: loss = 0.1467\n",
      "epoch 8275: loss = 0.1250\n",
      "epoch 8276: loss = 0.1449\n",
      "epoch 8277: loss = 0.1637\n",
      "epoch 8278: loss = 0.0573\n",
      "epoch 8279: loss = 0.1471\n",
      "epoch 8280: loss = 0.1370\n",
      "epoch 8281: loss = 0.1010\n",
      "epoch 8282: loss = 0.0401\n",
      "epoch 8283: loss = 0.1410\n",
      "epoch 8284: loss = 0.1195\n",
      "epoch 8285: loss = 0.0583\n",
      "epoch 8286: loss = 0.1128\n",
      "epoch 8287: loss = 0.0635\n",
      "epoch 8288: loss = 0.0807\n",
      "epoch 8289: loss = 0.0813\n",
      "epoch 8290: loss = 0.1419\n",
      "epoch 8291: loss = 0.1460\n",
      "epoch 8292: loss = 0.1514\n",
      "epoch 8293: loss = 0.1173\n",
      "epoch 8294: loss = 0.1253\n",
      "epoch 8295: loss = 0.0748\n",
      "epoch 8296: loss = 0.1132\n",
      "epoch 8297: loss = 0.0784\n",
      "epoch 8298: loss = 0.1054\n",
      "epoch 8299: loss = 0.0989\n",
      "epoch 8300: loss = 0.1690\n",
      "epoch 8301: loss = 0.0992\n",
      "epoch 8302: loss = 0.1592\n",
      "epoch 8303: loss = 0.1789\n",
      "epoch 8304: loss = 0.0865\n",
      "epoch 8305: loss = 0.0286\n",
      "epoch 8306: loss = 0.1227\n",
      "epoch 8307: loss = 0.1424\n",
      "epoch 8308: loss = 0.1541\n",
      "epoch 8309: loss = 0.0906\n",
      "epoch 8310: loss = 0.1357\n",
      "epoch 8311: loss = 0.1310\n",
      "epoch 8312: loss = 0.1426\n",
      "epoch 8313: loss = 0.0517\n",
      "epoch 8314: loss = 0.1245\n",
      "epoch 8315: loss = 0.0155\n",
      "epoch 8316: loss = 0.1078\n",
      "epoch 8317: loss = 0.1517\n",
      "epoch 8318: loss = 0.0943\n",
      "epoch 8319: loss = 0.0653\n",
      "epoch 8320: loss = 0.1527\n",
      "epoch 8321: loss = 0.1644\n",
      "epoch 8322: loss = 0.1396\n",
      "epoch 8323: loss = 0.1705\n",
      "epoch 8324: loss = 0.1573\n",
      "epoch 8325: loss = 0.0326\n",
      "epoch 8326: loss = 0.1545\n",
      "epoch 8327: loss = 0.1197\n",
      "epoch 8328: loss = 0.1183\n",
      "epoch 8329: loss = 0.1365\n",
      "epoch 8330: loss = 0.1126\n",
      "epoch 8331: loss = 0.1198\n",
      "epoch 8332: loss = 0.1259\n",
      "epoch 8333: loss = 0.1719\n",
      "epoch 8334: loss = 0.1234\n",
      "epoch 8335: loss = 0.0670\n",
      "epoch 8336: loss = 0.0434\n",
      "epoch 8337: loss = 0.1316\n",
      "epoch 8338: loss = 0.1510\n",
      "epoch 8339: loss = 0.1492\n",
      "epoch 8340: loss = 0.0096\n",
      "epoch 8341: loss = 0.1466\n",
      "epoch 8342: loss = 0.1932\n",
      "epoch 8343: loss = 0.0815\n",
      "epoch 8344: loss = 0.0801\n",
      "epoch 8345: loss = 0.1202\n",
      "epoch 8346: loss = 0.1152\n",
      "epoch 8347: loss = 0.1270\n",
      "epoch 8348: loss = 0.1412\n",
      "epoch 8349: loss = 0.1056\n",
      "epoch 8350: loss = 0.0901\n",
      "epoch 8351: loss = 0.1630\n",
      "epoch 8352: loss = 0.0409\n",
      "epoch 8353: loss = 0.1129\n",
      "epoch 8354: loss = 0.1168\n",
      "epoch 8355: loss = 0.0557\n",
      "epoch 8356: loss = 0.1516\n",
      "epoch 8357: loss = 0.1539\n",
      "epoch 8358: loss = 0.1260\n",
      "epoch 8359: loss = 0.1696\n",
      "epoch 8360: loss = 0.0522\n",
      "epoch 8361: loss = 0.0328\n",
      "epoch 8362: loss = 0.1240\n",
      "epoch 8363: loss = 0.1292\n",
      "epoch 8364: loss = 0.1127\n",
      "epoch 8365: loss = 0.0960\n",
      "epoch 8366: loss = 0.1385\n",
      "epoch 8367: loss = 0.1298\n",
      "epoch 8368: loss = 0.1489\n",
      "epoch 8369: loss = 0.1664\n",
      "epoch 8370: loss = 0.0813\n",
      "epoch 8371: loss = 0.1483\n",
      "epoch 8372: loss = 0.1686\n",
      "epoch 8373: loss = 0.1747\n",
      "epoch 8374: loss = 0.0591\n",
      "epoch 8375: loss = 0.0735\n",
      "epoch 8376: loss = 0.0658\n",
      "epoch 8377: loss = 0.1601\n",
      "epoch 8378: loss = 0.0530\n",
      "epoch 8379: loss = 0.1222\n",
      "epoch 8380: loss = 0.1232\n",
      "epoch 8381: loss = 0.1047\n",
      "epoch 8382: loss = 0.0880\n",
      "epoch 8383: loss = 0.0971\n",
      "epoch 8384: loss = 0.1019\n",
      "epoch 8385: loss = 0.1202\n",
      "epoch 8386: loss = 0.0596\n",
      "epoch 8387: loss = 0.1584\n",
      "epoch 8388: loss = 0.0406\n",
      "epoch 8389: loss = 0.1801\n",
      "epoch 8390: loss = 0.1752\n",
      "epoch 8391: loss = 0.1227\n",
      "epoch 8392: loss = 0.1193\n",
      "epoch 8393: loss = 0.1207\n",
      "epoch 8394: loss = 0.1031\n",
      "epoch 8395: loss = 0.0864\n",
      "epoch 8396: loss = 0.0896\n",
      "epoch 8397: loss = 0.1314\n",
      "epoch 8398: loss = 0.0872\n",
      "epoch 8399: loss = 0.0477\n",
      "epoch 8400: loss = 0.0690\n",
      "epoch 8401: loss = 0.1393\n",
      "epoch 8402: loss = 0.1105\n",
      "epoch 8403: loss = 0.0649\n",
      "epoch 8404: loss = 0.1054\n",
      "epoch 8405: loss = 0.1117\n",
      "epoch 8406: loss = 0.0111\n",
      "epoch 8407: loss = 0.0797\n",
      "epoch 8408: loss = 0.1051\n",
      "epoch 8409: loss = 0.0849\n",
      "epoch 8410: loss = 0.1045\n",
      "epoch 8411: loss = 0.1475\n",
      "epoch 8412: loss = 0.1141\n",
      "epoch 8413: loss = 0.1621\n",
      "epoch 8414: loss = 0.0509\n",
      "epoch 8415: loss = 0.1222\n",
      "epoch 8416: loss = 0.1201\n",
      "epoch 8417: loss = 0.0966\n",
      "epoch 8418: loss = 0.1106\n",
      "epoch 8419: loss = 0.0981\n",
      "epoch 8420: loss = 0.1408\n",
      "epoch 8421: loss = 0.1561\n",
      "epoch 8422: loss = 0.1031\n",
      "epoch 8423: loss = 0.1399\n",
      "epoch 8424: loss = 0.1267\n",
      "epoch 8425: loss = 0.0628\n",
      "epoch 8426: loss = 0.1026\n",
      "epoch 8427: loss = 0.0971\n",
      "epoch 8428: loss = 0.1091\n",
      "epoch 8429: loss = 0.1007\n",
      "epoch 8430: loss = 0.1579\n",
      "epoch 8431: loss = 0.0685\n",
      "epoch 8432: loss = 0.0248\n",
      "epoch 8433: loss = 0.1431\n",
      "epoch 8434: loss = 0.0874\n",
      "epoch 8435: loss = 0.1662\n",
      "epoch 8436: loss = 0.0732\n",
      "epoch 8437: loss = 0.0713\n",
      "epoch 8438: loss = 0.0702\n",
      "epoch 8439: loss = 0.0715\n",
      "epoch 8440: loss = 0.0466\n",
      "epoch 8441: loss = 0.1483\n",
      "epoch 8442: loss = 0.0649\n",
      "epoch 8443: loss = 0.0724\n",
      "epoch 8444: loss = 0.0807\n",
      "epoch 8445: loss = 0.1361\n",
      "epoch 8446: loss = 0.1676\n",
      "epoch 8447: loss = 0.1230\n",
      "epoch 8448: loss = 0.1513\n",
      "epoch 8449: loss = 0.1387\n",
      "epoch 8450: loss = 0.0500\n",
      "epoch 8451: loss = 0.1198\n",
      "epoch 8452: loss = 0.1243\n",
      "epoch 8453: loss = 0.1615\n",
      "epoch 8454: loss = 0.1342\n",
      "epoch 8455: loss = 0.1549\n",
      "epoch 8456: loss = 0.1034\n",
      "epoch 8457: loss = 0.1260\n",
      "epoch 8458: loss = 0.1280\n",
      "epoch 8459: loss = 0.1323\n",
      "epoch 8460: loss = 0.0498\n",
      "epoch 8461: loss = 0.0815\n",
      "epoch 8462: loss = 0.0938\n",
      "epoch 8463: loss = 0.0848\n",
      "epoch 8464: loss = 0.1703\n",
      "epoch 8465: loss = 0.1704\n",
      "epoch 8466: loss = 0.1874\n",
      "epoch 8467: loss = 0.1724\n",
      "epoch 8468: loss = 0.0312\n",
      "epoch 8469: loss = 0.1094\n",
      "epoch 8470: loss = 0.0625\n",
      "epoch 8471: loss = 0.0704\n",
      "epoch 8472: loss = 0.0803\n",
      "epoch 8473: loss = 0.1054\n",
      "epoch 8474: loss = 0.1422\n",
      "epoch 8475: loss = 0.1111\n",
      "epoch 8476: loss = 0.1403\n",
      "epoch 8477: loss = 0.1151\n",
      "epoch 8478: loss = 0.1407\n",
      "epoch 8479: loss = 0.1244\n",
      "epoch 8480: loss = 0.1184\n",
      "epoch 8481: loss = 0.1816\n",
      "epoch 8482: loss = 0.1054\n",
      "epoch 8483: loss = 0.0707\n",
      "epoch 8484: loss = 0.0734\n",
      "epoch 8485: loss = 0.1309\n",
      "epoch 8486: loss = 0.0796\n",
      "epoch 8487: loss = 0.0925\n",
      "epoch 8488: loss = 0.1643\n",
      "epoch 8489: loss = 0.1431\n",
      "epoch 8490: loss = 0.0632\n",
      "epoch 8491: loss = 0.1075\n",
      "epoch 8492: loss = 0.1373\n",
      "epoch 8493: loss = 0.1020\n",
      "epoch 8494: loss = 0.1509\n",
      "epoch 8495: loss = 0.0806\n",
      "epoch 8496: loss = 0.0912\n",
      "epoch 8497: loss = 0.0512\n",
      "epoch 8498: loss = 0.0846\n",
      "epoch 8499: loss = 0.1910\n",
      "epoch 8500: loss = 0.1711\n",
      "epoch 8501: loss = 0.1537\n",
      "epoch 8502: loss = 0.0898\n",
      "epoch 8503: loss = 0.0952\n",
      "epoch 8504: loss = 0.0306\n",
      "epoch 8505: loss = 0.0845\n",
      "epoch 8506: loss = 0.1105\n",
      "epoch 8507: loss = 0.1078\n",
      "epoch 8508: loss = 0.1826\n",
      "epoch 8509: loss = 0.1767\n",
      "epoch 8510: loss = 0.0908\n",
      "epoch 8511: loss = 0.0835\n",
      "epoch 8512: loss = 0.1635\n",
      "epoch 8513: loss = 0.0425\n",
      "epoch 8514: loss = 0.0794\n",
      "epoch 8515: loss = 0.1169\n",
      "epoch 8516: loss = 0.0872\n",
      "epoch 8517: loss = 0.1331\n",
      "epoch 8518: loss = 0.1511\n",
      "epoch 8519: loss = 0.1319\n",
      "epoch 8520: loss = 0.0682\n",
      "epoch 8521: loss = 0.0636\n",
      "epoch 8522: loss = 0.1210\n",
      "epoch 8523: loss = 0.1138\n",
      "epoch 8524: loss = 0.0283\n",
      "epoch 8525: loss = 0.0917\n",
      "epoch 8526: loss = 0.1577\n",
      "epoch 8527: loss = 0.0897\n",
      "epoch 8528: loss = 0.1310\n",
      "epoch 8529: loss = 0.1183\n",
      "epoch 8530: loss = 0.0522\n",
      "epoch 8531: loss = 0.0520\n",
      "epoch 8532: loss = 0.1188\n",
      "epoch 8533: loss = 0.1540\n",
      "epoch 8534: loss = 0.1073\n",
      "epoch 8535: loss = 0.1062\n",
      "epoch 8536: loss = 0.1542\n",
      "epoch 8537: loss = 0.1591\n",
      "epoch 8538: loss = 0.0682\n",
      "epoch 8539: loss = 0.0823\n",
      "epoch 8540: loss = 0.1271\n",
      "epoch 8541: loss = 0.0482\n",
      "epoch 8542: loss = 0.0623\n",
      "epoch 8543: loss = 0.1153\n",
      "epoch 8544: loss = 0.1633\n",
      "epoch 8545: loss = 0.0584\n",
      "epoch 8546: loss = 0.1828\n",
      "epoch 8547: loss = 0.1446\n",
      "epoch 8548: loss = 0.0611\n",
      "epoch 8549: loss = 0.1595\n",
      "epoch 8550: loss = 0.1569\n",
      "epoch 8551: loss = 0.1212\n",
      "epoch 8552: loss = 0.1273\n",
      "epoch 8553: loss = 0.1286\n",
      "epoch 8554: loss = 0.1313\n",
      "epoch 8555: loss = 0.1699\n",
      "epoch 8556: loss = 0.1177\n",
      "epoch 8557: loss = 0.1647\n",
      "epoch 8558: loss = 0.0830\n",
      "epoch 8559: loss = 0.0912\n",
      "epoch 8560: loss = 0.1172\n",
      "epoch 8561: loss = 0.1199\n",
      "epoch 8562: loss = 0.1768\n",
      "epoch 8563: loss = 0.1159\n",
      "epoch 8564: loss = 0.1089\n",
      "epoch 8565: loss = 0.0518\n",
      "epoch 8566: loss = 0.1653\n",
      "epoch 8567: loss = 0.1023\n",
      "epoch 8568: loss = 0.1636\n",
      "epoch 8569: loss = 0.1117\n",
      "epoch 8570: loss = 0.0460\n",
      "epoch 8571: loss = 0.0620\n",
      "epoch 8572: loss = 0.1533\n",
      "epoch 8573: loss = 0.1172\n",
      "epoch 8574: loss = 0.1302\n",
      "epoch 8575: loss = 0.0849\n",
      "epoch 8576: loss = 0.0667\n",
      "epoch 8577: loss = 0.0826\n",
      "epoch 8578: loss = 0.1808\n",
      "epoch 8579: loss = 0.0826\n",
      "epoch 8580: loss = 0.1474\n",
      "epoch 8581: loss = 0.1125\n",
      "epoch 8582: loss = 0.0689\n",
      "epoch 8583: loss = 0.0910\n",
      "epoch 8584: loss = 0.0801\n",
      "epoch 8585: loss = 0.1158\n",
      "epoch 8586: loss = 0.0501\n",
      "epoch 8587: loss = 0.0977\n",
      "epoch 8588: loss = 0.1140\n",
      "epoch 8589: loss = 0.1777\n",
      "epoch 8590: loss = 0.0993\n",
      "epoch 8591: loss = 0.1586\n",
      "epoch 8592: loss = 0.0442\n",
      "epoch 8593: loss = 0.1068\n",
      "epoch 8594: loss = 0.1285\n",
      "epoch 8595: loss = 0.1405\n",
      "epoch 8596: loss = 0.1634\n",
      "epoch 8597: loss = 0.1102\n",
      "epoch 8598: loss = 0.1824\n",
      "epoch 8599: loss = 0.0711\n",
      "epoch 8600: loss = 0.1422\n",
      "epoch 8601: loss = 0.1570\n",
      "epoch 8602: loss = 0.1325\n",
      "epoch 8603: loss = 0.0447\n",
      "epoch 8604: loss = 0.2016\n",
      "epoch 8605: loss = 0.1865\n",
      "epoch 8606: loss = 0.0661\n",
      "epoch 8607: loss = 0.0675\n",
      "epoch 8608: loss = 0.0755\n",
      "epoch 8609: loss = 0.1167\n",
      "epoch 8610: loss = 0.0690\n",
      "epoch 8611: loss = 0.0380\n",
      "epoch 8612: loss = 0.1043\n",
      "epoch 8613: loss = 0.0998\n",
      "epoch 8614: loss = 0.0502\n",
      "epoch 8615: loss = 0.1754\n",
      "epoch 8616: loss = 0.1048\n",
      "epoch 8617: loss = 0.1823\n",
      "epoch 8618: loss = 0.1480\n",
      "epoch 8619: loss = 0.1033\n",
      "epoch 8620: loss = 0.1986\n",
      "epoch 8621: loss = 0.1930\n",
      "epoch 8622: loss = 0.1132\n",
      "epoch 8623: loss = 0.1474\n",
      "epoch 8624: loss = 0.1148\n",
      "epoch 8625: loss = 0.0978\n",
      "epoch 8626: loss = 0.1832\n",
      "epoch 8627: loss = 0.1033\n",
      "epoch 8628: loss = 0.1337\n",
      "epoch 8629: loss = 0.1535\n",
      "epoch 8630: loss = 0.1133\n",
      "epoch 8631: loss = 0.1231\n",
      "epoch 8632: loss = 0.0677\n",
      "epoch 8633: loss = 0.0328\n",
      "epoch 8634: loss = 0.0683\n",
      "epoch 8635: loss = 0.1022\n",
      "epoch 8636: loss = 0.1418\n",
      "epoch 8637: loss = 0.0795\n",
      "epoch 8638: loss = 0.0909\n",
      "epoch 8639: loss = 0.1197\n",
      "epoch 8640: loss = 0.1113\n",
      "epoch 8641: loss = 0.0588\n",
      "epoch 8642: loss = 0.0856\n",
      "epoch 8643: loss = 0.1310\n",
      "epoch 8644: loss = 0.1120\n",
      "epoch 8645: loss = 0.1313\n",
      "epoch 8646: loss = 0.0902\n",
      "epoch 8647: loss = 0.1214\n",
      "epoch 8648: loss = 0.0944\n",
      "epoch 8649: loss = 0.0874\n",
      "epoch 8650: loss = 0.1037\n",
      "epoch 8651: loss = 0.1363\n",
      "epoch 8652: loss = 0.0835\n",
      "epoch 8653: loss = 0.0450\n",
      "epoch 8654: loss = 0.0442\n",
      "epoch 8655: loss = 0.0620\n",
      "epoch 8656: loss = 0.1022\n",
      "epoch 8657: loss = 0.0414\n",
      "epoch 8658: loss = 0.0981\n",
      "epoch 8659: loss = 0.1168\n",
      "epoch 8660: loss = 0.1379\n",
      "epoch 8661: loss = 0.1248\n",
      "epoch 8662: loss = 0.0976\n",
      "epoch 8663: loss = 0.0368\n",
      "epoch 8664: loss = 0.1048\n",
      "epoch 8665: loss = 0.0877\n",
      "epoch 8666: loss = 0.1603\n",
      "epoch 8667: loss = 0.0608\n",
      "epoch 8668: loss = 0.1404\n",
      "epoch 8669: loss = 0.1757\n",
      "epoch 8670: loss = 0.1161\n",
      "epoch 8671: loss = 0.0909\n",
      "epoch 8672: loss = 0.1299\n",
      "epoch 8673: loss = 0.0839\n",
      "epoch 8674: loss = 0.1527\n",
      "epoch 8675: loss = 0.0338\n",
      "epoch 8676: loss = 0.0500\n",
      "epoch 8677: loss = 0.1923\n",
      "epoch 8678: loss = 0.1751\n",
      "epoch 8679: loss = 0.0904\n",
      "epoch 8680: loss = 0.0610\n",
      "epoch 8681: loss = 0.0865\n",
      "epoch 8682: loss = 0.1083\n",
      "epoch 8683: loss = 0.1593\n",
      "epoch 8684: loss = 0.1607\n",
      "epoch 8685: loss = 0.0988\n",
      "epoch 8686: loss = 0.1366\n",
      "epoch 8687: loss = 0.0661\n",
      "epoch 8688: loss = 0.1446\n",
      "epoch 8689: loss = 0.1491\n",
      "epoch 8690: loss = 0.0983\n",
      "epoch 8691: loss = 0.0943\n",
      "epoch 8692: loss = 0.1161\n",
      "epoch 8693: loss = 0.1024\n",
      "epoch 8694: loss = 0.1839\n",
      "epoch 8695: loss = 0.1174\n",
      "epoch 8696: loss = 0.1497\n",
      "epoch 8697: loss = 0.1419\n",
      "epoch 8698: loss = 0.1323\n",
      "epoch 8699: loss = 0.1678\n",
      "epoch 8700: loss = 0.1011\n",
      "epoch 8701: loss = 0.0301\n",
      "epoch 8702: loss = 0.0995\n",
      "epoch 8703: loss = 0.0430\n",
      "epoch 8704: loss = 0.0733\n",
      "epoch 8705: loss = 0.0864\n",
      "epoch 8706: loss = 0.1576\n",
      "epoch 8707: loss = 0.1791\n",
      "epoch 8708: loss = 0.1431\n",
      "epoch 8709: loss = 0.1128\n",
      "epoch 8710: loss = 0.1200\n",
      "epoch 8711: loss = 0.1337\n",
      "epoch 8712: loss = 0.1258\n",
      "epoch 8713: loss = 0.0628\n",
      "epoch 8714: loss = 0.0187\n",
      "epoch 8715: loss = 0.0964\n",
      "epoch 8716: loss = 0.1190\n",
      "epoch 8717: loss = 0.0894\n",
      "epoch 8718: loss = 0.1298\n",
      "epoch 8719: loss = 0.1680\n",
      "epoch 8720: loss = 0.1507\n",
      "epoch 8721: loss = 0.1755\n",
      "epoch 8722: loss = 0.1063\n",
      "epoch 8723: loss = 0.1217\n",
      "epoch 8724: loss = 0.1057\n",
      "epoch 8725: loss = 0.1169\n",
      "epoch 8726: loss = 0.1493\n",
      "epoch 8727: loss = 0.0773\n",
      "epoch 8728: loss = 0.0462\n",
      "epoch 8729: loss = 0.1459\n",
      "epoch 8730: loss = 0.0979\n",
      "epoch 8731: loss = 0.1458\n",
      "epoch 8732: loss = 0.0435\n",
      "epoch 8733: loss = 0.0749\n",
      "epoch 8734: loss = 0.0870\n",
      "epoch 8735: loss = 0.1647\n",
      "epoch 8736: loss = 0.1305\n",
      "epoch 8737: loss = 0.0790\n",
      "epoch 8738: loss = 0.0318\n",
      "epoch 8739: loss = 0.0641\n",
      "epoch 8740: loss = 0.1288\n",
      "epoch 8741: loss = 0.1204\n",
      "epoch 8742: loss = 0.1589\n",
      "epoch 8743: loss = 0.0773\n",
      "epoch 8744: loss = 0.0675\n",
      "epoch 8745: loss = 0.1633\n",
      "epoch 8746: loss = 0.1046\n",
      "epoch 8747: loss = 0.0988\n",
      "epoch 8748: loss = 0.0676\n",
      "epoch 8749: loss = 0.1510\n",
      "epoch 8750: loss = 0.1206\n",
      "epoch 8751: loss = 0.1270\n",
      "epoch 8752: loss = 0.1020\n",
      "epoch 8753: loss = 0.0263\n",
      "epoch 8754: loss = 0.0653\n",
      "epoch 8755: loss = 0.1087\n",
      "epoch 8756: loss = 0.1038\n",
      "epoch 8757: loss = 0.0933\n",
      "epoch 8758: loss = 0.1502\n",
      "epoch 8759: loss = 0.1470\n",
      "epoch 8760: loss = 0.1672\n",
      "epoch 8761: loss = 0.0648\n",
      "epoch 8762: loss = 0.0950\n",
      "epoch 8763: loss = 0.1803\n",
      "epoch 8764: loss = 0.1971\n",
      "epoch 8765: loss = 0.1524\n",
      "epoch 8766: loss = 0.1132\n",
      "epoch 8767: loss = 0.0908\n",
      "epoch 8768: loss = 0.1543\n",
      "epoch 8769: loss = 0.1198\n",
      "epoch 8770: loss = 0.0508\n",
      "epoch 8771: loss = 0.0772\n",
      "epoch 8772: loss = 0.1060\n",
      "epoch 8773: loss = 0.1434\n",
      "epoch 8774: loss = 0.1341\n",
      "epoch 8775: loss = 0.0864\n",
      "epoch 8776: loss = 0.1357\n",
      "epoch 8777: loss = 0.0825\n",
      "epoch 8778: loss = 0.0577\n",
      "epoch 8779: loss = 0.1139\n",
      "epoch 8780: loss = 0.1247\n",
      "epoch 8781: loss = 0.0869\n",
      "epoch 8782: loss = 0.1674\n",
      "epoch 8783: loss = 0.1205\n",
      "epoch 8784: loss = 0.0847\n",
      "epoch 8785: loss = 0.1130\n",
      "epoch 8786: loss = 0.1766\n",
      "epoch 8787: loss = 0.1513\n",
      "epoch 8788: loss = 0.0883\n",
      "epoch 8789: loss = 0.1134\n",
      "epoch 8790: loss = 0.1274\n",
      "epoch 8791: loss = 0.0911\n",
      "epoch 8792: loss = 0.1491\n",
      "epoch 8793: loss = 0.1168\n",
      "epoch 8794: loss = 0.0463\n",
      "epoch 8795: loss = 0.1195\n",
      "epoch 8796: loss = 0.1795\n",
      "epoch 8797: loss = 0.1612\n",
      "epoch 8798: loss = 0.0633\n",
      "epoch 8799: loss = 0.1581\n",
      "epoch 8800: loss = 0.1813\n",
      "epoch 8801: loss = 0.1407\n",
      "epoch 8802: loss = 0.0824\n",
      "epoch 8803: loss = 0.1013\n",
      "epoch 8804: loss = 0.1290\n",
      "epoch 8805: loss = 0.1128\n",
      "epoch 8806: loss = 0.1419\n",
      "epoch 8807: loss = 0.1169\n",
      "epoch 8808: loss = 0.1427\n",
      "epoch 8809: loss = 0.1577\n",
      "epoch 8810: loss = 0.0491\n",
      "epoch 8811: loss = 0.1761\n",
      "epoch 8812: loss = 0.1042\n",
      "epoch 8813: loss = 0.0901\n",
      "epoch 8814: loss = 0.1124\n",
      "epoch 8815: loss = 0.1306\n",
      "epoch 8816: loss = 0.0693\n",
      "epoch 8817: loss = 0.0612\n",
      "epoch 8818: loss = 0.0913\n",
      "epoch 8819: loss = 0.1326\n",
      "epoch 8820: loss = 0.0904\n",
      "epoch 8821: loss = 0.1085\n",
      "epoch 8822: loss = 0.0816\n",
      "epoch 8823: loss = 0.1295\n",
      "epoch 8824: loss = 0.1525\n",
      "epoch 8825: loss = 0.0870\n",
      "epoch 8826: loss = 0.1507\n",
      "epoch 8827: loss = 0.0890\n",
      "epoch 8828: loss = 0.0711\n",
      "epoch 8829: loss = 0.0828\n",
      "epoch 8830: loss = 0.0878\n",
      "epoch 8831: loss = 0.1265\n",
      "epoch 8832: loss = 0.1322\n",
      "epoch 8833: loss = 0.0395\n",
      "epoch 8834: loss = 0.0657\n",
      "epoch 8835: loss = 0.0955\n",
      "epoch 8836: loss = 0.0660\n",
      "epoch 8837: loss = 0.0710\n",
      "epoch 8838: loss = 0.0385\n",
      "epoch 8839: loss = 0.1166\n",
      "epoch 8840: loss = 0.1824\n",
      "epoch 8841: loss = 0.1162\n",
      "epoch 8842: loss = 0.1820\n",
      "epoch 8843: loss = 0.1645\n",
      "epoch 8844: loss = 0.1439\n",
      "epoch 8845: loss = 0.0876\n",
      "epoch 8846: loss = 0.0685\n",
      "epoch 8847: loss = 0.1276\n",
      "epoch 8848: loss = 0.1535\n",
      "epoch 8849: loss = 0.1810\n",
      "epoch 8850: loss = 0.1423\n",
      "epoch 8851: loss = 0.1458\n",
      "epoch 8852: loss = 0.1546\n",
      "epoch 8853: loss = 0.1010\n",
      "epoch 8854: loss = 0.1232\n",
      "epoch 8855: loss = 0.1273\n",
      "epoch 8856: loss = 0.1552\n",
      "epoch 8857: loss = 0.1353\n",
      "epoch 8858: loss = 0.1672\n",
      "epoch 8859: loss = 0.0963\n",
      "epoch 8860: loss = 0.0996\n",
      "epoch 8861: loss = 0.1108\n",
      "epoch 8862: loss = 0.0990\n",
      "epoch 8863: loss = 0.0411\n",
      "epoch 8864: loss = 0.0836\n",
      "epoch 8865: loss = 0.1920\n",
      "epoch 8866: loss = 0.1705\n",
      "epoch 8867: loss = 0.1767\n",
      "epoch 8868: loss = 0.0185\n",
      "epoch 8869: loss = 0.1180\n",
      "epoch 8870: loss = 0.1303\n",
      "epoch 8871: loss = 0.0355\n",
      "epoch 8872: loss = 0.1061\n",
      "epoch 8873: loss = 0.0556\n",
      "epoch 8874: loss = 0.1007\n",
      "epoch 8875: loss = 0.0245\n",
      "epoch 8876: loss = 0.1332\n",
      "epoch 8877: loss = 0.0938\n",
      "epoch 8878: loss = 0.0404\n",
      "epoch 8879: loss = 0.1755\n",
      "epoch 8880: loss = 0.1100\n",
      "epoch 8881: loss = 0.0928\n",
      "epoch 8882: loss = 0.0969\n",
      "epoch 8883: loss = 0.1145\n",
      "epoch 8884: loss = 0.1012\n",
      "epoch 8885: loss = 0.1539\n",
      "epoch 8886: loss = 0.0762\n",
      "epoch 8887: loss = 0.0642\n",
      "epoch 8888: loss = 0.0963\n",
      "epoch 8889: loss = 0.1210\n",
      "epoch 8890: loss = 0.0373\n",
      "epoch 8891: loss = 0.1206\n",
      "epoch 8892: loss = 0.1174\n",
      "epoch 8893: loss = 0.0710\n",
      "epoch 8894: loss = 0.1316\n",
      "epoch 8895: loss = 0.1125\n",
      "epoch 8896: loss = 0.1566\n",
      "epoch 8897: loss = 0.0435\n",
      "epoch 8898: loss = 0.0938\n",
      "epoch 8899: loss = 0.1264\n",
      "epoch 8900: loss = 0.1411\n",
      "epoch 8901: loss = 0.1769\n",
      "epoch 8902: loss = 0.1039\n",
      "epoch 8903: loss = 0.0673\n",
      "epoch 8904: loss = 0.1008\n",
      "epoch 8905: loss = 0.1038\n",
      "epoch 8906: loss = 0.0944\n",
      "epoch 8907: loss = 0.0637\n",
      "epoch 8908: loss = 0.1148\n",
      "epoch 8909: loss = 0.1003\n",
      "epoch 8910: loss = 0.1099\n",
      "epoch 8911: loss = 0.1538\n",
      "epoch 8912: loss = 0.0594\n",
      "epoch 8913: loss = 0.1246\n",
      "epoch 8914: loss = 0.1047\n",
      "epoch 8915: loss = 0.1485\n",
      "epoch 8916: loss = 0.0743\n",
      "epoch 8917: loss = 0.0854\n",
      "epoch 8918: loss = 0.1752\n",
      "epoch 8919: loss = 0.0959\n",
      "epoch 8920: loss = 0.1273\n",
      "epoch 8921: loss = 0.0840\n",
      "epoch 8922: loss = 0.0760\n",
      "epoch 8923: loss = 0.0486\n",
      "epoch 8924: loss = 0.1003\n",
      "epoch 8925: loss = 0.0230\n",
      "epoch 8926: loss = 0.0330\n",
      "epoch 8927: loss = 0.0589\n",
      "epoch 8928: loss = 0.1342\n",
      "epoch 8929: loss = 0.0674\n",
      "epoch 8930: loss = 0.0433\n",
      "epoch 8931: loss = 0.0885\n",
      "epoch 8932: loss = 0.0710\n",
      "epoch 8933: loss = 0.1551\n",
      "epoch 8934: loss = 0.0923\n",
      "epoch 8935: loss = 0.1175\n",
      "epoch 8936: loss = 0.1515\n",
      "epoch 8937: loss = 0.2004\n",
      "epoch 8938: loss = 0.1729\n",
      "epoch 8939: loss = 0.0491\n",
      "epoch 8940: loss = 0.0750\n",
      "epoch 8941: loss = 0.1050\n",
      "epoch 8942: loss = 0.0771\n",
      "epoch 8943: loss = 0.0490\n",
      "epoch 8944: loss = 0.1461\n",
      "epoch 8945: loss = 0.1704\n",
      "epoch 8946: loss = 0.0667\n",
      "epoch 8947: loss = 0.0786\n",
      "epoch 8948: loss = 0.1409\n",
      "epoch 8949: loss = 0.1151\n",
      "epoch 8950: loss = 0.0839\n",
      "epoch 8951: loss = 0.0819\n",
      "epoch 8952: loss = 0.0811\n",
      "epoch 8953: loss = 0.0855\n",
      "epoch 8954: loss = 0.1246\n",
      "epoch 8955: loss = 0.1262\n",
      "epoch 8956: loss = 0.1421\n",
      "epoch 8957: loss = 0.0852\n",
      "epoch 8958: loss = 0.0916\n",
      "epoch 8959: loss = 0.0758\n",
      "epoch 8960: loss = 0.1103\n",
      "epoch 8961: loss = 0.1297\n",
      "epoch 8962: loss = 0.1077\n",
      "epoch 8963: loss = 0.1025\n",
      "epoch 8964: loss = 0.1028\n",
      "epoch 8965: loss = 0.1854\n",
      "epoch 8966: loss = 0.0690\n",
      "epoch 8967: loss = 0.0591\n",
      "epoch 8968: loss = 0.1001\n",
      "epoch 8969: loss = 0.1117\n",
      "epoch 8970: loss = 0.1737\n",
      "epoch 8971: loss = 0.1161\n",
      "epoch 8972: loss = 0.0695\n",
      "epoch 8973: loss = 0.0870\n",
      "epoch 8974: loss = 0.0468\n",
      "epoch 8975: loss = 0.0983\n",
      "epoch 8976: loss = 0.1516\n",
      "epoch 8977: loss = 0.0776\n",
      "epoch 8978: loss = 0.0863\n",
      "epoch 8979: loss = 0.1391\n",
      "epoch 8980: loss = 0.0965\n",
      "epoch 8981: loss = 0.0960\n",
      "epoch 8982: loss = 0.0712\n",
      "epoch 8983: loss = 0.0711\n",
      "epoch 8984: loss = 0.0853\n",
      "epoch 8985: loss = 0.0956\n",
      "epoch 8986: loss = 0.0700\n",
      "epoch 8987: loss = 0.1778\n",
      "epoch 8988: loss = 0.0558\n",
      "epoch 8989: loss = 0.0333\n",
      "epoch 8990: loss = 0.0770\n",
      "epoch 8991: loss = 0.0556\n",
      "epoch 8992: loss = 0.1522\n",
      "epoch 8993: loss = 0.1567\n",
      "epoch 8994: loss = 0.1460\n",
      "epoch 8995: loss = 0.1624\n",
      "epoch 8996: loss = 0.1609\n",
      "epoch 8997: loss = 0.0839\n",
      "epoch 8998: loss = 0.1107\n",
      "epoch 8999: loss = 0.1955\n",
      "epoch 9000: loss = 0.1830\n",
      "epoch 9001: loss = 0.1640\n",
      "epoch 9002: loss = 0.0532\n",
      "epoch 9003: loss = 0.0459\n",
      "epoch 9004: loss = 0.0613\n",
      "epoch 9005: loss = 0.1007\n",
      "epoch 9006: loss = 0.0934\n",
      "epoch 9007: loss = 0.1095\n",
      "epoch 9008: loss = 0.1638\n",
      "epoch 9009: loss = 0.1752\n",
      "epoch 9010: loss = 0.1204\n",
      "epoch 9011: loss = 0.1678\n",
      "epoch 9012: loss = 0.0911\n",
      "epoch 9013: loss = 0.1069\n",
      "epoch 9014: loss = 0.1528\n",
      "epoch 9015: loss = 0.0436\n",
      "epoch 9016: loss = 0.0382\n",
      "epoch 9017: loss = 0.1286\n",
      "epoch 9018: loss = 0.1261\n",
      "epoch 9019: loss = 0.1714\n",
      "epoch 9020: loss = 0.0145\n",
      "epoch 9021: loss = 0.0673\n",
      "epoch 9022: loss = 0.0894\n",
      "epoch 9023: loss = 0.0631\n",
      "epoch 9024: loss = 0.1189\n",
      "epoch 9025: loss = 0.1515\n",
      "epoch 9026: loss = 0.0420\n",
      "epoch 9027: loss = 0.0667\n",
      "epoch 9028: loss = 0.1050\n",
      "epoch 9029: loss = 0.0800\n",
      "epoch 9030: loss = 0.1218\n",
      "epoch 9031: loss = 0.0909\n",
      "epoch 9032: loss = 0.0376\n",
      "epoch 9033: loss = 0.1171\n",
      "epoch 9034: loss = 0.0781\n",
      "epoch 9035: loss = 0.1662\n",
      "epoch 9036: loss = 0.0884\n",
      "epoch 9037: loss = 0.1188\n",
      "epoch 9038: loss = 0.0460\n",
      "epoch 9039: loss = 0.1096\n",
      "epoch 9040: loss = 0.0269\n",
      "epoch 9041: loss = 0.0146\n",
      "epoch 9042: loss = 0.1405\n",
      "epoch 9043: loss = 0.1068\n",
      "epoch 9044: loss = 0.0224\n",
      "epoch 9045: loss = 0.0845\n",
      "epoch 9046: loss = 0.0494\n",
      "epoch 9047: loss = 0.1270\n",
      "epoch 9048: loss = 0.1539\n",
      "epoch 9049: loss = 0.1400\n",
      "epoch 9050: loss = 0.0565\n",
      "epoch 9051: loss = 0.0375\n",
      "epoch 9052: loss = 0.0067\n",
      "epoch 9053: loss = 0.0998\n",
      "epoch 9054: loss = 0.0582\n",
      "epoch 9055: loss = 0.1054\n",
      "epoch 9056: loss = 0.1175\n",
      "epoch 9057: loss = 0.0365\n",
      "epoch 9058: loss = 0.0522\n",
      "epoch 9059: loss = 0.1656\n",
      "epoch 9060: loss = 0.1116\n",
      "epoch 9061: loss = 0.1068\n",
      "epoch 9062: loss = 0.1802\n",
      "epoch 9063: loss = 0.1456\n",
      "epoch 9064: loss = 0.1659\n",
      "epoch 9065: loss = 0.1809\n",
      "epoch 9066: loss = 0.0250\n",
      "epoch 9067: loss = 0.1363\n",
      "epoch 9068: loss = 0.1104\n",
      "epoch 9069: loss = 0.1706\n",
      "epoch 9070: loss = 0.1080\n",
      "epoch 9071: loss = 0.1016\n",
      "epoch 9072: loss = 0.1252\n",
      "epoch 9073: loss = 0.1060\n",
      "epoch 9074: loss = 0.0593\n",
      "epoch 9075: loss = 0.1373\n",
      "epoch 9076: loss = 0.0841\n",
      "epoch 9077: loss = 0.1912\n",
      "epoch 9078: loss = 0.1867\n",
      "epoch 9079: loss = 0.1315\n",
      "epoch 9080: loss = 0.1594\n",
      "epoch 9081: loss = 0.1010\n",
      "epoch 9082: loss = 0.1444\n",
      "epoch 9083: loss = 0.0676\n",
      "epoch 9084: loss = 0.0663\n",
      "epoch 9085: loss = 0.1504\n",
      "epoch 9086: loss = 0.1697\n",
      "epoch 9087: loss = 0.1234\n",
      "epoch 9088: loss = 0.1470\n",
      "epoch 9089: loss = 0.1409\n",
      "epoch 9090: loss = 0.0905\n",
      "epoch 9091: loss = 0.0559\n",
      "epoch 9092: loss = 0.1450\n",
      "epoch 9093: loss = 0.1433\n",
      "epoch 9094: loss = 0.1065\n",
      "epoch 9095: loss = 0.0949\n",
      "epoch 9096: loss = 0.1202\n",
      "epoch 9097: loss = 0.1469\n",
      "epoch 9098: loss = 0.0849\n",
      "epoch 9099: loss = 0.1400\n",
      "epoch 9100: loss = 0.1592\n",
      "epoch 9101: loss = 0.1402\n",
      "epoch 9102: loss = 0.1298\n",
      "epoch 9103: loss = 0.0708\n",
      "epoch 9104: loss = 0.0847\n",
      "epoch 9105: loss = 0.0778\n",
      "epoch 9106: loss = 0.1582\n",
      "epoch 9107: loss = 0.1512\n",
      "epoch 9108: loss = 0.1068\n",
      "epoch 9109: loss = 0.2075\n",
      "epoch 9110: loss = 0.0946\n",
      "epoch 9111: loss = 0.0872\n",
      "epoch 9112: loss = 0.1499\n",
      "epoch 9113: loss = 0.1741\n",
      "epoch 9114: loss = 0.0702\n",
      "epoch 9115: loss = 0.1783\n",
      "epoch 9116: loss = 0.1514\n",
      "epoch 9117: loss = 0.1586\n",
      "epoch 9118: loss = 0.1838\n",
      "epoch 9119: loss = 0.0793\n",
      "epoch 9120: loss = 0.0983\n",
      "epoch 9121: loss = 0.0672\n",
      "epoch 9122: loss = 0.1301\n",
      "epoch 9123: loss = 0.0935\n",
      "epoch 9124: loss = 0.1507\n",
      "epoch 9125: loss = 0.1408\n",
      "epoch 9126: loss = 0.1572\n",
      "epoch 9127: loss = 0.0880\n",
      "epoch 9128: loss = 0.0587\n",
      "epoch 9129: loss = 0.1345\n",
      "epoch 9130: loss = 0.1824\n",
      "epoch 9131: loss = 0.1240\n",
      "epoch 9132: loss = 0.1172\n",
      "epoch 9133: loss = 0.1793\n",
      "epoch 9134: loss = 0.1374\n",
      "epoch 9135: loss = 0.0405\n",
      "epoch 9136: loss = 0.1503\n",
      "epoch 9137: loss = 0.0767\n",
      "epoch 9138: loss = 0.1077\n",
      "epoch 9139: loss = 0.1806\n",
      "epoch 9140: loss = 0.1526\n",
      "epoch 9141: loss = 0.1563\n",
      "epoch 9142: loss = 0.1119\n",
      "epoch 9143: loss = 0.0863\n",
      "epoch 9144: loss = 0.0742\n",
      "epoch 9145: loss = 0.1011\n",
      "epoch 9146: loss = 0.0426\n",
      "epoch 9147: loss = 0.0926\n",
      "epoch 9148: loss = 0.0867\n",
      "epoch 9149: loss = 0.0675\n",
      "epoch 9150: loss = 0.1146\n",
      "epoch 9151: loss = 0.1336\n",
      "epoch 9152: loss = 0.0426\n",
      "epoch 9153: loss = 0.0604\n",
      "epoch 9154: loss = 0.0251\n",
      "epoch 9155: loss = 0.0999\n",
      "epoch 9156: loss = 0.1622\n",
      "epoch 9157: loss = 0.1473\n",
      "epoch 9158: loss = 0.0905\n",
      "epoch 9159: loss = 0.1276\n",
      "epoch 9160: loss = 0.0485\n",
      "epoch 9161: loss = 0.1335\n",
      "epoch 9162: loss = 0.1645\n",
      "epoch 9163: loss = 0.1986\n",
      "epoch 9164: loss = 0.0800\n",
      "epoch 9165: loss = 0.1091\n",
      "epoch 9166: loss = 0.1295\n",
      "epoch 9167: loss = 0.1412\n",
      "epoch 9168: loss = 0.1103\n",
      "epoch 9169: loss = 0.1507\n",
      "epoch 9170: loss = 0.1367\n",
      "epoch 9171: loss = 0.1005\n",
      "epoch 9172: loss = 0.1893\n",
      "epoch 9173: loss = 0.2043\n",
      "epoch 9174: loss = 0.1413\n",
      "epoch 9175: loss = 0.0841\n",
      "epoch 9176: loss = 0.0064\n",
      "epoch 9177: loss = 0.0478\n",
      "epoch 9178: loss = 0.0468\n",
      "epoch 9179: loss = 0.1015\n",
      "epoch 9180: loss = 0.1136\n",
      "epoch 9181: loss = 0.0841\n",
      "epoch 9182: loss = 0.1489\n",
      "epoch 9183: loss = 0.1137\n",
      "epoch 9184: loss = 0.1680\n",
      "epoch 9185: loss = 0.1437\n",
      "epoch 9186: loss = 0.1374\n",
      "epoch 9187: loss = 0.0447\n",
      "epoch 9188: loss = 0.0704\n",
      "epoch 9189: loss = 0.0744\n",
      "epoch 9190: loss = 0.0618\n",
      "epoch 9191: loss = 0.1285\n",
      "epoch 9192: loss = 0.1128\n",
      "epoch 9193: loss = 0.0783\n",
      "epoch 9194: loss = 0.1374\n",
      "epoch 9195: loss = 0.1181\n",
      "epoch 9196: loss = 0.1145\n",
      "epoch 9197: loss = 0.0829\n",
      "epoch 9198: loss = 0.1280\n",
      "epoch 9199: loss = 0.1323\n",
      "epoch 9200: loss = 0.0210\n",
      "epoch 9201: loss = 0.0631\n",
      "epoch 9202: loss = 0.1001\n",
      "epoch 9203: loss = 0.0894\n",
      "epoch 9204: loss = 0.0915\n",
      "epoch 9205: loss = 0.1084\n",
      "epoch 9206: loss = 0.0461\n",
      "epoch 9207: loss = 0.1102\n",
      "epoch 9208: loss = 0.1302\n",
      "epoch 9209: loss = 0.1386\n",
      "epoch 9210: loss = 0.1152\n",
      "epoch 9211: loss = 0.0703\n",
      "epoch 9212: loss = 0.1101\n",
      "epoch 9213: loss = 0.0087\n",
      "epoch 9214: loss = 0.1590\n",
      "epoch 9215: loss = 0.1426\n",
      "epoch 9216: loss = 0.0769\n",
      "epoch 9217: loss = 0.0103\n",
      "epoch 9218: loss = 0.1181\n",
      "epoch 9219: loss = 0.0775\n",
      "epoch 9220: loss = 0.1063\n",
      "epoch 9221: loss = 0.1451\n",
      "epoch 9222: loss = 0.1663\n",
      "epoch 9223: loss = 0.1313\n",
      "epoch 9224: loss = 0.0845\n",
      "epoch 9225: loss = 0.0859\n",
      "epoch 9226: loss = 0.0148\n",
      "epoch 9227: loss = 0.1308\n",
      "epoch 9228: loss = 0.1037\n",
      "epoch 9229: loss = 0.1321\n",
      "epoch 9230: loss = 0.0954\n",
      "epoch 9231: loss = 0.0647\n",
      "epoch 9232: loss = 0.1241\n",
      "epoch 9233: loss = 0.0817\n",
      "epoch 9234: loss = 0.1063\n",
      "epoch 9235: loss = 0.0431\n",
      "epoch 9236: loss = 0.0323\n",
      "epoch 9237: loss = 0.0990\n",
      "epoch 9238: loss = 0.1124\n",
      "epoch 9239: loss = 0.0805\n",
      "epoch 9240: loss = 0.1924\n",
      "epoch 9241: loss = 0.1384\n",
      "epoch 9242: loss = 0.0618\n",
      "epoch 9243: loss = 0.0720\n",
      "epoch 9244: loss = 0.1051\n",
      "epoch 9245: loss = 0.1486\n",
      "epoch 9246: loss = 0.0842\n",
      "epoch 9247: loss = 0.0581\n",
      "epoch 9248: loss = 0.0917\n",
      "epoch 9249: loss = 0.1201\n",
      "epoch 9250: loss = 0.1811\n",
      "epoch 9251: loss = 0.1591\n",
      "epoch 9252: loss = 0.0506\n",
      "epoch 9253: loss = 0.0356\n",
      "epoch 9254: loss = 0.0806\n",
      "epoch 9255: loss = 0.0432\n",
      "epoch 9256: loss = 0.1170\n",
      "epoch 9257: loss = 0.1476\n",
      "epoch 9258: loss = 0.1037\n",
      "epoch 9259: loss = 0.0842\n",
      "epoch 9260: loss = 0.1710\n",
      "epoch 9261: loss = 0.0606\n",
      "epoch 9262: loss = 0.1671\n",
      "epoch 9263: loss = 0.0858\n",
      "epoch 9264: loss = 0.1511\n",
      "epoch 9265: loss = 0.0800\n",
      "epoch 9266: loss = 0.0412\n",
      "epoch 9267: loss = 0.0783\n",
      "epoch 9268: loss = 0.1145\n",
      "epoch 9269: loss = 0.1130\n",
      "epoch 9270: loss = 0.1130\n",
      "epoch 9271: loss = 0.1227\n",
      "epoch 9272: loss = 0.1175\n",
      "epoch 9273: loss = 0.1602\n",
      "epoch 9274: loss = 0.1556\n",
      "epoch 9275: loss = 0.0917\n",
      "epoch 9276: loss = 0.1488\n",
      "epoch 9277: loss = 0.1496\n",
      "epoch 9278: loss = 0.0801\n",
      "epoch 9279: loss = 0.1351\n",
      "epoch 9280: loss = 0.0807\n",
      "epoch 9281: loss = 0.0720\n",
      "epoch 9282: loss = 0.0714\n",
      "epoch 9283: loss = 0.1389\n",
      "epoch 9284: loss = 0.1751\n",
      "epoch 9285: loss = 0.1048\n",
      "epoch 9286: loss = 0.0585\n",
      "epoch 9287: loss = 0.0994\n",
      "epoch 9288: loss = 0.1712\n",
      "epoch 9289: loss = 0.1413\n",
      "epoch 9290: loss = 0.0955\n",
      "epoch 9291: loss = 0.0324\n",
      "epoch 9292: loss = 0.0634\n",
      "epoch 9293: loss = 0.1176\n",
      "epoch 9294: loss = 0.0513\n",
      "epoch 9295: loss = 0.0071\n",
      "epoch 9296: loss = 0.1305\n",
      "epoch 9297: loss = 0.0852\n",
      "epoch 9298: loss = 0.1015\n",
      "epoch 9299: loss = 0.1227\n",
      "epoch 9300: loss = 0.0419\n",
      "epoch 9301: loss = 0.0531\n",
      "epoch 9302: loss = 0.1407\n",
      "epoch 9303: loss = 0.1666\n",
      "epoch 9304: loss = 0.1340\n",
      "epoch 9305: loss = 0.1262\n",
      "epoch 9306: loss = 0.1091\n",
      "epoch 9307: loss = 0.1796\n",
      "epoch 9308: loss = 0.0370\n",
      "epoch 9309: loss = 0.1525\n",
      "epoch 9310: loss = 0.1087\n",
      "epoch 9311: loss = 0.0481\n",
      "epoch 9312: loss = 0.1711\n",
      "epoch 9313: loss = 0.1093\n",
      "epoch 9314: loss = 0.0563\n",
      "epoch 9315: loss = 0.1412\n",
      "epoch 9316: loss = 0.1259\n",
      "epoch 9317: loss = 0.0836\n",
      "epoch 9318: loss = 0.1100\n",
      "epoch 9319: loss = 0.1266\n",
      "epoch 9320: loss = 0.1876\n",
      "epoch 9321: loss = 0.1471\n",
      "epoch 9322: loss = 0.0979\n",
      "epoch 9323: loss = 0.1831\n",
      "epoch 9324: loss = 0.1463\n",
      "epoch 9325: loss = 0.0612\n",
      "epoch 9326: loss = 0.0875\n",
      "epoch 9327: loss = 0.0830\n",
      "epoch 9328: loss = 0.1015\n",
      "epoch 9329: loss = 0.0267\n",
      "epoch 9330: loss = 0.1763\n",
      "epoch 9331: loss = 0.1723\n",
      "epoch 9332: loss = 0.1296\n",
      "epoch 9333: loss = 0.0649\n",
      "epoch 9334: loss = 0.1892\n",
      "epoch 9335: loss = 0.1223\n",
      "epoch 9336: loss = 0.1429\n",
      "epoch 9337: loss = 0.1502\n",
      "epoch 9338: loss = 0.0967\n",
      "epoch 9339: loss = 0.0758\n",
      "epoch 9340: loss = 0.0690\n",
      "epoch 9341: loss = 0.0210\n",
      "epoch 9342: loss = 0.0608\n",
      "epoch 9343: loss = 0.1317\n",
      "epoch 9344: loss = 0.1609\n",
      "epoch 9345: loss = 0.0480\n",
      "epoch 9346: loss = 0.1423\n",
      "epoch 9347: loss = 0.2042\n",
      "epoch 9348: loss = 0.1110\n",
      "epoch 9349: loss = 0.0530\n",
      "epoch 9350: loss = 0.0918\n",
      "epoch 9351: loss = 0.0715\n",
      "epoch 9352: loss = 0.1135\n",
      "epoch 9353: loss = 0.1208\n",
      "epoch 9354: loss = 0.0841\n",
      "epoch 9355: loss = 0.1395\n",
      "epoch 9356: loss = 0.1290\n",
      "epoch 9357: loss = 0.0316\n",
      "epoch 9358: loss = 0.0850\n",
      "epoch 9359: loss = 0.0740\n",
      "epoch 9360: loss = 0.1137\n",
      "epoch 9361: loss = 0.1850\n",
      "epoch 9362: loss = 0.0790\n",
      "epoch 9363: loss = 0.1720\n",
      "epoch 9364: loss = 0.1591\n",
      "epoch 9365: loss = 0.1056\n",
      "epoch 9366: loss = 0.1037\n",
      "epoch 9367: loss = 0.1595\n",
      "epoch 9368: loss = 0.0210\n",
      "epoch 9369: loss = 0.0564\n",
      "epoch 9370: loss = 0.1410\n",
      "epoch 9371: loss = 0.1657\n",
      "epoch 9372: loss = 0.0978\n",
      "epoch 9373: loss = 0.1459\n",
      "epoch 9374: loss = 0.1194\n",
      "epoch 9375: loss = 0.1131\n",
      "epoch 9376: loss = 0.1267\n",
      "epoch 9377: loss = 0.0859\n",
      "epoch 9378: loss = 0.1806\n",
      "epoch 9379: loss = 0.1008\n",
      "epoch 9380: loss = 0.0948\n",
      "epoch 9381: loss = 0.0576\n",
      "epoch 9382: loss = 0.0941\n",
      "epoch 9383: loss = 0.0763\n",
      "epoch 9384: loss = 0.0882\n",
      "epoch 9385: loss = 0.1573\n",
      "epoch 9386: loss = 0.1724\n",
      "epoch 9387: loss = 0.0777\n",
      "epoch 9388: loss = 0.0998\n",
      "epoch 9389: loss = 0.0477\n",
      "epoch 9390: loss = 0.1196\n",
      "epoch 9391: loss = 0.1152\n",
      "epoch 9392: loss = 0.1000\n",
      "epoch 9393: loss = 0.1246\n",
      "epoch 9394: loss = 0.1045\n",
      "epoch 9395: loss = 0.1200\n",
      "epoch 9396: loss = 0.1531\n",
      "epoch 9397: loss = 0.0728\n",
      "epoch 9398: loss = 0.1519\n",
      "epoch 9399: loss = 0.1660\n",
      "epoch 9400: loss = 0.1109\n",
      "epoch 9401: loss = 0.0756\n",
      "epoch 9402: loss = 0.0369\n",
      "epoch 9403: loss = 0.0946\n",
      "epoch 9404: loss = 0.0980\n",
      "epoch 9405: loss = 0.1589\n",
      "epoch 9406: loss = 0.1103\n",
      "epoch 9407: loss = 0.0733\n",
      "epoch 9408: loss = 0.1442\n",
      "epoch 9409: loss = 0.1445\n",
      "epoch 9410: loss = 0.0818\n",
      "epoch 9411: loss = 0.0599\n",
      "epoch 9412: loss = 0.1122\n",
      "epoch 9413: loss = 0.0418\n",
      "epoch 9414: loss = 0.0814\n",
      "epoch 9415: loss = 0.0621\n",
      "epoch 9416: loss = 0.1173\n",
      "epoch 9417: loss = 0.1008\n",
      "epoch 9418: loss = 0.1038\n",
      "epoch 9419: loss = 0.1694\n",
      "epoch 9420: loss = 0.1236\n",
      "epoch 9421: loss = 0.1491\n",
      "epoch 9422: loss = 0.0204\n",
      "epoch 9423: loss = 0.1408\n",
      "epoch 9424: loss = 0.0539\n",
      "epoch 9425: loss = 0.0955\n",
      "epoch 9426: loss = 0.1079\n",
      "epoch 9427: loss = 0.0949\n",
      "epoch 9428: loss = 0.1231\n",
      "epoch 9429: loss = 0.1810\n",
      "epoch 9430: loss = 0.1604\n",
      "epoch 9431: loss = 0.0961\n",
      "epoch 9432: loss = 0.1070\n",
      "epoch 9433: loss = 0.0969\n",
      "epoch 9434: loss = 0.1223\n",
      "epoch 9435: loss = 0.1577\n",
      "epoch 9436: loss = 0.0779\n",
      "epoch 9437: loss = 0.0792\n",
      "epoch 9438: loss = 0.0603\n",
      "epoch 9439: loss = 0.0875\n",
      "epoch 9440: loss = 0.0803\n",
      "epoch 9441: loss = 0.0990\n",
      "epoch 9442: loss = 0.0993\n",
      "epoch 9443: loss = 0.1198\n",
      "epoch 9444: loss = 0.1382\n",
      "epoch 9445: loss = 0.0411\n",
      "epoch 9446: loss = 0.0944\n",
      "epoch 9447: loss = 0.1304\n",
      "epoch 9448: loss = 0.1717\n",
      "epoch 9449: loss = 0.1584\n",
      "epoch 9450: loss = 0.1362\n",
      "epoch 9451: loss = 0.1287\n",
      "epoch 9452: loss = 0.0406\n",
      "epoch 9453: loss = 0.0959\n",
      "epoch 9454: loss = 0.0525\n",
      "epoch 9455: loss = 0.1439\n",
      "epoch 9456: loss = 0.0966\n",
      "epoch 9457: loss = 0.1805\n",
      "epoch 9458: loss = 0.1644\n",
      "epoch 9459: loss = 0.1325\n",
      "epoch 9460: loss = 0.0234\n",
      "epoch 9461: loss = 0.0504\n",
      "epoch 9462: loss = 0.1487\n",
      "epoch 9463: loss = 0.0760\n",
      "epoch 9464: loss = 0.1478\n",
      "epoch 9465: loss = 0.1155\n",
      "epoch 9466: loss = 0.0599\n",
      "epoch 9467: loss = 0.1759\n",
      "epoch 9468: loss = 0.1289\n",
      "epoch 9469: loss = 0.0735\n",
      "epoch 9470: loss = 0.0809\n",
      "epoch 9471: loss = 0.0864\n",
      "epoch 9472: loss = 0.1098\n",
      "epoch 9473: loss = 0.0912\n",
      "epoch 9474: loss = 0.1134\n",
      "epoch 9475: loss = 0.0245\n",
      "epoch 9476: loss = 0.0524\n",
      "epoch 9477: loss = 0.0974\n",
      "epoch 9478: loss = 0.0691\n",
      "epoch 9479: loss = 0.1399\n",
      "epoch 9480: loss = 0.0883\n",
      "epoch 9481: loss = 0.1407\n",
      "epoch 9482: loss = 0.1148\n",
      "epoch 9483: loss = 0.1131\n",
      "epoch 9484: loss = 0.1466\n",
      "epoch 9485: loss = 0.1583\n",
      "epoch 9486: loss = 0.1263\n",
      "epoch 9487: loss = 0.1751\n",
      "epoch 9488: loss = 0.1514\n",
      "epoch 9489: loss = 0.1716\n",
      "epoch 9490: loss = 0.1889\n",
      "epoch 9491: loss = 0.0479\n",
      "epoch 9492: loss = 0.1075\n",
      "epoch 9493: loss = 0.1059\n",
      "epoch 9494: loss = 0.0561\n",
      "epoch 9495: loss = 0.1044\n",
      "epoch 9496: loss = 0.0949\n",
      "epoch 9497: loss = 0.0912\n",
      "epoch 9498: loss = 0.0827\n",
      "epoch 9499: loss = 0.0942\n",
      "epoch 9500: loss = 0.1328\n",
      "epoch 9501: loss = 0.1362\n",
      "epoch 9502: loss = 0.0973\n",
      "epoch 9503: loss = 0.1045\n",
      "epoch 9504: loss = 0.0910\n",
      "epoch 9505: loss = 0.1529\n",
      "epoch 9506: loss = 0.0807\n",
      "epoch 9507: loss = 0.1078\n",
      "epoch 9508: loss = 0.1628\n",
      "epoch 9509: loss = 0.1937\n",
      "epoch 9510: loss = 0.1430\n",
      "epoch 9511: loss = 0.1227\n",
      "epoch 9512: loss = 0.1306\n",
      "epoch 9513: loss = 0.1683\n",
      "epoch 9514: loss = 0.1648\n",
      "epoch 9515: loss = 0.1456\n",
      "epoch 9516: loss = 0.0869\n",
      "epoch 9517: loss = 0.1253\n",
      "epoch 9518: loss = 0.1402\n",
      "epoch 9519: loss = 0.1549\n",
      "epoch 9520: loss = 0.1038\n",
      "epoch 9521: loss = 0.1520\n",
      "epoch 9522: loss = 0.1620\n",
      "epoch 9523: loss = 0.0889\n",
      "epoch 9524: loss = 0.0628\n",
      "epoch 9525: loss = 0.0471\n",
      "epoch 9526: loss = 0.1345\n",
      "epoch 9527: loss = 0.0407\n",
      "epoch 9528: loss = 0.0776\n",
      "epoch 9529: loss = 0.0838\n",
      "epoch 9530: loss = 0.0936\n",
      "epoch 9531: loss = 0.1449\n",
      "epoch 9532: loss = 0.1264\n",
      "epoch 9533: loss = 0.0809\n",
      "epoch 9534: loss = 0.0423\n",
      "epoch 9535: loss = 0.1447\n",
      "epoch 9536: loss = 0.0523\n",
      "epoch 9537: loss = 0.1506\n",
      "epoch 9538: loss = 0.1140\n",
      "epoch 9539: loss = 0.1278\n",
      "epoch 9540: loss = 0.1834\n",
      "epoch 9541: loss = 0.1478\n",
      "epoch 9542: loss = 0.1097\n",
      "epoch 9543: loss = 0.0636\n",
      "epoch 9544: loss = 0.1018\n",
      "epoch 9545: loss = 0.0883\n",
      "epoch 9546: loss = 0.0691\n",
      "epoch 9547: loss = 0.1142\n",
      "epoch 9548: loss = 0.1701\n",
      "epoch 9549: loss = 0.0624\n",
      "epoch 9550: loss = 0.1284\n",
      "epoch 9551: loss = 0.0825\n",
      "epoch 9552: loss = 0.0689\n",
      "epoch 9553: loss = 0.1309\n",
      "epoch 9554: loss = 0.0991\n",
      "epoch 9555: loss = 0.0413\n",
      "epoch 9556: loss = 0.0969\n",
      "epoch 9557: loss = 0.1445\n",
      "epoch 9558: loss = 0.1307\n",
      "epoch 9559: loss = 0.1797\n",
      "epoch 9560: loss = 0.1450\n",
      "epoch 9561: loss = 0.0859\n",
      "epoch 9562: loss = 0.1378\n",
      "epoch 9563: loss = 0.1768\n",
      "epoch 9564: loss = 0.0921\n",
      "epoch 9565: loss = 0.1108\n",
      "epoch 9566: loss = 0.0365\n",
      "epoch 9567: loss = 0.0179\n",
      "epoch 9568: loss = 0.0594\n",
      "epoch 9569: loss = 0.0780\n",
      "epoch 9570: loss = 0.1140\n",
      "epoch 9571: loss = 0.0570\n",
      "epoch 9572: loss = 0.0699\n",
      "epoch 9573: loss = 0.1311\n",
      "epoch 9574: loss = 0.0407\n",
      "epoch 9575: loss = 0.1236\n",
      "epoch 9576: loss = 0.0819\n",
      "epoch 9577: loss = 0.1177\n",
      "epoch 9578: loss = 0.0997\n",
      "epoch 9579: loss = 0.0840\n",
      "epoch 9580: loss = 0.0269\n",
      "epoch 9581: loss = 0.1599\n",
      "epoch 9582: loss = 0.1150\n",
      "epoch 9583: loss = 0.1810\n",
      "epoch 9584: loss = 0.1202\n",
      "epoch 9585: loss = 0.0409\n",
      "epoch 9586: loss = 0.1391\n",
      "epoch 9587: loss = 0.0882\n",
      "epoch 9588: loss = 0.1069\n",
      "epoch 9589: loss = 0.1272\n",
      "epoch 9590: loss = 0.0772\n",
      "epoch 9591: loss = 0.1384\n",
      "epoch 9592: loss = 0.1052\n",
      "epoch 9593: loss = 0.0534\n",
      "epoch 9594: loss = 0.1589\n",
      "epoch 9595: loss = 0.1353\n",
      "epoch 9596: loss = 0.0945\n",
      "epoch 9597: loss = 0.0764\n",
      "epoch 9598: loss = 0.1513\n",
      "epoch 9599: loss = 0.0724\n",
      "epoch 9600: loss = 0.1204\n",
      "epoch 9601: loss = 0.1711\n",
      "epoch 9602: loss = 0.0414\n",
      "epoch 9603: loss = 0.0936\n",
      "epoch 9604: loss = 0.1628\n",
      "epoch 9605: loss = 0.1606\n",
      "epoch 9606: loss = 0.1591\n",
      "epoch 9607: loss = 0.0344\n",
      "epoch 9608: loss = 0.1740\n",
      "epoch 9609: loss = 0.1591\n",
      "epoch 9610: loss = 0.1767\n",
      "epoch 9611: loss = 0.1250\n",
      "epoch 9612: loss = 0.0711\n",
      "epoch 9613: loss = 0.1295\n",
      "epoch 9614: loss = 0.1719\n",
      "epoch 9615: loss = 0.0681\n",
      "epoch 9616: loss = 0.1512\n",
      "epoch 9617: loss = 0.1268\n",
      "epoch 9618: loss = 0.1233\n",
      "epoch 9619: loss = 0.1180\n",
      "epoch 9620: loss = 0.1290\n",
      "epoch 9621: loss = 0.1371\n",
      "epoch 9622: loss = 0.1769\n",
      "epoch 9623: loss = 0.0994\n",
      "epoch 9624: loss = 0.0913\n",
      "epoch 9625: loss = 0.1552\n",
      "epoch 9626: loss = 0.1311\n",
      "epoch 9627: loss = 0.1472\n",
      "epoch 9628: loss = 0.1586\n",
      "epoch 9629: loss = 0.1465\n",
      "epoch 9630: loss = 0.1424\n",
      "epoch 9631: loss = 0.1320\n",
      "epoch 9632: loss = 0.1787\n",
      "epoch 9633: loss = 0.1379\n",
      "epoch 9634: loss = 0.1463\n",
      "epoch 9635: loss = 0.0910\n",
      "epoch 9636: loss = 0.0391\n",
      "epoch 9637: loss = 0.1658\n",
      "epoch 9638: loss = 0.0725\n",
      "epoch 9639: loss = 0.1016\n",
      "epoch 9640: loss = 0.0948\n",
      "epoch 9641: loss = 0.1174\n",
      "epoch 9642: loss = 0.1141\n",
      "epoch 9643: loss = 0.1150\n",
      "epoch 9644: loss = 0.0613\n",
      "epoch 9645: loss = 0.1079\n",
      "epoch 9646: loss = 0.1194\n",
      "epoch 9647: loss = 0.0617\n",
      "epoch 9648: loss = 0.0607\n",
      "epoch 9649: loss = 0.1005\n",
      "epoch 9650: loss = 0.1032\n",
      "epoch 9651: loss = 0.0374\n",
      "epoch 9652: loss = 0.1230\n",
      "epoch 9653: loss = 0.1434\n",
      "epoch 9654: loss = 0.1330\n",
      "epoch 9655: loss = 0.1622\n",
      "epoch 9656: loss = 0.1146\n",
      "epoch 9657: loss = 0.0844\n",
      "epoch 9658: loss = 0.1220\n",
      "epoch 9659: loss = 0.0261\n",
      "epoch 9660: loss = 0.1167\n",
      "epoch 9661: loss = 0.1184\n",
      "epoch 9662: loss = 0.1238\n",
      "epoch 9663: loss = 0.1093\n",
      "epoch 9664: loss = 0.0377\n",
      "epoch 9665: loss = 0.1210\n",
      "epoch 9666: loss = 0.0604\n",
      "epoch 9667: loss = 0.1094\n",
      "epoch 9668: loss = 0.0950\n",
      "epoch 9669: loss = 0.1177\n",
      "epoch 9670: loss = 0.1614\n",
      "epoch 9671: loss = 0.1393\n",
      "epoch 9672: loss = 0.0624\n",
      "epoch 9673: loss = 0.0768\n",
      "epoch 9674: loss = 0.0660\n",
      "epoch 9675: loss = 0.1681\n",
      "epoch 9676: loss = 0.1557\n",
      "epoch 9677: loss = 0.1533\n",
      "epoch 9678: loss = 0.1893\n",
      "epoch 9679: loss = 0.0960\n",
      "epoch 9680: loss = 0.1324\n",
      "epoch 9681: loss = 0.1155\n",
      "epoch 9682: loss = 0.1140\n",
      "epoch 9683: loss = 0.1074\n",
      "epoch 9684: loss = 0.1349\n",
      "epoch 9685: loss = 0.0887\n",
      "epoch 9686: loss = 0.0473\n",
      "epoch 9687: loss = 0.0642\n",
      "epoch 9688: loss = 0.0265\n",
      "epoch 9689: loss = 0.0666\n",
      "epoch 9690: loss = 0.1296\n",
      "epoch 9691: loss = 0.1038\n",
      "epoch 9692: loss = 0.1129\n",
      "epoch 9693: loss = 0.1022\n",
      "epoch 9694: loss = 0.0949\n",
      "epoch 9695: loss = 0.1048\n",
      "epoch 9696: loss = 0.0982\n",
      "epoch 9697: loss = 0.0178\n",
      "epoch 9698: loss = 0.0844\n",
      "epoch 9699: loss = 0.2003\n",
      "epoch 9700: loss = 0.1757\n",
      "epoch 9701: loss = 0.0714\n",
      "epoch 9702: loss = 0.0449\n",
      "epoch 9703: loss = 0.1245\n",
      "epoch 9704: loss = 0.0663\n",
      "epoch 9705: loss = 0.1376\n",
      "epoch 9706: loss = 0.1457\n",
      "epoch 9707: loss = 0.1277\n",
      "epoch 9708: loss = 0.0315\n",
      "epoch 9709: loss = 0.1532\n",
      "epoch 9710: loss = 0.1306\n",
      "epoch 9711: loss = 0.0985\n",
      "epoch 9712: loss = 0.1399\n",
      "epoch 9713: loss = 0.1471\n",
      "epoch 9714: loss = 0.0816\n",
      "epoch 9715: loss = 0.0429\n",
      "epoch 9716: loss = 0.0684\n",
      "epoch 9717: loss = 0.0745\n",
      "epoch 9718: loss = 0.0748\n",
      "epoch 9719: loss = 0.0785\n",
      "epoch 9720: loss = 0.1660\n",
      "epoch 9721: loss = 0.1445\n",
      "epoch 9722: loss = 0.1550\n",
      "epoch 9723: loss = 0.1334\n",
      "epoch 9724: loss = 0.1431\n",
      "epoch 9725: loss = 0.1811\n",
      "epoch 9726: loss = 0.1570\n",
      "epoch 9727: loss = 0.0392\n",
      "epoch 9728: loss = 0.1547\n",
      "epoch 9729: loss = 0.1086\n",
      "epoch 9730: loss = 0.1575\n",
      "epoch 9731: loss = 0.1433\n",
      "epoch 9732: loss = 0.1166\n",
      "epoch 9733: loss = 0.0981\n",
      "epoch 9734: loss = 0.0627\n",
      "epoch 9735: loss = 0.0583\n",
      "epoch 9736: loss = 0.1666\n",
      "epoch 9737: loss = 0.1011\n",
      "epoch 9738: loss = 0.0837\n",
      "epoch 9739: loss = 0.0898\n",
      "epoch 9740: loss = 0.1110\n",
      "epoch 9741: loss = 0.0827\n",
      "epoch 9742: loss = 0.0587\n",
      "epoch 9743: loss = 0.1673\n",
      "epoch 9744: loss = 0.0101\n",
      "epoch 9745: loss = 0.1192\n",
      "epoch 9746: loss = 0.1895\n",
      "epoch 9747: loss = 0.1693\n",
      "epoch 9748: loss = 0.1599\n",
      "epoch 9749: loss = 0.1230\n",
      "epoch 9750: loss = 0.1732\n",
      "epoch 9751: loss = 0.1809\n",
      "epoch 9752: loss = 0.0886\n",
      "epoch 9753: loss = 0.0822\n",
      "epoch 9754: loss = 0.1508\n",
      "epoch 9755: loss = 0.0877\n",
      "epoch 9756: loss = 0.1484\n",
      "epoch 9757: loss = 0.1124\n",
      "epoch 9758: loss = 0.0767\n",
      "epoch 9759: loss = 0.0531\n",
      "epoch 9760: loss = 0.1015\n",
      "epoch 9761: loss = 0.1425\n",
      "epoch 9762: loss = 0.1099\n",
      "epoch 9763: loss = 0.1280\n",
      "epoch 9764: loss = 0.0937\n",
      "epoch 9765: loss = 0.1922\n",
      "epoch 9766: loss = 0.1348\n",
      "epoch 9767: loss = 0.1017\n",
      "epoch 9768: loss = 0.1793\n",
      "epoch 9769: loss = 0.1310\n",
      "epoch 9770: loss = 0.1576\n",
      "epoch 9771: loss = 0.1523\n",
      "epoch 9772: loss = 0.1135\n",
      "epoch 9773: loss = 0.1263\n",
      "epoch 9774: loss = 0.1329\n",
      "epoch 9775: loss = 0.1308\n",
      "epoch 9776: loss = 0.1471\n",
      "epoch 9777: loss = 0.1387\n",
      "epoch 9778: loss = 0.0437\n",
      "epoch 9779: loss = 0.0448\n",
      "epoch 9780: loss = 0.0666\n",
      "epoch 9781: loss = 0.1498\n",
      "epoch 9782: loss = 0.1158\n",
      "epoch 9783: loss = 0.1188\n",
      "epoch 9784: loss = 0.0627\n",
      "epoch 9785: loss = 0.0321\n",
      "epoch 9786: loss = 0.1003\n",
      "epoch 9787: loss = 0.1557\n",
      "epoch 9788: loss = 0.1958\n",
      "epoch 9789: loss = 0.0830\n",
      "epoch 9790: loss = 0.0759\n",
      "epoch 9791: loss = 0.1393\n",
      "epoch 9792: loss = 0.1011\n",
      "epoch 9793: loss = 0.1742\n",
      "epoch 9794: loss = 0.1871\n",
      "epoch 9795: loss = 0.1086\n",
      "epoch 9796: loss = 0.1241\n",
      "epoch 9797: loss = 0.1602\n",
      "epoch 9798: loss = 0.0593\n",
      "epoch 9799: loss = 0.1579\n",
      "epoch 9800: loss = 0.1473\n",
      "epoch 9801: loss = 0.1191\n",
      "epoch 9802: loss = 0.1656\n",
      "epoch 9803: loss = 0.1208\n",
      "epoch 9804: loss = 0.1331\n",
      "epoch 9805: loss = 0.0780\n",
      "epoch 9806: loss = 0.1074\n",
      "epoch 9807: loss = 0.0355\n",
      "epoch 9808: loss = 0.0835\n",
      "epoch 9809: loss = 0.0317\n",
      "epoch 9810: loss = 0.0717\n",
      "epoch 9811: loss = 0.0841\n",
      "epoch 9812: loss = 0.1346\n",
      "epoch 9813: loss = 0.1014\n",
      "epoch 9814: loss = 0.0851\n",
      "epoch 9815: loss = 0.1567\n",
      "epoch 9816: loss = 0.1822\n",
      "epoch 9817: loss = 0.1430\n",
      "epoch 9818: loss = 0.1069\n",
      "epoch 9819: loss = 0.1151\n",
      "epoch 9820: loss = 0.0628\n",
      "epoch 9821: loss = 0.1512\n",
      "epoch 9822: loss = 0.0687\n",
      "epoch 9823: loss = 0.1353\n",
      "epoch 9824: loss = 0.0740\n",
      "epoch 9825: loss = 0.0826\n",
      "epoch 9826: loss = 0.1493\n",
      "epoch 9827: loss = 0.0496\n",
      "epoch 9828: loss = 0.1410\n",
      "epoch 9829: loss = 0.1554\n",
      "epoch 9830: loss = 0.1460\n",
      "epoch 9831: loss = 0.1180\n",
      "epoch 9832: loss = 0.1191\n",
      "epoch 9833: loss = 0.0975\n",
      "epoch 9834: loss = 0.1148\n",
      "epoch 9835: loss = 0.0791\n",
      "epoch 9836: loss = 0.1321\n",
      "epoch 9837: loss = 0.1171\n",
      "epoch 9838: loss = 0.0924\n",
      "epoch 9839: loss = 0.1088\n",
      "epoch 9840: loss = 0.1820\n",
      "epoch 9841: loss = 0.1571\n",
      "epoch 9842: loss = 0.1656\n",
      "epoch 9843: loss = 0.1064\n",
      "epoch 9844: loss = 0.1308\n",
      "epoch 9845: loss = 0.1289\n",
      "epoch 9846: loss = 0.0962\n",
      "epoch 9847: loss = 0.1597\n",
      "epoch 9848: loss = 0.0827\n",
      "epoch 9849: loss = 0.1410\n",
      "epoch 9850: loss = 0.1335\n",
      "epoch 9851: loss = 0.1409\n",
      "epoch 9852: loss = 0.0707\n",
      "epoch 9853: loss = 0.1088\n",
      "epoch 9854: loss = 0.0786\n",
      "epoch 9855: loss = 0.0750\n",
      "epoch 9856: loss = 0.1138\n",
      "epoch 9857: loss = 0.0939\n",
      "epoch 9858: loss = 0.1186\n",
      "epoch 9859: loss = 0.0675\n",
      "epoch 9860: loss = 0.1341\n",
      "epoch 9861: loss = 0.1327\n",
      "epoch 9862: loss = 0.1720\n",
      "epoch 9863: loss = 0.0579\n",
      "epoch 9864: loss = 0.0811\n",
      "epoch 9865: loss = 0.1083\n",
      "epoch 9866: loss = 0.1029\n",
      "epoch 9867: loss = 0.1580\n",
      "epoch 9868: loss = 0.1363\n",
      "epoch 9869: loss = 0.1787\n",
      "epoch 9870: loss = 0.0317\n",
      "epoch 9871: loss = 0.1809\n",
      "epoch 9872: loss = 0.0630\n",
      "epoch 9873: loss = 0.1145\n",
      "epoch 9874: loss = 0.0725\n",
      "epoch 9875: loss = 0.1526\n",
      "epoch 9876: loss = 0.1481\n",
      "epoch 9877: loss = 0.1166\n",
      "epoch 9878: loss = 0.0634\n",
      "epoch 9879: loss = 0.0181\n",
      "epoch 9880: loss = 0.1429\n",
      "epoch 9881: loss = 0.1177\n",
      "epoch 9882: loss = 0.0948\n",
      "epoch 9883: loss = 0.0883\n",
      "epoch 9884: loss = 0.1227\n",
      "epoch 9885: loss = 0.1093\n",
      "epoch 9886: loss = 0.1465\n",
      "epoch 9887: loss = 0.1142\n",
      "epoch 9888: loss = 0.1713\n",
      "epoch 9889: loss = 0.1692\n",
      "epoch 9890: loss = 0.0986\n",
      "epoch 9891: loss = 0.0932\n",
      "epoch 9892: loss = 0.1600\n",
      "epoch 9893: loss = 0.0254\n",
      "epoch 9894: loss = 0.1393\n",
      "epoch 9895: loss = 0.1165\n",
      "epoch 9896: loss = 0.0785\n",
      "epoch 9897: loss = 0.0108\n",
      "epoch 9898: loss = 0.1226\n",
      "epoch 9899: loss = 0.1675\n",
      "epoch 9900: loss = 0.1628\n",
      "epoch 9901: loss = 0.1149\n",
      "epoch 9902: loss = 0.1220\n",
      "epoch 9903: loss = 0.1193\n",
      "epoch 9904: loss = 0.1019\n",
      "epoch 9905: loss = 0.1095\n",
      "epoch 9906: loss = 0.1143\n",
      "epoch 9907: loss = 0.0686\n",
      "epoch 9908: loss = 0.1770\n",
      "epoch 9909: loss = 0.1055\n",
      "epoch 9910: loss = 0.0865\n",
      "epoch 9911: loss = 0.1368\n",
      "epoch 9912: loss = 0.1016\n",
      "epoch 9913: loss = 0.1287\n",
      "epoch 9914: loss = 0.1485\n",
      "epoch 9915: loss = 0.0680\n",
      "epoch 9916: loss = 0.1147\n",
      "epoch 9917: loss = 0.0658\n",
      "epoch 9918: loss = 0.0690\n",
      "epoch 9919: loss = 0.1146\n",
      "epoch 9920: loss = 0.1316\n",
      "epoch 9921: loss = 0.0983\n",
      "epoch 9922: loss = 0.0851\n",
      "epoch 9923: loss = 0.0480\n",
      "epoch 9924: loss = 0.1022\n",
      "epoch 9925: loss = 0.1841\n",
      "epoch 9926: loss = 0.0360\n",
      "epoch 9927: loss = 0.0467\n",
      "epoch 9928: loss = 0.1737\n",
      "epoch 9929: loss = 0.1857\n",
      "epoch 9930: loss = 0.0555\n",
      "epoch 9931: loss = 0.0718\n",
      "epoch 9932: loss = 0.1465\n",
      "epoch 9933: loss = 0.1395\n",
      "epoch 9934: loss = 0.0741\n",
      "epoch 9935: loss = 0.1784\n",
      "epoch 9936: loss = 0.0875\n",
      "epoch 9937: loss = 0.1325\n",
      "epoch 9938: loss = 0.1214\n",
      "epoch 9939: loss = 0.0890\n",
      "epoch 9940: loss = 0.1390\n",
      "epoch 9941: loss = 0.1012\n",
      "epoch 9942: loss = 0.1065\n",
      "epoch 9943: loss = 0.0108\n",
      "epoch 9944: loss = 0.1230\n",
      "epoch 9945: loss = 0.1113\n",
      "epoch 9946: loss = 0.0593\n",
      "epoch 9947: loss = 0.1693\n",
      "epoch 9948: loss = 0.1435\n",
      "epoch 9949: loss = 0.0931\n",
      "epoch 9950: loss = 0.0751\n",
      "epoch 9951: loss = 0.1396\n",
      "epoch 9952: loss = 0.0433\n",
      "epoch 9953: loss = 0.1690\n",
      "epoch 9954: loss = 0.0326\n",
      "epoch 9955: loss = 0.0761\n",
      "epoch 9956: loss = 0.1117\n",
      "epoch 9957: loss = 0.1671\n",
      "epoch 9958: loss = 0.0401\n",
      "epoch 9959: loss = 0.0888\n",
      "epoch 9960: loss = 0.0448\n",
      "epoch 9961: loss = 0.1604\n",
      "epoch 9962: loss = 0.1226\n",
      "epoch 9963: loss = 0.1118\n",
      "epoch 9964: loss = 0.1479\n",
      "epoch 9965: loss = 0.1683\n",
      "epoch 9966: loss = 0.1264\n",
      "epoch 9967: loss = 0.1336\n",
      "epoch 9968: loss = 0.0415\n",
      "epoch 9969: loss = 0.0663\n",
      "epoch 9970: loss = 0.0766\n",
      "epoch 9971: loss = 0.1518\n",
      "epoch 9972: loss = 0.0891\n",
      "epoch 9973: loss = 0.0914\n",
      "epoch 9974: loss = 0.1006\n",
      "epoch 9975: loss = 0.1489\n",
      "epoch 9976: loss = 0.1477\n",
      "epoch 9977: loss = 0.1050\n",
      "epoch 9978: loss = 0.1254\n",
      "epoch 9979: loss = 0.1470\n",
      "epoch 9980: loss = 0.0784\n",
      "epoch 9981: loss = 0.0971\n",
      "epoch 9982: loss = 0.0946\n",
      "epoch 9983: loss = 0.1031\n",
      "epoch 9984: loss = 0.1317\n",
      "epoch 9985: loss = 0.0879\n",
      "epoch 9986: loss = 0.1097\n",
      "epoch 9987: loss = 0.0730\n",
      "epoch 9988: loss = 0.0938\n",
      "epoch 9989: loss = 0.1488\n",
      "epoch 9990: loss = 0.0843\n",
      "epoch 9991: loss = 0.0914\n",
      "epoch 9992: loss = 0.0988\n",
      "epoch 9993: loss = 0.1168\n",
      "epoch 9994: loss = 0.1423\n",
      "epoch 9995: loss = 0.0638\n",
      "epoch 9996: loss = 0.1267\n",
      "epoch 9997: loss = 0.0605\n",
      "epoch 9998: loss = 0.1343\n",
      "epoch 9999: loss = 0.1199\n",
      "epoch 10000: loss = 0.0946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.26923078, 0.26923078],\n",
       "       [0.21939377, 0.20192307],\n",
       "       [0.20192307, 0.13461539],\n",
       "       [0.14933336, 0.11941336],\n",
       "       [0.13461539, 0.10096154],\n",
       "       [0.10096154, 0.0673077 ],\n",
       "       [0.0673077 , 0.05288462],\n",
       "       [0.05288462, 0.05177101],\n",
       "       [0.03365385, 0.03365385]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Get Anchor Boxs\n",
    "from utils.kmeans import kmeans, iou_dist, euclidean_dist\n",
    "import numpy as np\n",
    "\n",
    "all_boxes = train_label[2][train_label[2][..., 4] == 1][..., 2:4]\n",
    "anchors = kmeans(\n",
    "    all_boxes,\n",
    "    n_cluster=9,\n",
    "    dist_func=iou_dist,\n",
    "    stop_dist=0.000001)\n",
    "\n",
    "anchors = np.sort(anchors, axis=0)[::-1]\n",
    "display(anchors)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(all_boxes[..., 0], all_boxes[..., 1])\n",
    "plt.scatter(anchors[..., 0],\n",
    "            anchors[..., 1],\n",
    "            c=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Number Plate Region\\env\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Number Plate Region\\env\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 416, 416, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 416, 416, 32)         864       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 416, 416, 32)         128       ['conv2d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)     (None, 416, 416, 32)         0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " zero_padding2d (ZeroPaddin  (None, 417, 417, 32)         0         ['leaky_re_lu[0][0]']         \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 208, 208, 64)         18432     ['zero_padding2d[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 208, 208, 64)         256       ['conv2d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 208, 208, 64)         0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " drop_block2d (DropBlock2D)  (None, 208, 208, 64)         0         ['leaky_re_lu_1[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 208, 208, 32)         2048      ['drop_block2d[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 208, 208, 32)         128       ['conv2d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 208, 208, 32)         0         ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " drop_block2d_1 (DropBlock2  (None, 208, 208, 32)         0         ['leaky_re_lu_2[0][0]']       \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 208, 208, 64)         18432     ['drop_block2d_1[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 208, 208, 64)         256       ['conv2d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 208, 208, 64)         0         ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 208, 208, 64)         0         ['drop_block2d[0][0]',        \n",
      "                                                                     'leaky_re_lu_3[0][0]']       \n",
      "                                                                                                  \n",
      " zero_padding2d_1 (ZeroPadd  (None, 209, 209, 64)         0         ['add[0][0]']                 \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 104, 104, 128)        73728     ['zero_padding2d_1[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 104, 104, 128)        512       ['conv2d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 104, 104, 128)        0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " drop_block2d_2 (DropBlock2  (None, 104, 104, 128)        0         ['leaky_re_lu_4[0][0]']       \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 104, 104, 64)         8192      ['drop_block2d_2[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 104, 104, 64)         256       ['conv2d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 104, 104, 64)         0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " drop_block2d_3 (DropBlock2  (None, 104, 104, 64)         0         ['leaky_re_lu_5[0][0]']       \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 104, 104, 128)        73728     ['drop_block2d_3[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 104, 104, 128)        512       ['conv2d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 104, 104, 128)        0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 104, 104, 128)        0         ['drop_block2d_2[0][0]',      \n",
      "                                                                     'leaky_re_lu_6[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 104, 104, 64)         8192      ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 104, 104, 64)         256       ['conv2d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 104, 104, 64)         0         ['batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " drop_block2d_4 (DropBlock2  (None, 104, 104, 64)         0         ['leaky_re_lu_7[0][0]']       \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 104, 104, 128)        73728     ['drop_block2d_4[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 104, 104, 128)        512       ['conv2d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 104, 104, 128)        0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 104, 104, 128)        0         ['add_1[0][0]',               \n",
      "                                                                     'leaky_re_lu_8[0][0]']       \n",
      "                                                                                                  \n",
      " zero_padding2d_2 (ZeroPadd  (None, 105, 105, 128)        0         ['add_2[0][0]']               \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 52, 52, 256)          294912    ['zero_padding2d_2[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 52, 52, 256)          1024      ['conv2d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)   (None, 52, 52, 256)          0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " drop_block2d_5 (DropBlock2  (None, 52, 52, 256)          0         ['leaky_re_lu_9[0][0]']       \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, 52, 52, 128)          32768     ['drop_block2d_5[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 52, 52, 128)          512       ['conv2d_10[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 52, 52, 128)          0         ['batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_6 (DropBlock2  (None, 52, 52, 128)          0         ['leaky_re_lu_10[0][0]']      \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 52, 52, 256)          294912    ['drop_block2d_6[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 52, 52, 256)          1024      ['conv2d_11[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, 52, 52, 256)          0         ['batch_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 52, 52, 256)          0         ['drop_block2d_5[0][0]',      \n",
      "                                                                     'leaky_re_lu_11[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 52, 52, 128)          32768     ['add_3[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_12 (Ba  (None, 52, 52, 128)          512       ['conv2d_12[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_12 (LeakyReLU)  (None, 52, 52, 128)          0         ['batch_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_7 (DropBlock2  (None, 52, 52, 128)          0         ['leaky_re_lu_12[0][0]']      \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 52, 52, 256)          294912    ['drop_block2d_7[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, 52, 52, 256)          1024      ['conv2d_13[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_13 (LeakyReLU)  (None, 52, 52, 256)          0         ['batch_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 52, 52, 256)          0         ['add_3[0][0]',               \n",
      "                                                                     'leaky_re_lu_13[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, 52, 52, 128)          32768     ['add_4[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, 52, 52, 128)          512       ['conv2d_14[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_14 (LeakyReLU)  (None, 52, 52, 128)          0         ['batch_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_8 (DropBlock2  (None, 52, 52, 128)          0         ['leaky_re_lu_14[0][0]']      \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, 52, 52, 256)          294912    ['drop_block2d_8[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, 52, 52, 256)          1024      ['conv2d_15[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_15 (LeakyReLU)  (None, 52, 52, 256)          0         ['batch_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 52, 52, 256)          0         ['add_4[0][0]',               \n",
      "                                                                     'leaky_re_lu_15[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)          (None, 52, 52, 128)          32768     ['add_5[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_16 (Ba  (None, 52, 52, 128)          512       ['conv2d_16[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_16 (LeakyReLU)  (None, 52, 52, 128)          0         ['batch_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_9 (DropBlock2  (None, 52, 52, 128)          0         ['leaky_re_lu_16[0][0]']      \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)          (None, 52, 52, 256)          294912    ['drop_block2d_9[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_17 (Ba  (None, 52, 52, 256)          1024      ['conv2d_17[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_17 (LeakyReLU)  (None, 52, 52, 256)          0         ['batch_normalization_17[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, 52, 52, 256)          0         ['add_5[0][0]',               \n",
      "                                                                     'leaky_re_lu_17[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)          (None, 52, 52, 128)          32768     ['add_6[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_18 (Ba  (None, 52, 52, 128)          512       ['conv2d_18[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_18 (LeakyReLU)  (None, 52, 52, 128)          0         ['batch_normalization_18[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_10 (DropBlock  (None, 52, 52, 128)          0         ['leaky_re_lu_18[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)          (None, 52, 52, 256)          294912    ['drop_block2d_10[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, 52, 52, 256)          1024      ['conv2d_19[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_19 (LeakyReLU)  (None, 52, 52, 256)          0         ['batch_normalization_19[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_7 (Add)                 (None, 52, 52, 256)          0         ['add_6[0][0]',               \n",
      "                                                                     'leaky_re_lu_19[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)          (None, 52, 52, 128)          32768     ['add_7[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_20 (Ba  (None, 52, 52, 128)          512       ['conv2d_20[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_20 (LeakyReLU)  (None, 52, 52, 128)          0         ['batch_normalization_20[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_11 (DropBlock  (None, 52, 52, 128)          0         ['leaky_re_lu_20[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)          (None, 52, 52, 256)          294912    ['drop_block2d_11[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_21 (Ba  (None, 52, 52, 256)          1024      ['conv2d_21[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_21 (LeakyReLU)  (None, 52, 52, 256)          0         ['batch_normalization_21[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_8 (Add)                 (None, 52, 52, 256)          0         ['add_7[0][0]',               \n",
      "                                                                     'leaky_re_lu_21[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)          (None, 52, 52, 128)          32768     ['add_8[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_22 (Ba  (None, 52, 52, 128)          512       ['conv2d_22[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_22 (LeakyReLU)  (None, 52, 52, 128)          0         ['batch_normalization_22[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_12 (DropBlock  (None, 52, 52, 128)          0         ['leaky_re_lu_22[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)          (None, 52, 52, 256)          294912    ['drop_block2d_12[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_23 (Ba  (None, 52, 52, 256)          1024      ['conv2d_23[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_23 (LeakyReLU)  (None, 52, 52, 256)          0         ['batch_normalization_23[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_9 (Add)                 (None, 52, 52, 256)          0         ['add_8[0][0]',               \n",
      "                                                                     'leaky_re_lu_23[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)          (None, 52, 52, 128)          32768     ['add_9[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_24 (Ba  (None, 52, 52, 128)          512       ['conv2d_24[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_24 (LeakyReLU)  (None, 52, 52, 128)          0         ['batch_normalization_24[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_13 (DropBlock  (None, 52, 52, 128)          0         ['leaky_re_lu_24[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)          (None, 52, 52, 256)          294912    ['drop_block2d_13[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_25 (Ba  (None, 52, 52, 256)          1024      ['conv2d_25[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_25 (LeakyReLU)  (None, 52, 52, 256)          0         ['batch_normalization_25[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_10 (Add)                (None, 52, 52, 256)          0         ['add_9[0][0]',               \n",
      "                                                                     'leaky_re_lu_25[0][0]']      \n",
      "                                                                                                  \n",
      " zero_padding2d_3 (ZeroPadd  (None, 53, 53, 256)          0         ['add_10[0][0]']              \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)          (None, 26, 26, 512)          1179648   ['zero_padding2d_3[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_26 (Ba  (None, 26, 26, 512)          2048      ['conv2d_26[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_26 (LeakyReLU)  (None, 26, 26, 512)          0         ['batch_normalization_26[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_14 (DropBlock  (None, 26, 26, 512)          0         ['leaky_re_lu_26[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)          (None, 26, 26, 256)          131072    ['drop_block2d_14[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_27 (Ba  (None, 26, 26, 256)          1024      ['conv2d_27[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_27 (LeakyReLU)  (None, 26, 26, 256)          0         ['batch_normalization_27[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_15 (DropBlock  (None, 26, 26, 256)          0         ['leaky_re_lu_27[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)          (None, 26, 26, 512)          1179648   ['drop_block2d_15[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_28 (Ba  (None, 26, 26, 512)          2048      ['conv2d_28[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_28 (LeakyReLU)  (None, 26, 26, 512)          0         ['batch_normalization_28[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_11 (Add)                (None, 26, 26, 512)          0         ['drop_block2d_14[0][0]',     \n",
      "                                                                     'leaky_re_lu_28[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)          (None, 26, 26, 256)          131072    ['add_11[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_29 (Ba  (None, 26, 26, 256)          1024      ['conv2d_29[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_29 (LeakyReLU)  (None, 26, 26, 256)          0         ['batch_normalization_29[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_16 (DropBlock  (None, 26, 26, 256)          0         ['leaky_re_lu_29[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)          (None, 26, 26, 512)          1179648   ['drop_block2d_16[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_30 (Ba  (None, 26, 26, 512)          2048      ['conv2d_30[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_30 (LeakyReLU)  (None, 26, 26, 512)          0         ['batch_normalization_30[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_12 (Add)                (None, 26, 26, 512)          0         ['add_11[0][0]',              \n",
      "                                                                     'leaky_re_lu_30[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)          (None, 26, 26, 256)          131072    ['add_12[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_31 (Ba  (None, 26, 26, 256)          1024      ['conv2d_31[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_31 (LeakyReLU)  (None, 26, 26, 256)          0         ['batch_normalization_31[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_17 (DropBlock  (None, 26, 26, 256)          0         ['leaky_re_lu_31[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)          (None, 26, 26, 512)          1179648   ['drop_block2d_17[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_32 (Ba  (None, 26, 26, 512)          2048      ['conv2d_32[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_32 (LeakyReLU)  (None, 26, 26, 512)          0         ['batch_normalization_32[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_13 (Add)                (None, 26, 26, 512)          0         ['add_12[0][0]',              \n",
      "                                                                     'leaky_re_lu_32[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)          (None, 26, 26, 256)          131072    ['add_13[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_33 (Ba  (None, 26, 26, 256)          1024      ['conv2d_33[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_33 (LeakyReLU)  (None, 26, 26, 256)          0         ['batch_normalization_33[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_18 (DropBlock  (None, 26, 26, 256)          0         ['leaky_re_lu_33[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)          (None, 26, 26, 512)          1179648   ['drop_block2d_18[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_34 (Ba  (None, 26, 26, 512)          2048      ['conv2d_34[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_34 (LeakyReLU)  (None, 26, 26, 512)          0         ['batch_normalization_34[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_14 (Add)                (None, 26, 26, 512)          0         ['add_13[0][0]',              \n",
      "                                                                     'leaky_re_lu_34[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)          (None, 26, 26, 256)          131072    ['add_14[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_35 (Ba  (None, 26, 26, 256)          1024      ['conv2d_35[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_35 (LeakyReLU)  (None, 26, 26, 256)          0         ['batch_normalization_35[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_19 (DropBlock  (None, 26, 26, 256)          0         ['leaky_re_lu_35[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)          (None, 26, 26, 512)          1179648   ['drop_block2d_19[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_36 (Ba  (None, 26, 26, 512)          2048      ['conv2d_36[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_36 (LeakyReLU)  (None, 26, 26, 512)          0         ['batch_normalization_36[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_15 (Add)                (None, 26, 26, 512)          0         ['add_14[0][0]',              \n",
      "                                                                     'leaky_re_lu_36[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)          (None, 26, 26, 256)          131072    ['add_15[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_37 (Ba  (None, 26, 26, 256)          1024      ['conv2d_37[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_37 (LeakyReLU)  (None, 26, 26, 256)          0         ['batch_normalization_37[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_20 (DropBlock  (None, 26, 26, 256)          0         ['leaky_re_lu_37[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)          (None, 26, 26, 512)          1179648   ['drop_block2d_20[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_38 (Ba  (None, 26, 26, 512)          2048      ['conv2d_38[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_38 (LeakyReLU)  (None, 26, 26, 512)          0         ['batch_normalization_38[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_16 (Add)                (None, 26, 26, 512)          0         ['add_15[0][0]',              \n",
      "                                                                     'leaky_re_lu_38[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)          (None, 26, 26, 256)          131072    ['add_16[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_39 (Ba  (None, 26, 26, 256)          1024      ['conv2d_39[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_39 (LeakyReLU)  (None, 26, 26, 256)          0         ['batch_normalization_39[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_21 (DropBlock  (None, 26, 26, 256)          0         ['leaky_re_lu_39[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)          (None, 26, 26, 512)          1179648   ['drop_block2d_21[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_40 (Ba  (None, 26, 26, 512)          2048      ['conv2d_40[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_40 (LeakyReLU)  (None, 26, 26, 512)          0         ['batch_normalization_40[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_17 (Add)                (None, 26, 26, 512)          0         ['add_16[0][0]',              \n",
      "                                                                     'leaky_re_lu_40[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)          (None, 26, 26, 256)          131072    ['add_17[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_41 (Ba  (None, 26, 26, 256)          1024      ['conv2d_41[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_41 (LeakyReLU)  (None, 26, 26, 256)          0         ['batch_normalization_41[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_22 (DropBlock  (None, 26, 26, 256)          0         ['leaky_re_lu_41[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)          (None, 26, 26, 512)          1179648   ['drop_block2d_22[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_42 (Ba  (None, 26, 26, 512)          2048      ['conv2d_42[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_42 (LeakyReLU)  (None, 26, 26, 512)          0         ['batch_normalization_42[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_18 (Add)                (None, 26, 26, 512)          0         ['add_17[0][0]',              \n",
      "                                                                     'leaky_re_lu_42[0][0]']      \n",
      "                                                                                                  \n",
      " zero_padding2d_4 (ZeroPadd  (None, 27, 27, 512)          0         ['add_18[0][0]']              \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)          (None, 13, 13, 1024)         4718592   ['zero_padding2d_4[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_43 (Ba  (None, 13, 13, 1024)         4096      ['conv2d_43[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_43 (LeakyReLU)  (None, 13, 13, 1024)         0         ['batch_normalization_43[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_23 (DropBlock  (None, 13, 13, 1024)         0         ['leaky_re_lu_43[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)          (None, 13, 13, 512)          524288    ['drop_block2d_23[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_44 (Ba  (None, 13, 13, 512)          2048      ['conv2d_44[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_44 (LeakyReLU)  (None, 13, 13, 512)          0         ['batch_normalization_44[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_24 (DropBlock  (None, 13, 13, 512)          0         ['leaky_re_lu_44[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)          (None, 13, 13, 1024)         4718592   ['drop_block2d_24[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_45 (Ba  (None, 13, 13, 1024)         4096      ['conv2d_45[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_45 (LeakyReLU)  (None, 13, 13, 1024)         0         ['batch_normalization_45[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_19 (Add)                (None, 13, 13, 1024)         0         ['drop_block2d_23[0][0]',     \n",
      "                                                                     'leaky_re_lu_45[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)          (None, 13, 13, 512)          524288    ['add_19[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_46 (Ba  (None, 13, 13, 512)          2048      ['conv2d_46[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_46 (LeakyReLU)  (None, 13, 13, 512)          0         ['batch_normalization_46[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_25 (DropBlock  (None, 13, 13, 512)          0         ['leaky_re_lu_46[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)          (None, 13, 13, 1024)         4718592   ['drop_block2d_25[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_47 (Ba  (None, 13, 13, 1024)         4096      ['conv2d_47[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_47 (LeakyReLU)  (None, 13, 13, 1024)         0         ['batch_normalization_47[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_20 (Add)                (None, 13, 13, 1024)         0         ['add_19[0][0]',              \n",
      "                                                                     'leaky_re_lu_47[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)          (None, 13, 13, 512)          524288    ['add_20[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_48 (Ba  (None, 13, 13, 512)          2048      ['conv2d_48[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_48 (LeakyReLU)  (None, 13, 13, 512)          0         ['batch_normalization_48[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_26 (DropBlock  (None, 13, 13, 512)          0         ['leaky_re_lu_48[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)          (None, 13, 13, 1024)         4718592   ['drop_block2d_26[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_49 (Ba  (None, 13, 13, 1024)         4096      ['conv2d_49[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_49 (LeakyReLU)  (None, 13, 13, 1024)         0         ['batch_normalization_49[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_21 (Add)                (None, 13, 13, 1024)         0         ['add_20[0][0]',              \n",
      "                                                                     'leaky_re_lu_49[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)          (None, 13, 13, 512)          524288    ['add_21[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_50 (Ba  (None, 13, 13, 512)          2048      ['conv2d_50[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_50 (LeakyReLU)  (None, 13, 13, 512)          0         ['batch_normalization_50[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_27 (DropBlock  (None, 13, 13, 512)          0         ['leaky_re_lu_50[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)          (None, 13, 13, 1024)         4718592   ['drop_block2d_27[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_51 (Ba  (None, 13, 13, 1024)         4096      ['conv2d_51[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_51 (LeakyReLU)  (None, 13, 13, 1024)         0         ['batch_normalization_51[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_22 (Add)                (None, 13, 13, 1024)         0         ['add_21[0][0]',              \n",
      "                                                                     'leaky_re_lu_51[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)          (None, 13, 13, 512)          524288    ['add_22[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_52 (Ba  (None, 13, 13, 512)          2048      ['conv2d_52[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_52 (LeakyReLU)  (None, 13, 13, 512)          0         ['batch_normalization_52[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_28 (DropBlock  (None, 13, 13, 512)          0         ['leaky_re_lu_52[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)          (None, 13, 13, 1024)         4718592   ['drop_block2d_28[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_53 (Ba  (None, 13, 13, 1024)         4096      ['conv2d_53[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_53 (LeakyReLU)  (None, 13, 13, 1024)         0         ['batch_normalization_53[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_29 (DropBlock  (None, 13, 13, 1024)         0         ['leaky_re_lu_53[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)          (None, 13, 13, 512)          524288    ['drop_block2d_29[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_54 (Ba  (None, 13, 13, 512)          2048      ['conv2d_54[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_54 (LeakyReLU)  (None, 13, 13, 512)          0         ['batch_normalization_54[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_30 (DropBlock  (None, 13, 13, 512)          0         ['leaky_re_lu_54[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)          (None, 13, 13, 1024)         4718592   ['drop_block2d_30[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_55 (Ba  (None, 13, 13, 1024)         4096      ['conv2d_55[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_55 (LeakyReLU)  (None, 13, 13, 1024)         0         ['batch_normalization_55[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_31 (DropBlock  (None, 13, 13, 1024)         0         ['leaky_re_lu_55[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)          (None, 13, 13, 512)          524288    ['drop_block2d_31[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_56 (Ba  (None, 13, 13, 512)          2048      ['conv2d_56[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_56 (LeakyReLU)  (None, 13, 13, 512)          0         ['batch_normalization_56[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)          (None, 13, 13, 256)          131072    ['leaky_re_lu_56[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_58 (Ba  (None, 13, 13, 256)          1024      ['conv2d_58[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_58 (LeakyReLU)  (None, 13, 13, 256)          0         ['batch_normalization_58[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2  (None, 26, 26, 256)          0         ['leaky_re_lu_58[0][0]']      \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 26, 26, 512)          0         ['up_sampling2d[0][0]',       \n",
      "                                                                     'conv2d_37[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)          (None, 26, 26, 256)          131072    ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_59 (Ba  (None, 26, 26, 256)          1024      ['conv2d_59[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_59 (LeakyReLU)  (None, 26, 26, 256)          0         ['batch_normalization_59[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_32 (DropBlock  (None, 26, 26, 256)          0         ['leaky_re_lu_59[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)          (None, 26, 26, 512)          1179648   ['drop_block2d_32[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_60 (Ba  (None, 26, 26, 512)          2048      ['conv2d_60[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_60 (LeakyReLU)  (None, 26, 26, 512)          0         ['batch_normalization_60[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_33 (DropBlock  (None, 26, 26, 512)          0         ['leaky_re_lu_60[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)          (None, 26, 26, 256)          131072    ['drop_block2d_33[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_61 (Ba  (None, 26, 26, 256)          1024      ['conv2d_61[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_61 (LeakyReLU)  (None, 26, 26, 256)          0         ['batch_normalization_61[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_34 (DropBlock  (None, 26, 26, 256)          0         ['leaky_re_lu_61[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)          (None, 26, 26, 512)          1179648   ['drop_block2d_34[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_62 (Ba  (None, 26, 26, 512)          2048      ['conv2d_62[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_62 (LeakyReLU)  (None, 26, 26, 512)          0         ['batch_normalization_62[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_35 (DropBlock  (None, 26, 26, 512)          0         ['leaky_re_lu_62[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)          (None, 26, 26, 256)          131072    ['drop_block2d_35[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_63 (Ba  (None, 26, 26, 256)          1024      ['conv2d_63[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_63 (LeakyReLU)  (None, 26, 26, 256)          0         ['batch_normalization_63[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)          (None, 26, 26, 128)          32768     ['leaky_re_lu_63[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_65 (Ba  (None, 26, 26, 128)          512       ['conv2d_65[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_65 (LeakyReLU)  (None, 26, 26, 128)          0         ['batch_normalization_65[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSamplin  (None, 52, 52, 128)          0         ['leaky_re_lu_65[0][0]']      \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 52, 52, 256)          0         ['up_sampling2d_1[0][0]',     \n",
      " )                                                                   'batch_normalization_22[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)          (None, 52, 52, 128)          32768     ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_66 (Ba  (None, 52, 52, 128)          512       ['conv2d_66[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_66 (LeakyReLU)  (None, 52, 52, 128)          0         ['batch_normalization_66[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_36 (DropBlock  (None, 52, 52, 128)          0         ['leaky_re_lu_66[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)          (None, 52, 52, 256)          294912    ['drop_block2d_36[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_67 (Ba  (None, 52, 52, 256)          1024      ['conv2d_67[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_67 (LeakyReLU)  (None, 52, 52, 256)          0         ['batch_normalization_67[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_37 (DropBlock  (None, 52, 52, 256)          0         ['leaky_re_lu_67[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)          (None, 52, 52, 128)          32768     ['drop_block2d_37[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_68 (Ba  (None, 52, 52, 128)          512       ['conv2d_68[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_68 (LeakyReLU)  (None, 52, 52, 128)          0         ['batch_normalization_68[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_38 (DropBlock  (None, 52, 52, 128)          0         ['leaky_re_lu_68[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_69 (Conv2D)          (None, 52, 52, 256)          294912    ['drop_block2d_38[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_69 (Ba  (None, 52, 52, 256)          1024      ['conv2d_69[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_69 (LeakyReLU)  (None, 52, 52, 256)          0         ['batch_normalization_69[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " drop_block2d_39 (DropBlock  (None, 52, 52, 256)          0         ['leaky_re_lu_69[0][0]']      \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_70 (Conv2D)          (None, 52, 52, 128)          32768     ['drop_block2d_39[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_70 (Ba  (None, 52, 52, 128)          512       ['conv2d_70[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_70 (LeakyReLU)  (None, 52, 52, 128)          0         ['batch_normalization_70[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)          (None, 13, 13, 1024)         4718592   ['leaky_re_lu_56[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)          (None, 26, 26, 512)          1179648   ['leaky_re_lu_63[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_71 (Conv2D)          (None, 52, 52, 256)          294912    ['leaky_re_lu_70[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_57 (Ba  (None, 13, 13, 1024)         4096      ['conv2d_57[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_64 (Ba  (None, 26, 26, 512)          2048      ['conv2d_64[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_71 (Ba  (None, 52, 52, 256)          1024      ['conv2d_71[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_57 (LeakyReLU)  (None, 13, 13, 1024)         0         ['batch_normalization_57[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " leaky_re_lu_64 (LeakyReLU)  (None, 26, 26, 512)          0         ['batch_normalization_64[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " leaky_re_lu_71 (LeakyReLU)  (None, 52, 52, 256)          0         ['batch_normalization_71[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)          (None, 13, 13, 2)            2050      ['leaky_re_lu_57[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_77 (Conv2D)          (None, 13, 13, 2)            2050      ['leaky_re_lu_57[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_81 (Conv2D)          (None, 13, 13, 2)            2050      ['leaky_re_lu_57[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_85 (Conv2D)          (None, 26, 26, 2)            1026      ['leaky_re_lu_64[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_89 (Conv2D)          (None, 26, 26, 2)            1026      ['leaky_re_lu_64[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_93 (Conv2D)          (None, 26, 26, 2)            1026      ['leaky_re_lu_64[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_97 (Conv2D)          (None, 52, 52, 2)            514       ['leaky_re_lu_71[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_101 (Conv2D)         (None, 52, 52, 2)            514       ['leaky_re_lu_71[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_105 (Conv2D)         (None, 52, 52, 2)            514       ['leaky_re_lu_71[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)          (None, 13, 13, 2)            2050      ['leaky_re_lu_57[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLamb  (None, 13, 13, 2)            0         ['conv2d_73[0][0]']           \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)          (None, 13, 13, 1)            1025      ['leaky_re_lu_57[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)          (None, 13, 13, 10)           10250     ['leaky_re_lu_57[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_76 (Conv2D)          (None, 13, 13, 2)            2050      ['leaky_re_lu_57[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLa  (None, 13, 13, 2)            0         ['conv2d_77[0][0]']           \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_78 (Conv2D)          (None, 13, 13, 1)            1025      ['leaky_re_lu_57[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_79 (Conv2D)          (None, 13, 13, 10)           10250     ['leaky_re_lu_57[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_80 (Conv2D)          (None, 13, 13, 2)            2050      ['leaky_re_lu_57[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.multiply_2 (TFOpLa  (None, 13, 13, 2)            0         ['conv2d_81[0][0]']           \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_82 (Conv2D)          (None, 13, 13, 1)            1025      ['leaky_re_lu_57[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_83 (Conv2D)          (None, 13, 13, 10)           10250     ['leaky_re_lu_57[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_84 (Conv2D)          (None, 26, 26, 2)            1026      ['leaky_re_lu_64[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.multiply_3 (TFOpLa  (None, 26, 26, 2)            0         ['conv2d_85[0][0]']           \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_86 (Conv2D)          (None, 26, 26, 1)            513       ['leaky_re_lu_64[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_87 (Conv2D)          (None, 26, 26, 10)           5130      ['leaky_re_lu_64[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_88 (Conv2D)          (None, 26, 26, 2)            1026      ['leaky_re_lu_64[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.multiply_4 (TFOpLa  (None, 26, 26, 2)            0         ['conv2d_89[0][0]']           \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_90 (Conv2D)          (None, 26, 26, 1)            513       ['leaky_re_lu_64[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_91 (Conv2D)          (None, 26, 26, 10)           5130      ['leaky_re_lu_64[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_92 (Conv2D)          (None, 26, 26, 2)            1026      ['leaky_re_lu_64[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.multiply_5 (TFOpLa  (None, 26, 26, 2)            0         ['conv2d_93[0][0]']           \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_94 (Conv2D)          (None, 26, 26, 1)            513       ['leaky_re_lu_64[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_95 (Conv2D)          (None, 26, 26, 10)           5130      ['leaky_re_lu_64[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_96 (Conv2D)          (None, 52, 52, 2)            514       ['leaky_re_lu_71[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.multiply_6 (TFOpLa  (None, 52, 52, 2)            0         ['conv2d_97[0][0]']           \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_98 (Conv2D)          (None, 52, 52, 1)            257       ['leaky_re_lu_71[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_99 (Conv2D)          (None, 52, 52, 10)           2570      ['leaky_re_lu_71[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_100 (Conv2D)         (None, 52, 52, 2)            514       ['leaky_re_lu_71[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.multiply_7 (TFOpLa  (None, 52, 52, 2)            0         ['conv2d_101[0][0]']          \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_102 (Conv2D)         (None, 52, 52, 1)            257       ['leaky_re_lu_71[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_103 (Conv2D)         (None, 52, 52, 10)           2570      ['leaky_re_lu_71[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_104 (Conv2D)         (None, 52, 52, 2)            514       ['leaky_re_lu_71[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.multiply_8 (TFOpLa  (None, 52, 52, 2)            0         ['conv2d_105[0][0]']          \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_106 (Conv2D)         (None, 52, 52, 1)            257       ['leaky_re_lu_71[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_107 (Conv2D)         (None, 52, 52, 10)           2570      ['leaky_re_lu_71[0][0]']      \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 13, 13, 45)           0         ['conv2d_72[0][0]',           \n",
      " )                                                                   'tf.math.multiply[0][0]',    \n",
      "                                                                     'conv2d_74[0][0]',           \n",
      "                                                                     'conv2d_75[0][0]',           \n",
      "                                                                     'conv2d_76[0][0]',           \n",
      "                                                                     'tf.math.multiply_1[0][0]',  \n",
      "                                                                     'conv2d_78[0][0]',           \n",
      "                                                                     'conv2d_79[0][0]',           \n",
      "                                                                     'conv2d_80[0][0]',           \n",
      "                                                                     'tf.math.multiply_2[0][0]',  \n",
      "                                                                     'conv2d_82[0][0]',           \n",
      "                                                                     'conv2d_83[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate  (None, 26, 26, 45)           0         ['conv2d_84[0][0]',           \n",
      " )                                                                   'tf.math.multiply_3[0][0]',  \n",
      "                                                                     'conv2d_86[0][0]',           \n",
      "                                                                     'conv2d_87[0][0]',           \n",
      "                                                                     'conv2d_88[0][0]',           \n",
      "                                                                     'tf.math.multiply_4[0][0]',  \n",
      "                                                                     'conv2d_90[0][0]',           \n",
      "                                                                     'conv2d_91[0][0]',           \n",
      "                                                                     'conv2d_92[0][0]',           \n",
      "                                                                     'tf.math.multiply_5[0][0]',  \n",
      "                                                                     'conv2d_94[0][0]',           \n",
      "                                                                     'conv2d_95[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate  (None, 52, 52, 45)           0         ['conv2d_96[0][0]',           \n",
      " )                                                                   'tf.math.multiply_6[0][0]',  \n",
      "                                                                     'conv2d_98[0][0]',           \n",
      "                                                                     'conv2d_99[0][0]',           \n",
      "                                                                     'conv2d_100[0][0]',          \n",
      "                                                                     'tf.math.multiply_7[0][0]',  \n",
      "                                                                     'conv2d_102[0][0]',          \n",
      "                                                                     'conv2d_103[0][0]',          \n",
      "                                                                     'conv2d_104[0][0]',          \n",
      "                                                                     'tf.math.multiply_8[0][0]',  \n",
      "                                                                     'conv2d_106[0][0]',          \n",
      "                                                                     'conv2d_107[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 61542887 (234.77 MB)\n",
      "Trainable params: 61490279 (234.57 MB)\n",
      "Non-trainable params: 52608 (205.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Build NN Model from class\n",
    "yolo = Yolo(class_names=class_names)\n",
    "anchors=[[0.26923078, 0.26923078],\n",
    "        [0.2626829 , 0.20192307],\n",
    "        [0.20192307, 0.13508038],\n",
    "        [0.13461539, 0.13461539],\n",
    "        [0.10096154, 0.10096154],\n",
    "        [0.07956883, 0.0673077 ],\n",
    "        [0.0673077 , 0.05288462],\n",
    "        [0.05288462, 0.03784578],\n",
    "        [0.03365385, 0.03365385]]\n",
    "\n",
    "yolo.create_model(anchors=anchors)\n",
    "yolo.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Optimizer\n",
    "from keras.optimizers import Adam\n",
    "optimizer = Adam(learning_rate=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Leaning Rate Scheduling\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch <= 20:\n",
    "        return lr\n",
    "    elif epoch <= 70:\n",
    "        return 3e-5\n",
    "    else:\n",
    "        return 1e-5\n",
    "\n",
    "callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.0369116]), array([0.00898685]), array([0.00223204])]\n"
     ]
    }
   ],
   "source": [
    "#### Loss function from YOLO Class\n",
    "from utils.tools import get_class_weight\n",
    "\n",
    "binary_weight_list = []\n",
    "\n",
    "for i in range(len(train_label)):\n",
    "    binary_weight_list.append(\n",
    "        get_class_weight(\n",
    "        train_label[i][..., 4:5],\n",
    "        method='binary'\n",
    "        )\n",
    "    )\n",
    "print(binary_weight_list)\n",
    "\n",
    "binary_weight_list = [0.1]*3\n",
    "\n",
    "\n",
    "ignore_thresh = 0.7\n",
    "use_focal_loss = True\n",
    "\n",
    "loss_weight = {\n",
    "    \"xy\":1,\n",
    "    \"wh\":1,\n",
    "    \"conf\":5,\n",
    "    \"prob\":1\n",
    "    }\n",
    "\n",
    "loss_fn = yolo.loss(\n",
    "    binary_weight_list,\n",
    "    loss_weight=loss_weight,\n",
    "    ignore_thresh=ignore_thresh\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build Metrics from Yolo Class\n",
    "metrics = yolo.metrics(\"obj+iou+class\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Model Compilation\n",
    "yolo.model.compile(\n",
    "    optimizer = optimizer,\n",
    "    #optimizer=SGD(learning_rate=1e-10, momentum=0.9, decay=5e-4),\n",
    "    loss = loss_fn,\n",
    "    metrics = metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From d:\\Number Plate Region\\env\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Number Plate Region\\env\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "200/200 [==============================] - 2189s 11s/step - loss: 965.4499 - concatenate_2_loss: 110.4549 - concatenate_3_loss: 195.8499 - concatenate_4_loss: 659.1446 - concatenate_2_obj_acc: 0.7257 - concatenate_2_mean_iou: 0.3102 - concatenate_2_class_acc: 0.1037 - concatenate_3_obj_acc: 0.7918 - concatenate_3_mean_iou: 0.3542 - concatenate_3_class_acc: 0.0964 - concatenate_4_obj_acc: 0.8105 - concatenate_4_mean_iou: 0.3236 - concatenate_4_class_acc: 0.1011 - val_loss: 300.1154 - val_concatenate_2_loss: 85.2784 - val_concatenate_3_loss: 95.1138 - val_concatenate_4_loss: 119.7232 - val_concatenate_2_obj_acc: 0.9663 - val_concatenate_2_mean_iou: 0.2775 - val_concatenate_2_class_acc: 0.1088 - val_concatenate_3_obj_acc: 0.9916 - val_concatenate_3_mean_iou: 0.4612 - val_concatenate_3_class_acc: 0.1063 - val_concatenate_4_obj_acc: 0.9979 - val_concatenate_4_mean_iou: 0.3926 - val_concatenate_4_class_acc: 0.1020 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 2244s 11s/step - loss: 355.2970 - concatenate_2_loss: 64.0284 - concatenate_3_loss: 91.0875 - concatenate_4_loss: 200.1810 - concatenate_2_obj_acc: 0.9440 - concatenate_2_mean_iou: 0.3544 - concatenate_2_class_acc: 0.1058 - concatenate_3_obj_acc: 0.9700 - concatenate_3_mean_iou: 0.4322 - concatenate_3_class_acc: 0.1059 - concatenate_4_obj_acc: 0.9773 - concatenate_4_mean_iou: 0.4478 - concatenate_4_class_acc: 0.1000 - val_loss: 301.7103 - val_concatenate_2_loss: 97.8985 - val_concatenate_3_loss: 84.8782 - val_concatenate_4_loss: 118.9337 - val_concatenate_2_obj_acc: 0.9663 - val_concatenate_2_mean_iou: 0.2074 - val_concatenate_2_class_acc: 0.1042 - val_concatenate_3_obj_acc: 0.9916 - val_concatenate_3_mean_iou: 0.4634 - val_concatenate_3_class_acc: 0.1053 - val_concatenate_4_obj_acc: 0.9979 - val_concatenate_4_mean_iou: 0.4227 - val_concatenate_4_class_acc: 0.0988 - lr: 5.0000e-05\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 2077s 10s/step - loss: 220.4293 - concatenate_2_loss: 51.9035 - concatenate_3_loss: 62.6425 - concatenate_4_loss: 105.8833 - concatenate_2_obj_acc: 0.9580 - concatenate_2_mean_iou: 0.3881 - concatenate_2_class_acc: 0.1154 - concatenate_3_obj_acc: 0.9859 - concatenate_3_mean_iou: 0.4797 - concatenate_3_class_acc: 0.1093 - concatenate_4_obj_acc: 0.9960 - concatenate_4_mean_iou: 0.4987 - concatenate_4_class_acc: 0.1020 - val_loss: 295.8874 - val_concatenate_2_loss: 111.9846 - val_concatenate_3_loss: 78.8543 - val_concatenate_4_loss: 105.0485 - val_concatenate_2_obj_acc: 0.9664 - val_concatenate_2_mean_iou: 0.1966 - val_concatenate_2_class_acc: 0.1006 - val_concatenate_3_obj_acc: 0.9914 - val_concatenate_3_mean_iou: 0.4650 - val_concatenate_3_class_acc: 0.1062 - val_concatenate_4_obj_acc: 0.9979 - val_concatenate_4_mean_iou: 0.4223 - val_concatenate_4_class_acc: 0.0924 - lr: 5.0000e-05\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 1998s 10s/step - loss: 176.0104 - concatenate_2_loss: 44.9471 - concatenate_3_loss: 50.7239 - concatenate_4_loss: 80.3394 - concatenate_2_obj_acc: 0.9659 - concatenate_2_mean_iou: 0.4308 - concatenate_2_class_acc: 0.1222 - concatenate_3_obj_acc: 0.9876 - concatenate_3_mean_iou: 0.5193 - concatenate_3_class_acc: 0.1185 - concatenate_4_obj_acc: 0.9967 - concatenate_4_mean_iou: 0.5297 - concatenate_4_class_acc: 0.1057 - val_loss: 251.6433 - val_concatenate_2_loss: 87.2482 - val_concatenate_3_loss: 69.8667 - val_concatenate_4_loss: 94.5285 - val_concatenate_2_obj_acc: 0.9667 - val_concatenate_2_mean_iou: 0.2842 - val_concatenate_2_class_acc: 0.1049 - val_concatenate_3_obj_acc: 0.9914 - val_concatenate_3_mean_iou: 0.4791 - val_concatenate_3_class_acc: 0.0973 - val_concatenate_4_obj_acc: 0.9979 - val_concatenate_4_mean_iou: 0.4222 - val_concatenate_4_class_acc: 0.1007 - lr: 5.0000e-05\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 1959s 10s/step - loss: 153.2447 - concatenate_2_loss: 40.5853 - concatenate_3_loss: 44.5514 - concatenate_4_loss: 68.1081 - concatenate_2_obj_acc: 0.9722 - concatenate_2_mean_iou: 0.4729 - concatenate_2_class_acc: 0.1187 - concatenate_3_obj_acc: 0.9888 - concatenate_3_mean_iou: 0.5555 - concatenate_3_class_acc: 0.1261 - concatenate_4_obj_acc: 0.9967 - concatenate_4_mean_iou: 0.5517 - concatenate_4_class_acc: 0.1108 - val_loss: 218.6173 - val_concatenate_2_loss: 70.2351 - val_concatenate_3_loss: 63.1612 - val_concatenate_4_loss: 85.2210 - val_concatenate_2_obj_acc: 0.9675 - val_concatenate_2_mean_iou: 0.3227 - val_concatenate_2_class_acc: 0.0950 - val_concatenate_3_obj_acc: 0.9914 - val_concatenate_3_mean_iou: 0.4673 - val_concatenate_3_class_acc: 0.0940 - val_concatenate_4_obj_acc: 0.9979 - val_concatenate_4_mean_iou: 0.4273 - val_concatenate_4_class_acc: 0.0991 - lr: 5.0000e-05\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 1966s 10s/step - loss: 139.2480 - concatenate_2_loss: 37.5380 - concatenate_3_loss: 40.5716 - concatenate_4_loss: 61.1386 - concatenate_2_obj_acc: 0.9762 - concatenate_2_mean_iou: 0.5068 - concatenate_2_class_acc: 0.1279 - concatenate_3_obj_acc: 0.9895 - concatenate_3_mean_iou: 0.5872 - concatenate_3_class_acc: 0.1313 - concatenate_4_obj_acc: 0.9966 - concatenate_4_mean_iou: 0.5714 - concatenate_4_class_acc: 0.1164 - val_loss: 199.4507 - val_concatenate_2_loss: 63.1106 - val_concatenate_3_loss: 57.7720 - val_concatenate_4_loss: 78.5682 - val_concatenate_2_obj_acc: 0.9707 - val_concatenate_2_mean_iou: 0.3238 - val_concatenate_2_class_acc: 0.0959 - val_concatenate_3_obj_acc: 0.9915 - val_concatenate_3_mean_iou: 0.4654 - val_concatenate_3_class_acc: 0.0795 - val_concatenate_4_obj_acc: 0.9979 - val_concatenate_4_mean_iou: 0.4276 - val_concatenate_4_class_acc: 0.0918 - lr: 5.0000e-05\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 1976s 10s/step - loss: 130.1829 - concatenate_2_loss: 35.8250 - concatenate_3_loss: 37.9452 - concatenate_4_loss: 56.4126 - concatenate_2_obj_acc: 0.9785 - concatenate_2_mean_iou: 0.5312 - concatenate_2_class_acc: 0.1256 - concatenate_3_obj_acc: 0.9902 - concatenate_3_mean_iou: 0.6128 - concatenate_3_class_acc: 0.1360 - concatenate_4_obj_acc: 0.9965 - concatenate_4_mean_iou: 0.5853 - concatenate_4_class_acc: 0.1186 - val_loss: 182.1000 - val_concatenate_2_loss: 56.7982 - val_concatenate_3_loss: 52.5148 - val_concatenate_4_loss: 72.7871 - val_concatenate_2_obj_acc: 0.9746 - val_concatenate_2_mean_iou: 0.3196 - val_concatenate_2_class_acc: 0.1002 - val_concatenate_3_obj_acc: 0.9912 - val_concatenate_3_mean_iou: 0.4806 - val_concatenate_3_class_acc: 0.0783 - val_concatenate_4_obj_acc: 0.9979 - val_concatenate_4_mean_iou: 0.4289 - val_concatenate_4_class_acc: 0.0938 - lr: 5.0000e-05\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 1970s 10s/step - loss: 123.1403 - concatenate_2_loss: 34.2527 - concatenate_3_loss: 36.0518 - concatenate_4_loss: 52.8359 - concatenate_2_obj_acc: 0.9803 - concatenate_2_mean_iou: 0.5510 - concatenate_2_class_acc: 0.1334 - concatenate_3_obj_acc: 0.9907 - concatenate_3_mean_iou: 0.6320 - concatenate_3_class_acc: 0.1378 - concatenate_4_obj_acc: 0.9964 - concatenate_4_mean_iou: 0.5960 - concatenate_4_class_acc: 0.1287 - val_loss: 169.1801 - val_concatenate_2_loss: 53.2202 - val_concatenate_3_loss: 48.8547 - val_concatenate_4_loss: 67.1052 - val_concatenate_2_obj_acc: 0.9783 - val_concatenate_2_mean_iou: 0.3048 - val_concatenate_2_class_acc: 0.1090 - val_concatenate_3_obj_acc: 0.9915 - val_concatenate_3_mean_iou: 0.4781 - val_concatenate_3_class_acc: 0.0877 - val_concatenate_4_obj_acc: 0.9979 - val_concatenate_4_mean_iou: 0.4421 - val_concatenate_4_class_acc: 0.0992 - lr: 5.0000e-05\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 1952s 10s/step - loss: 117.4437 - concatenate_2_loss: 33.0697 - concatenate_3_loss: 34.4979 - concatenate_4_loss: 49.8762 - concatenate_2_obj_acc: 0.9815 - concatenate_2_mean_iou: 0.5726 - concatenate_2_class_acc: 0.1327 - concatenate_3_obj_acc: 0.9913 - concatenate_3_mean_iou: 0.6507 - concatenate_3_class_acc: 0.1504 - concatenate_4_obj_acc: 0.9962 - concatenate_4_mean_iou: 0.6098 - concatenate_4_class_acc: 0.1279 - val_loss: 154.0061 - val_concatenate_2_loss: 47.8462 - val_concatenate_3_loss: 44.3394 - val_concatenate_4_loss: 61.8205 - val_concatenate_2_obj_acc: 0.9813 - val_concatenate_2_mean_iou: 0.3306 - val_concatenate_2_class_acc: 0.1076 - val_concatenate_3_obj_acc: 0.9919 - val_concatenate_3_mean_iou: 0.5103 - val_concatenate_3_class_acc: 0.0952 - val_concatenate_4_obj_acc: 0.9979 - val_concatenate_4_mean_iou: 0.4639 - val_concatenate_4_class_acc: 0.1054 - lr: 5.0000e-05\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 1976s 10s/step - loss: 112.7521 - concatenate_2_loss: 32.1784 - concatenate_3_loss: 33.4167 - concatenate_4_loss: 47.1571 - concatenate_2_obj_acc: 0.9822 - concatenate_2_mean_iou: 0.5876 - concatenate_2_class_acc: 0.1387 - concatenate_3_obj_acc: 0.9917 - concatenate_3_mean_iou: 0.6661 - concatenate_3_class_acc: 0.1474 - concatenate_4_obj_acc: 0.9959 - concatenate_4_mean_iou: 0.6183 - concatenate_4_class_acc: 0.1312 - val_loss: 143.2231 - val_concatenate_2_loss: 45.2512 - val_concatenate_3_loss: 41.1254 - val_concatenate_4_loss: 56.8465 - val_concatenate_2_obj_acc: 0.9816 - val_concatenate_2_mean_iou: 0.3460 - val_concatenate_2_class_acc: 0.1142 - val_concatenate_3_obj_acc: 0.9916 - val_concatenate_3_mean_iou: 0.5236 - val_concatenate_3_class_acc: 0.1032 - val_concatenate_4_obj_acc: 0.9980 - val_concatenate_4_mean_iou: 0.4915 - val_concatenate_4_class_acc: 0.1118 - lr: 5.0000e-05\n"
     ]
    }
   ],
   "source": [
    "### Model Training And Validation\n",
    "train_history = yolo.model.fit(\n",
    "    train_img,\n",
    "    train_label,\n",
    "    epochs = n_epoch,\n",
    "    batch_size=5,\n",
    "    verbose=1,\n",
    "    validation_data=(valid_img, valid_label),\n",
    "    callbacks=[callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Number Plate Region\\env\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "yolo.model.save('./weight_yolov3_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = np.expand_dims(test_img[0], axis=0)\n",
    "prediction = yolo.model.predict(image)\n",
    "\n",
    "for i in range(len(image)):\n",
    "    yolo_data.vis_img(\n",
    "        image[i],\n",
    "        prediction[2][i],\n",
    "        prediction[1][i],\n",
    "        prediction[0][i],\n",
    "        conf_threshold=0.5,\n",
    "        nms_mode=2,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
